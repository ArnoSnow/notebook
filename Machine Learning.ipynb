{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b21f0de",
   "metadata": {},
   "source": [
    "### æœºå™¨å­¦ä¹ \n",
    "###### ä¸€ã€å®šä¹‰ä¸æµç¨‹\n",
    "ï¼ˆä¸€ï¼‰æœºå™¨å­¦ä¹ çš„å®šä¹‰ ï¼ˆäºŒï¼‰æœºå™¨å­¦ä¹ çš„æµç¨‹ ï¼ˆä¸‰ï¼‰æœºå™¨å­¦ä¹ çš„é›†æˆåº“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ae117",
   "metadata": {},
   "source": [
    "ï¼ˆä¸€ï¼‰æœºå™¨å­¦ä¹ çš„å®šä¹‰     \n",
    "è®¡ç®—å™¨ä»æä¾›çš„æ•°æ®é›†ä¸­å‘ç° xå’Œyç›´æ¥çš„å…³ç³»ï¼Œä»è€Œä½¿å¾—å®ƒé¢„æµ‹å‡ºæ¥çš„yè¶Šæ¥è¶Šå‡†ç¡®çš„è¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa88b04",
   "metadata": {},
   "source": [
    "ï¼ˆäºŒï¼‰æœºå™¨å­¦ä¹ çš„æµç¨‹    \n",
    "1ã€æ˜ç¡®ä»»åŠ¡ç›®çš„ï¼ˆæœ‰ç›‘ç£å­¦ä¹ ï¼šåˆ†ç±»é—®é¢˜ã€å›å½’é—®é¢˜ ï¼› æ— ç›‘ç£å­¦ä¹ ï¼šèšç±»é—®é¢˜ å¦å¤–è¿˜æœ‰åŠç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œæš‚æ—¶æœªæ¶‰åŠï¼‰     \n",
    "2ã€æ•°æ®é¢„å¤„ç†ï¼ˆæ¸…æ´—ï¼šå¤„ç†ç¼ºå¤±å€¼ï¼Œå¼‚å¸¸å€¼ï¼Œé”™è¯¯å€¼ï¼›è½¬åŒ–ï¼šå½’ä¸€åŒ–ã€æ ‡å‡†åŒ–ï¼‰     \n",
    "sklearn.preprocessing / pandasçš„ç¼ºå¤±å€¼å¤„ç†èƒ½åŠ› fillna dropna findna   \n",
    "3ã€ç‰¹å¾å·¥ç¨‹ï¼ˆç‰¹å¾å³ç»´åº¦ï¼Œå¯å¢åŠ å¯å‡å°‘å¯çº¿æ€§å˜åŒ–ï¼Œæ¯”å¦‚é‡‡ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å’Œçº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰ï¼Œå…¶ä¹Ÿç®—æ•°æ®é¢„å¤„ç†çš„ä¸€éƒ¨åˆ†ï¼‰   \n",
    "sklearn.decomposition  \n",
    "sklearn.feature_selection      \n",
    "4ã€ä¾æ®ç›®çš„å’Œæ•°æ®é›†ç‰¹å¾é€‰å‡ºç®—æ³•ï¼ˆå¯¹åº”å…³ç³»å¯è§ä¸‹å›¾ï¼‰       \n",
    "sklearn.neighbors   \n",
    "sklearn.neural_network    \n",
    "skleran.naive_bayes   \n",
    "skleran.cluster   \n",
    "skleran.linear_model        \n",
    "5ã€æ¨¡å‹è®­ç»ƒï¼ˆå°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä¸€èˆ¬æµ‹è¯•é›†å æ®20-30%ï¼‰     \n",
    "sklearn.model_selection   \n",
    "6ã€è¯„ä¼°ä¼˜åŒ–   \n",
    "sklearn.metrics    \n",
    "7ã€å®é™…ä¸Šçº¿          \n",
    "8ã€æ¥æ”¶åé¦ˆå†ä¼˜åŒ–          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "image_path = r\"D:\\1-script\\3-PYTHON\\imageå­˜å‚¨jupyteråœ¨ç”¨çš„å›¾ç‰‡å­˜æ”¾äºæ­¤\\æœºå™¨å­¦ä¹ çš„æµç¨‹.png\"\n",
    "display(Image(filename = image_path,width=500, height=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33ef803",
   "metadata": {},
   "source": [
    "ï¼ˆä¸‰ï¼‰æœºå™¨å­¦ä¹ çš„é›†æˆåº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84199608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee3dc7",
   "metadata": {},
   "source": [
    "### æœºå™¨å­¦ä¹ \n",
    "###### äºŒã€æ•°æ®é¢„å¤„ç†\n",
    "ï¼ˆä¸€ï¼‰å½’ä¸€åŒ– ï¼ˆäºŒï¼‰æ ‡å‡†åŒ– ï¼ˆä¸‰ï¼‰æ­£åˆ™åŒ– ï¼ˆå››ï¼‰ç®€æ˜“æ¨èä½¿ç”¨å“ªç§æ•°æ®é¢„å¤„ç†æ–¹å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1d501",
   "metadata": {},
   "source": [
    "ï¼ˆä¸€ï¼‰å½’ä¸€åŒ–     \n",
    "1.å½’ä¸€åŒ–å®šä¹‰  2.å½’ä¸€åŒ–æ–¹æ³•è¯´æ˜ä¸é€‚ç”¨åœºæ™¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cd8b4",
   "metadata": {},
   "source": [
    "1. å½’ä¸€åŒ–å®šä¹‰   \n",
    "å½’ä¸€åŒ–æ˜¯ä¸€ç§æ•°æ®é¢„å¤„ç†æŠ€æœ¯ï¼Œç”¨äºè°ƒæ•´æ•°å€¼æ•°æ®çš„èŒƒå›´ï¼Œä»¥ä¾¿å°†å…¶æ ‡å‡†åŒ–åˆ°ä¸€ä¸ªç‰¹å®šçš„èŒƒå›´ï¼Œé€šå¸¸æ˜¯ 0 åˆ° 1ã€‚è¿™ç§å¤„ç†å¯ä»¥ä½¿æ•°æ®åœ¨ä¸åŒçš„å°ºåº¦ä¸‹å…·æœ‰å¯æ¯”æ€§ï¼ŒåŒæ—¶æœ‰åŠ©äºæœºå™¨å­¦ä¹ ç®—æ³•æ›´å¥½åœ°ç†è§£å’Œå¤„ç†æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d549c95c",
   "metadata": {},
   "source": [
    "2. å½’ä¸€åŒ–æ–¹æ³•è¯´æ˜ä¸é€‚ç”¨åœºæ™¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡è®¾å­˜åœ¨ä¸€ä¸ªç‰¹å¾æ•°ç»„ä¸ºx_list , å…¶ä¸­åŒ…å«Nä¸ªxå…ƒç´ \n",
    "x_list = [50,75,100,200]\n",
    "print(\"åŸå§‹æ•°å€¼:\", x_list)\n",
    "\n",
    "# æ–¹æ³•ä¸€ æœ€å°-æœ€å¤§å½’ä¸€åŒ–ï¼ˆMin-Max Scalingï¼‰:\n",
    "# ç›®çš„ï¼šå°†ç‰¹å¾ç¼©æ”¾åˆ° [0, 1] èŒƒå›´å†…ï¼Œå¯¹äºå¼‚å¸¸å€¼å¤ªæ•æ„Ÿï¼Œå› ä¸ºä»–ä¾èµ–äºæœ€å¤§æœ€å°å€¼åšè®¡ç®— ï¼Œè¿™ç§æƒ…å†µæ¨èç”¨æ ‡å‡†åŒ–\n",
    "# é€‚ç”¨æƒ…å†µï¼šå½“æ•°æ®ä¸éµå¾ªæ­£æ€åˆ†å¸ƒæ—¶ï¼›å½“ç‰¹å¾çš„æœ€å¤§å€¼å’Œæœ€å°å€¼æ˜¯å·²çŸ¥çš„ã€‚\n",
    "x_preprocess = [(x-min(x_list)) /  (max(x_list)-min(x_list)) for x in x_list]\n",
    "print(f'æœ€å°-æœ€å¤§å½’ä¸€åŒ–é¢„å¤„ç†åçš„æ•°å€¼: {x_preprocess}')\n",
    "\n",
    "# æ–¹æ³•äºŒ å¯¹æ•°å½’ä¸€åŒ–:\n",
    "# ç›®çš„ï¼šç¼©æ”¾æ•°æ®ï¼Œä½¿å¾—é•¿å°¾æ•°æ®æ›´åŠ ç´§å¯†å‘ˆç°è¿‘ä¼¼æ­£æ€åˆ†å¸ƒï¼Œä½†å€¼ä¸ä¼šå›ºå®šåœ¨[0,1]\n",
    "# é€‚ç”¨æƒ…å†µï¼šé€‚åˆå¤„ç†é•¿å°¾æ•°æ® ç„¶åsklearnä¸­å¹¶æ— ç›´æ¥çš„å·¥å…· å¯ä»¥numpyå¯¹æ•°åŒ–ä¹‹åå†æœ€å¤§æœ€å°å€¼å¤„ç†\n",
    "# å¯¹äºæ»¡è¶³å¯¹æ•°åˆ†å¸ƒæˆ–æŒ‡æ•°åˆ†å¸ƒçš„æ•°æ®ï¼Œå¯¹æ•°å½’ä¸€åŒ–å¯ä»¥ä½¿å…¶æ¥è¿‘æ­£æ€åˆ†å¸ƒã€‚ä½†æ˜¯ï¼Œå‡å¦‚åŸå§‹æ•°æ®ä¸­å­˜åœ¨è´Ÿæ•°æˆ–é›¶ï¼Œåˆ™ä¸èƒ½ç›´æ¥ä½¿ç”¨å¯¹æ•°å‡½æ•°ï¼ˆå› ä¸º log(x) è¦æ±‚ xå¤§äº0ï¼‰ï¼Œéœ€è¦å…ˆå°†æ‰€æœ‰æ•°å€¼éƒ½è½¬æ¢ä¸ºæ­£æ•°ä¹‹åå†å½’ä¸€åŒ–ã€‚\n",
    "# å¯¹æ•°æ­£æ€åˆ†å¸ƒæ˜¯å…¸å‹çš„é•¿å°¾\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# æ¨¡æ‹Ÿä¸€äº›å…·æœ‰é•¿å°¾åˆ†å¸ƒçš„æ•°æ®\n",
    "data = np.random.exponential(scale=2, size=1000).reshape(-1, 1)\n",
    "\n",
    "# åº”ç”¨å¯¹æ•°å˜æ¢\n",
    "data_log_transformed = np.log1p(data)  # np.log1p(x) è®¡ç®— log(1 + x)ï¼Œç”¨äºå¤„ç†åŒ…å«0çš„æ•°æ®\n",
    "\n",
    "# ä½¿ç”¨ MinMaxScaler è¿›è¡Œå½’ä¸€åŒ–\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data_log_transformed)\n",
    "\n",
    "# ç»˜åˆ¶åŸå§‹æ•°æ®å’Œå¤„ç†åæ•°æ®çš„ç›´æ–¹å›¾ ä¸€ä¸ªä¼šæ‹–å‡ºä¸€ä¸ªé•¿é•¿çš„å°¾å·´ è€Œééª¤ç„¶å½’é›¶ å¯¹æ•°åçš„æ›´åŠ ç´§å¯† è¶‹å‘äºæ­£æ€åˆ†å¸ƒ\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data, bins=50, color='blue', alpha=0.7)\n",
    "plt.title(\"Original Data\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(data_normalized, bins=50, color='green', alpha=0.7)\n",
    "plt.title(\"Log Transformed and Normalized Data\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# æ–¹æ³•ä¸‰ åæ­£åˆ‡å½’ä¸€åŒ–\n",
    "# ç›®çš„ï¼šåˆ©ç”¨åæ­£åˆ‡å‡½æ•°ï¼ˆå³ arctanï¼‰æ¥å°†æ•°æ®æ˜ å°„åˆ°å›ºå®šçš„èŒƒå›´å†…ï¼Œåæ­£åˆ‡çš„å€¼åœ¨âˆ’ Ï€/2 åˆ° Ï€/2 ï¼› ä¸ºäº†è®©å½’ä¸€åŒ–åçš„æ•°æ®è½åœ¨æ›´å¸¸ç”¨çš„åŒºé—´ [0, 1] å†…ï¼Œå¯ä»¥è¿›ä¸€æ­¥å°† arctan çš„è¾“å‡ºè¿›è¡Œå˜æ¢ã€‚\n",
    "# é€‚ç”¨æƒ…å†µï¼šé€‚åˆå¤„ç†æ•°æ®èŒƒå›´å¹¿æ³›æˆ–åŒ…å«ç¦»ç¾¤å€¼çš„æ•°æ®é›† é‡å°¾åˆ†å¸ƒçš„æ•°æ®æ—¶éå¸¸æœ‰ç”¨ï¼Œå¦‚é‡‘èæ•°æ®æˆ–å…·æœ‰å¼‚å¸¸å€¼çš„ç¯å¢ƒæ•°æ®\n",
    "# sklearn ä¸€æ ·æ²¡æœ‰ç›´æ¥èƒ½ç”¨çš„ï¼Œ ä½†æ˜¯å¯ä»¥åŸºäºåº“çš„åŸºç¡€ç±»ï¼Œè‡ªå®šä¹‰ä¸€ä¸ªè½¬åŒ–å™¨\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class ArctanScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # æ— éœ€å­¦ä¹ ä»»ä½•å‚æ•°\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # åº”ç”¨åæ­£åˆ‡å½’ä¸€åŒ–\n",
    "        return (np.arctan(X) + np.pi / 2) / np.pi\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# ç”Ÿæˆéšæœºæ•°æ®\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "\n",
    "# åˆ›å»ºå½’ä¸€åŒ–ç®¡é“\n",
    "pipeline = Pipeline([\n",
    "    ('arctan_scaler', ArctanScaler())\n",
    "])\n",
    "\n",
    "# å½’ä¸€åŒ–æ•°æ®\n",
    "X_scaled = pipeline.fit_transform(X)\n",
    "\n",
    "print(\"Original Data Sample:\", X[:5])\n",
    "print(\"Scaled Data Sample:\", X_scaled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08668afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# ç”Ÿæˆå¯¹æ•°æ­£æ€åˆ†å¸ƒæ•°æ®\n",
    "data_lognorm = np.random.lognormal(mean=0, sigma=1, size=1000)\n",
    "\n",
    "# ç”ŸæˆæŒ‡æ•°åˆ†å¸ƒæ•°æ®\n",
    "data_exp = np.random.exponential(scale=1, size=1000)\n",
    "\n",
    "# ç»˜åˆ¶ç›´æ–¹å›¾\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data_lognorm, bins=30, alpha=0.7, color='blue')\n",
    "plt.title(\"Log-Normal Distribution\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(data_exp, bins=30, alpha=0.7, color='green')\n",
    "plt.title(\"Exponential Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# ç»˜åˆ¶Q-Qå›¾\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "stats.probplot(np.log(data_lognorm), dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot of Log of Log-Normal Data\")\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(data_exp, dist=\"expon\", plot=plt)\n",
    "plt.title(\"Q-Q Plot of Exponential Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212bcb7b",
   "metadata": {},
   "source": [
    "ï¼ˆäºŒï¼‰æ ‡å‡†åŒ– "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528aeb73",
   "metadata": {},
   "source": [
    "1. æ ‡å‡†åŒ–å®šä¹‰    \n",
    "è°ƒæ•´æ•°æ®çš„ç‰¹å¾ä½¿ä¹‹å…·æœ‰é›¶å‡å€¼å’Œå•ä½æ–¹å·®ï¼Œå½¢æˆâ€œæ ‡å‡†æ­£æ€â€åˆ†å¸ƒçš„ç‰¹æ€§ï¼Œä½¿å…¶å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5816f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³•ä¸€ Zå¾—åˆ†å½’ä¸€åŒ–ï¼ˆStandardizationï¼‰:\n",
    "# ç›®çš„ï¼šå°†æ•°æ®çš„å‡å€¼è½¬æ¢ä¸º 0ï¼Œæ ‡å‡†å·®è½¬æ¢ä¸º 1ã€‚\n",
    "# é€‚ç”¨æƒ…å†µï¼šæ•°æ®éµå¾ªæ­£æ€åˆ†å¸ƒï¼›åœ¨ä¸çŸ¥é“æ•°æ®çš„æœ€å°å€¼å’Œæœ€å¤§å€¼çš„æƒ…å†µä¸‹ï¼Œä¹Ÿå³å¯¹äºå¼‚å¸¸å€¼ä¸æ•æ„Ÿ\n",
    "x_list = [50,75,100,200]\n",
    "import numpy as np\n",
    "x_array = np.array(x_list)\n",
    "x_preprocess = [(x - np.mean(x_array))/np.std(x_array) for x in x_list]\n",
    "print(f'Zå¾—åˆ†å½’ä¸€åŒ–é¢„å¤„ç†åçš„æ•°å€¼: {x_preprocess}')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# ç¤ºä¾‹æ•°æ®\n",
    "X = np.array([[1.0, -1.0, 2.0],\n",
    "              [2.0, 0.0, 0.0],\n",
    "              [0.0, 1.0, -1.0]])\n",
    "\n",
    "# åˆ›å»ºæ ‡å‡†åŒ–è½¬æ¢å™¨å¯¹è±¡\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# è®­ç»ƒæ ‡å‡†åŒ–å‚æ•°å¹¶è½¬æ¢æ•°æ®\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# æ˜¾ç¤ºæ ‡å‡†åŒ–åçš„æ•°æ®\n",
    "print(\"Standardized Data:\\n\", X_scaled)\n",
    "\n",
    "# æ£€æŸ¥å‡å€¼å’Œæ ‡å‡†å·®\n",
    "print(\"Mean of each feature after scaling:\", X_scaled.mean(axis=0))\n",
    "print(\"Std deviation of each feature after scaling:\", X_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a5508",
   "metadata": {},
   "source": [
    "ï¼ˆä¸‰ï¼‰æ­£åˆ™åŒ–   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae521783",
   "metadata": {},
   "source": [
    "1. æ­£åˆ™åŒ–å®šä¹‰     \n",
    "åœ¨æ•°æ®é¢„å¤„ç†ä¸­ï¼ŒL1å’ŒL2å¸¸å¸¸æŒ‡çš„æ˜¯L1èŒƒæ•°å’ŒL2èŒƒæ•°çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å¾ç¼©æ”¾ï¼ˆscalingï¼‰å’Œå½’ä¸€åŒ–ï¼ˆnormalizationï¼‰çš„è¿‡ç¨‹ä¸­ã€‚è¿™äº›æ–¹æ³•ç”¨äºè°ƒæ•´æ•°æ®ç‰¹å¾çš„æ¯”ä¾‹ï¼Œä»¥æé«˜ç®—æ³•çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed019337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³•ä¸€ å•ä½å‘é‡å½’ä¸€åŒ–ï¼ˆL2 Normalizationï¼‰:\n",
    "# ç›®çš„ï¼šå°†ç‰¹å¾å‘é‡çš„é•¿åº¦ç¼©æ”¾è‡³ 1ã€‚\n",
    "# é€‚ç”¨æƒ…å†µï¼šåœ¨å¤„ç†æ–‡æœ¬æ•°æ®æˆ–éœ€è¦è®¡ç®—å‘é‡ç©ºé—´ä¸­çš„ç›¸ä¼¼åº¦æ—¶ã€‚\n",
    "x_list = [50,75,100,200]\n",
    "x_array = np.array(x_list)\n",
    "l2_norm = np.linalg.norm(x_array)\n",
    "x_preprocess = x_array / l2_norm\n",
    "print(\"å•ä½å‘é‡å½’ä¸€åŒ–é¢„å¤„ç†åçš„æ•°å€¼:\", x_preprocess)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ•°æ®é›†\n",
    "data = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "# åˆå§‹åŒ– Normalizer å¯¹è±¡\n",
    "normalizer = Normalizer(norm='l2')\n",
    "\n",
    "# ä½¿ç”¨ fit_transform æ–¹æ³•å½’ä¸€åŒ–æ•°æ®\n",
    "normalized_data = normalizer.fit_transform(data)\n",
    "\n",
    "print(\"åŸå§‹æ•°æ®:\")\n",
    "print(data)\n",
    "print(\"å½’ä¸€åŒ–åçš„æ•°æ®:\")\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da179ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é™„å½•å±•ç¤ºæ­£æ€åˆ†å¸ƒçš„æ£€éªŒæ–¹å¼\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# ç”Ÿæˆä¸€ä¸ªæ­£æ€åˆ†å¸ƒæ ·æœ¬æ•°æ®\n",
    "data = np.random.normal(loc=0, scale=1, size=1000)\n",
    "\n",
    "# ç»˜åˆ¶ç›´æ–¹å›¾\n",
    "plt.hist(data, bins=30, alpha=0.75, color='blue')\n",
    "plt.title('Histogram')\n",
    "plt.show()\n",
    "\n",
    "# ç»˜åˆ¶Q-Qå›¾\n",
    "stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot')\n",
    "plt.show()\n",
    "\n",
    "# è¿›è¡ŒShapiro-Wilkæ£€éªŒ\n",
    "shapiro_test = stats.shapiro(data)\n",
    "print(f\"Shapiro-Wilk Test: Statistic={shapiro_test[0]:.4f}, p-value={shapiro_test[1]:.4f}\") # è¾“å‡ºä¸­çš„ç»Ÿè®¡é‡è¶Šæ¥è¿‘1ï¼Œpå€¼è¶Šå¤§ï¼Œè¶Šä¸æ‹’ç»æ•°æ®æ¥è‡ªæ­£æ€åˆ†å¸ƒçš„åŸå‡è®¾ã€‚\n",
    "\n",
    "# è¿›è¡ŒKolmogorov-Smirnovæ£€éªŒï¼ˆå‡è®¾ä¸ºæ­£æ€ï¼‰\n",
    "ks_test = stats.kstest(data, 'norm', args=(np.mean(data), np.std(data)))\n",
    "print(f\"Kolmogorov-Smirnov Test: Statistic={ks_test[0]:.4f}, p-value={ks_test[1]:.4f}\") # æ­¤æ£€éªŒæ¯”è¾ƒå®é™…æ•°æ®ä¸æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚på€¼è¾ƒå¤§æ—¶ï¼Œæˆ‘ä»¬æ¥å—æ•°æ®ç¬¦åˆæ­£æ€åˆ†å¸ƒçš„å‡è®¾ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37606b",
   "metadata": {},
   "source": [
    "ï¼ˆå››ï¼‰ç®€æ˜“æ¨èä½¿ç”¨å“ªç§æ•°æ®é¢„å¤„ç†æ–¹å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc13580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_dataset(dataset):\n",
    "    # æ£€æŸ¥æ•°æ®ç±»å‹\n",
    "    if not isinstance(dataset, pd.Series) and not isinstance(dataset, np.ndarray):\n",
    "        return \"Dataset should be a pandas Series or numpy array.\"\n",
    "    \n",
    "    # å¼ºåˆ¶è½¬æ¢ä¸º numpy array è¿›è¡Œå¤„ç†\n",
    "    data = np.array(dataset).flatten()\n",
    "    \n",
    "    # æ£€æµ‹ç©ºå€¼\n",
    "    if np.any(np.isnan(data)):\n",
    "        return \"Data contains NaN values. Please handle them before analysis.\"\n",
    "\n",
    "    # åŸºæœ¬æè¿°ç»Ÿè®¡\n",
    "    descriptive_stats = {\n",
    "        'mean': np.mean(data),\n",
    "        'std': np.std(data),\n",
    "        'min': np.min(data),\n",
    "        'max': np.max(data),\n",
    "        'median': np.median(data),\n",
    "        'skewness': stats.skew(data),\n",
    "        'kurtosis': stats.kurtosis(data)\n",
    "    }\n",
    "    \n",
    "    # æ£€æµ‹æ­£æ€åˆ†å¸ƒ\n",
    "    k2, p_norm = stats.normaltest(data)\n",
    "    is_normal = p_norm > 0.05\n",
    "    \n",
    "    # æ£€æµ‹æŒ‡æ•°åˆ†å¸ƒ\n",
    "    _, p_exp = stats.kstest(data, 'expon', args=(np.min(data), np.mean(data)))\n",
    "    is_exponential = p_exp > 0.05\n",
    "    \n",
    "    # æ£€æµ‹å‡åŒ€åˆ†å¸ƒ\n",
    "    _, p_uniform = stats.kstest(data, 'uniform', args=(np.min(data), np.max(data)-np.min(data)))\n",
    "    is_uniform = p_uniform > 0.05\n",
    "    \n",
    "    # æ£€æµ‹å¼‚å¸¸å€¼\n",
    "    q75, q25 = np.percentile(data, [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "    lower_bound = q25 - 1.5 * iqr\n",
    "    upper_bound = q75 + 1.5 * iqr\n",
    "    outliers = np.where((data < lower_bound) | (data > upper_bound))\n",
    "    has_outliers = len(outliers[0]) > 0\n",
    "    \n",
    "    # æ¨èé¢„å¤„ç†æ–¹æ³•\n",
    "    if has_outliers:\n",
    "        preprocess_recommendation = \"Consider using robust scaling due to outliers.\"\n",
    "    elif is_normal:\n",
    "        preprocess_recommendation = \"Data seems normal. Standard scaling is recommended.\"\n",
    "    elif is_exponential or descriptive_stats['std'] > descriptive_stats['mean']:\n",
    "        preprocess_recommendation = \"Data seems exponential or skewed. Log transformation and MinMax scaling are recommended.\"\n",
    "    else:\n",
    "        preprocess_recommendation = \"MinMax scaling can be used.\"\n",
    "    \n",
    "    # ä¸šåŠ¡é¢†åŸŸæ¨è\n",
    "    if is_normal:\n",
    "        business_context = \"Likely applications: Any domain requiring normality assumptions (e.g., many statistical tests, Gaussian processes).\"\n",
    "    elif is_exponential:\n",
    "        business_context = \"Likely applications: Quality control, survival analysis, risk assessment.\"\n",
    "    elif is_uniform:\n",
    "        business_context = \"Likely applications: Simulations where uniform distribution is expected.\"\n",
    "    elif has_outliers:\n",
    "        business_context = \"Likely applications: Financial transactions, social media metrics, real estate prices.\"\n",
    "    else:\n",
    "        business_context = \"General data processing.\"\n",
    "    \n",
    "    return {\n",
    "        'Descriptive Statistics': descriptive_stats,\n",
    "        'Is Normal': is_normal,\n",
    "        'Is Exponential': is_exponential,\n",
    "        'Is Uniform': is_uniform,\n",
    "        'Has Outliers': has_outliers,\n",
    "        'Preprocess Recommendation': preprocess_recommendation,\n",
    "        'Business Context': business_context\n",
    "    }\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "data = pd.Series(np.random.lognormal(mean=0, sigma=1, size=1000))\n",
    "analysis_results = analyze_dataset(data)\n",
    "analysis_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f6b33",
   "metadata": {},
   "source": [
    "### æœºå™¨å­¦ä¹ \n",
    "###### ä¸‰ã€ç‰¹å¾é€‰æ‹©\n",
    "ï¼ˆä¸€ï¼‰å¡æ–¹æ ¡éªŒï¼ˆäºŒï¼‰tæ£€éªŒ ï¼ˆäº”ï¼‰ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ ï¼ˆå…­ï¼‰çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d11a4",
   "metadata": {},
   "source": [
    "ï¼ˆé›¶ï¼‰ä½•ä¸ºç‰¹å¾é€‰æ‹©                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eea5cd",
   "metadata": {},
   "source": [
    "å¤æ•°çš„ç‰¹å¾ æ¯ä¸ªç‰¹å¾æ˜¯ä¸€ä¸ªxï¼Œç†è®ºä¸Šä¼šæœ‰æ— æ•°çš„xi æ¯ä¸ªxiéƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„yj ç›®æ ‡å€¼        \n",
    "æˆ‘ä»¬éœ€è¦è®¨è®ºçš„é—®é¢˜æ˜¯ x å’Œ yä¸¤ä¸ªå˜é‡ä¹‹é—´æ˜¯å¦å­˜åœ¨å¼ºå…³è” è¶Šå…³è”çš„ç‰¹å¾æ‰åº”è¯¥è¢«é€‰æ‹©ä¸ºæ¨¡å‹çš„ç‰¹å¾ ä¸ç„¶å®¹æ˜“è¿‡æ‹Ÿåˆ    \n",
    "æ‰€ä»¥æˆ‘ä»¬éœ€è¦æŸç§è®¡ç®—æ–¹å¼æŒ‡å¯¼æˆ‘ä»¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b4975",
   "metadata": {},
   "source": [
    "å¡æ–¹æ£€éªŒï¼šåˆ†ç±»ä»»åŠ¡ XYç¦»æ•£å‹     \n",
    "é€‚ç”¨åœºæ™¯ï¼š   \n",
    "ç‹¬ç«‹æ€§æ£€éªŒï¼šæ£€éªŒä¸¤ä¸ªåˆ†ç±»å˜é‡ä¹‹é—´æ˜¯å¦å­˜åœ¨å…³è”ã€‚   \n",
    "æ‹Ÿåˆä¼˜åº¦æ£€éªŒï¼šæ£€éªŒè§‚æµ‹æ•°æ®ä¸ç†è®ºåˆ†å¸ƒä¹‹é—´çš„æ‹Ÿåˆç¨‹åº¦ã€‚    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb08e9",
   "metadata": {},
   "source": [
    "t æ£€éªŒï¼šå›å½’ä»»åŠ¡ XYè¿ç»­å‹ æ£€éªŒå‚æ•°æ˜¯å¦æ˜¾è‘—                 \n",
    "ä½¿ç”¨åœºæ™¯ï¼š\n",
    "å•æ ·æœ¬ t æ£€éªŒï¼šæ£€éªŒæ ·æœ¬å‡å€¼ä¸å·²çŸ¥æ€»ä½“å‡å€¼çš„å·®å¼‚ã€‚          \n",
    "åŒæ ·æœ¬ t æ£€éªŒï¼šæ£€éªŒä¸¤ä¸ªç‹¬ç«‹æ ·æœ¬å‡å€¼ä¹‹é—´çš„å·®å¼‚ã€‚           \n",
    "é…å¯¹æ ·æœ¬ t æ£€éªŒï¼šæ£€éªŒæˆå¯¹æ ·æœ¬ï¼ˆå¦‚å®éªŒå‰åæµ‹é‡ï¼‰å‡å€¼ä¹‹é—´çš„å·®å¼‚ã€‚             \n",
    "çº¿æ€§å›å½’ä¸­çš„ t æ£€éªŒï¼šæ£€éªŒå›å½’ç³»æ•°æ˜¯å¦æ˜¾è‘—ã€‚            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1478bd",
   "metadata": {},
   "source": [
    "ï¼ˆä¸€ï¼‰å¡æ–¹æ ¡éªŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779ebd5",
   "metadata": {},
   "source": [
    "1. å¡æ–¹æ£€éªŒåŸç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d18cd",
   "metadata": {},
   "source": [
    "å¡æ–¹æ£€éªŒåŸºäºç‹¬ç«‹æ€§å‡è®¾ï¼Œç‹¬ç«‹æ€§å‡è®¾å‡å®šå½“ä¸¤ä¸ªå˜é‡ä¸å­˜åœ¨ç›¸å…³æ€§çš„æ—¶å€™ ä»–ä»¬åŒæ—¶å‘ç”Ÿçš„æ¦‚ç‡= xå•ç‹¬å‘ç”Ÿçš„æ¦‚ç‡ * yå•ç‹¬å‘ç”Ÿçš„æ¦‚ç‡    \n",
    "å…¬å¼è¡¨ç°ä¸ºï¼šP(X=Xi and Y=Yj)=P(X=Xi)Ã—P(Y=Yj)   \n",
    "å…¬å¼è§£é‡Š  ï¼šP ä»£è¡¨æ¦‚ç‡ ç®€å•ç†è§£ä¸º Xiå‡ºç°çš„æ¦‚ç‡= Xiå‡ºç°çš„æ ·æœ¬æ•°/æ ·æœ¬æ€»æ•°N ï¼› Nåˆ™ä»£è¡¨æ ·æœ¬æ€»æ•°            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a42dc",
   "metadata": {},
   "source": [
    "2. è§‚å¯Ÿé¢‘æ•°ï¼ˆObserved Frequencyï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66192cc7",
   "metadata": {},
   "source": [
    "ä»»æ„ä¸€ä¸ªåˆ†ç±»çš„æ•°æ®è¡¨ å¯ä»¥é€šè¿‡åˆ—è”è¡¨çš„æ–¹å¼ åšå‡ºä¸€ä¸ªxå€¼ä¸ºiæ—¶ yå€¼ä¸ºjå‡ºç°çš„é¢‘æ¬¡ è¿™æ˜¯ç”±æ ·æœ¬æä¾›çš„çœŸå®çš„é¢‘åº¦å€¼ è¢«æˆ‘ä»¬ç›´æ¥è§‚å¯Ÿåˆ° æ•…è€Œå‘½åè§‚å¯Ÿé¢‘åº¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c5da7",
   "metadata": {},
   "source": [
    "3. æœŸæœ›é¢‘æ•°ï¼ˆExpected Frequencyï¼‰æ ¹æ®ç‹¬ç«‹æ€§å‡è®¾è®¡ç®—å¾—åˆ°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1134f",
   "metadata": {},
   "source": [
    "åŸºäºç‹¬ç«‹æ€§å‡è®¾å¯ä»¥é€šè¿‡å…¬å¼çš„å˜æ¢ç®—å‡ºxiyjçš„ä¸ç›¸å…³çš„æ—¶å€™çš„æœŸæœ›é¢‘åº¦å€¼              \n",
    "**æ­¥éª¤ä¸€** è®¤çŸ¥P(X=Xi and Y=Yj)= Eij/N          \n",
    "Eijä»£è¡¨ xiyjæ—¶å‡ºç°çš„æœŸæœ›é¢‘æ¬¡ / è”åˆæ€»æ ·æœ¬æ•° å³å¯ç®—å‡º Pï¼ˆX=Xi and Y=Yjï¼‰          \n",
    "å› æ­¤å…¬å¼æ¨å¯¼å¦‚ä¸‹             \n",
    "P(X=Xi and Y=Yj)= Eij/N =P(X=Xi)Ã—P(Y=Yj)             \n",
    "Eij = P(X=Xi)Ã—P(Y=Yj) * N           \n",
    " \n",
    "**æ­¥éª¤äºŒ**          \n",
    "åŒç†P(X=Xi) = Xiå‡ºç°çš„é¢‘æ¬¡/Xçš„æ€»æ ·æœ¬æ•°  =Næ ·æœ¬æ•°          \n",
    "   P(Y=Yj) = Yjå‡ºç°çš„é¢‘æ¬¡/Yçš„æ€»æ ·æœ¬æ•°  =Næ ·æœ¬æ•°        \n",
    "   \n",
    "å¦‚ä¸‹å›¾çš„è§‚å¯Ÿé¢‘åº¦æ‰€ç¤º å°±æ˜¯è¡Œåˆ—æ€»è®¡    \n",
    "Xiå‡ºç°çš„é¢‘æ¬¡ = Ri   \n",
    "Yjå‡ºç°çš„é¢‘æ¬¡ = Ci    \n",
    "å› æ­¤å…¬å¼æ¨å¯¼å¦‚ä¸‹   \n",
    "Eij = Ri/NÃ—Cj/N * N    \n",
    "\n",
    "**æ­¥éª¤ä¸‰**      \n",
    "æœ€ç»ˆæŒ‰ç…§æ•°å­¦çŸ¥è¯†ï¼Œç®€åŒ–ç®—å¼     \n",
    "Eij = Ri/NÃ—Cj/N * N = Ri*Cj/N    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6959855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "image_path = r\"D:\\1-script\\3-PYTHON\\imageå­˜å‚¨jupyteråœ¨ç”¨çš„å›¾ç‰‡å­˜æ”¾äºæ­¤\\åˆ—è”è¡¨ï¼ˆè§‚å¯Ÿé¢‘åº¦ï¼‰.jpg\"\n",
    "display(Image(filename = image_path,width=100, height=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723fe0e",
   "metadata": {},
   "source": [
    "4. å¡æ–¹å€¼çš„æµ‹ç®—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196b1ce",
   "metadata": {},
   "source": [
    "é€šè¿‡å®é™…æ ·æœ¬æ‹¿åˆ°è§‚å¯Ÿé¢‘æ•°             \n",
    "é€šè¿‡ç‹¬ç«‹æ€§å‡è®¾è·å–æœŸæœ›é¢‘æ•°              \n",
    "å¦‚æœä¸¤è€…çš„å·®å¼‚è¶Šå¤§ åˆ™ä»£è¡¨ç€ç°å®äºå‡è®¾å·®è·ç”šè¿œ ä»£è¡¨ç€ä¸¤ç§å¹¶ä¸æ˜¯å‡è®¾çš„æ— å…³ \n",
    "\n",
    "æ¥ä¸‹æ¥å°±æ˜¯é‡åŒ–ä¸¤è€…çš„å·®å¼‚ è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š          \n",
    "å…¬å¼   ï¼š    \n",
    "å¸Œè…Šå­—æ¯ chiÏ‡Â² å³å¡æ–¹å€¼/å¡æ–¹ç»Ÿè®¡é‡ =âˆ‘ (0i - Ei)Â² / Ei       \n",
    "å…¬å¼è§£é‡Šï¼š       \n",
    "âˆ‘ä»£è¡¨æ±‚å’Œ å³æ‰€æœ‰xyçš„è§‚å¯Ÿé¢‘æ¬¡å’ŒæœŸæœ›é¢‘æ¬¡çš„å·®å¼‚çš„å¹³æ–¹æ€»å’Œ        \n",
    "Â²æ˜¯ä¸ºäº†ç¡®ä¿ æ— è®ºæ˜¯å¤§äºå·®å¼‚è¿˜æ˜¯å°äºå·®å¼‚éƒ½è¢«ç®—ä½œå·®å¼‚ è€Œéäº’ç›¸æŠµæ¶ˆ     \n",
    "/Eiæ˜¯ä¸ºäº†ä¿è¯ ä¸åŒé¢‘æ•°çš„è´¡çŒ®æ˜¯ç›¸å¯¹çš„ ä½¿å¾—æ•´ä½“å¡æ–¹å€¼å…·æœ‰ä¸€è‡´çš„å°ºåº¦ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbff3d6",
   "metadata": {},
   "source": [
    "5. å¡æ–¹åˆ†å¸ƒ ä¸ æ˜¾è‘—æ€§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe07551",
   "metadata": {},
   "source": [
    "é€šè¿‡ä¸Šè¿°å‡è®¾ å’Œå…¬å¼æ¨å¯¼ æˆ‘ä»¬å°†å˜é‡xå’Œyä¹‹é—´æ˜¯å¦å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§çš„é—®é¢˜è½¬åŒ–ä¸ºäº†ä¸€ä¸ª å¡æ–¹å€¼ ï¼Œç°åœ¨éœ€è¦è§£å†³çš„æ˜¯å¡æ–¹å€¼å¤„äºå¤šå°‘æ‰èƒ½ç®—æ˜¯æ˜¾è‘—    \n",
    "ä¸ºäº†ç†è§£è¿™ä¸€ç‚¹ å†æ¬¡å¼•å…¥æ¦‚ç‡è®ºçš„å¡æ–¹åˆ†å¸ƒçŸ¥è¯†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88bb6e4",
   "metadata": {},
   "source": [
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce7dd6",
   "metadata": {},
   "source": [
    "å¡æ–¹åˆ†å¸ƒæ˜¯ä¸€ç§å¸¸è§çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¿æ³›åº”ç”¨äºå‡è®¾æ£€éªŒä¸­ åŒæ—¶æ­£æ€åˆ†å¸ƒçš„å¹³æ–¹ = å¡æ–¹åˆ†å¸ƒã€‚ ä¸ºäº†ç¡®å®šå¡æ–¹å€¼æ˜¯å¦æ˜¾è‘—ï¼Œæˆ‘ä»¬éœ€è¦ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š       \n",
    "ï¼ˆ1ï¼‰è®¡ç®—å¡æ–¹ç»Ÿè®¡é‡      \n",
    "ï¼ˆ2ï¼‰ç¡®å®šè‡ªç”±åº¦          \n",
    "ï¼ˆ3ï¼‰é€‰æ‹©æ˜¾è‘—æ€§æ°´å¹³ï¼ˆğ›¼ï¼‰ï¼šé€šå¸¸é€‰æ‹© 0.05ï¼ˆ95% ç½®ä¿¡æ°´å¹³ï¼‰ï¼Œä¹Ÿå¯ä»¥é€‰æ‹© 0.01 æˆ– 0.10 ç­‰å…¶ä»–æ°´å¹³       \n",
    "ï¼ˆ4ï¼‰æŸ¥æ‰¾ä¸´ç•Œå€¼ï¼šåœ¨å¡æ–¹åˆ†å¸ƒè¡¨ä¸­æŸ¥æ‰¾å¯¹åº”è‡ªç”±åº¦å’Œæ˜¾è‘—æ€§æ°´å¹³çš„ä¸´ç•Œå€¼ï¼ˆcritical valueï¼‰ã€‚       \n",
    "ï¼ˆ5ï¼‰æ¯”è¾ƒå¡æ–¹ç»Ÿè®¡é‡ä¸ä¸´ç•Œå€¼ï¼šå¦‚æœå¡æ–¹ç»Ÿè®¡é‡å¤§äºä¸´ç•Œå€¼ï¼Œåˆ™æ‹’ç»é›¶å‡è®¾ï¼Œè®¤ä¸ºè§‚å¯Ÿé¢‘æ•°å’ŒæœŸæœ›é¢‘æ•°ä¹‹é—´çš„å·®å¼‚æ˜¯æ˜¾è‘—çš„ã€‚         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db36eb",
   "metadata": {},
   "source": [
    "å› ä¸ºè‡ªç”±åº¦çš„æå‡ï¼ŒåŒæ ·çš„æ˜¾è‘—æ€§æ°´å¹³è¦æ±‚çš„ä¸´ç•Œå€¼ä¼šæ›´é«˜ ï¼Œåä¹‹å½“è‡ªç”±åº¦è¿‡å° å¾€å¾€å‘ç”Ÿåœ¨è¾ƒå°çš„æ ·æœ¬æ—¶ ä¼šå¯¼è‡´å¡æ–¹å€¼æ›´å®¹æ˜“è¶…è¿‡ä¸´ç•Œå€¼ å‘ˆç°å‡é˜³æ€§\n",
    "æ•…è€Œè¿›ä¸€æ­¥å¼•å…¥   \n",
    "Yatesä¿®æ­£ï¼ˆYates' correctionï¼‰    \n",
    "è€ƒè™‘ä½¿ç”¨å…¶ä»–éå‚æ•°æ£€éªŒæ–¹æ³•ï¼Œå¦‚Fisherç¡®åˆ‡æ£€éªŒï¼ˆFisher's exact testï¼‰ï¼Œå®ƒåœ¨å°æ ·æœ¬æƒ…å†µä¸‹æ›´ä¸ºåˆé€‚ã€‚   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86afdd3",
   "metadata": {},
   "source": [
    "6. PYTHON è¯­å¥ä½¿ç”¨ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41362030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 ä½¿ç”¨ç»Ÿè®¡åº“ ç†è§£è®¡ç®—å•ä¸ªxå’Œyçš„å…³ç³»\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# åˆ›å»ºåˆ—è”è¡¨\n",
    "data = np.array([[10, 20], [30, 40]])\n",
    "\n",
    "# æ‰§è¡Œå¡æ–¹æ£€éªŒ\n",
    "chi2, p, dof, expected = chi2_contingency(data)\n",
    "\n",
    "# è®¾å®šæ˜¾è‘—æ€§æ°´å¹³\n",
    "alpha = 0.05\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print(f\"Chi-Square Statistic: {chi2}\")\n",
    "print(f\"P-Value: {p}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")\n",
    "print(\"Expected Frequencies:\")\n",
    "print(expected)\n",
    "\n",
    "# åˆ¤æ–­ç»“æœ å½“ xä¸å–æŸä¸ªå€¼æ—¶ï¼Œè§‚å¯Ÿåˆ°yå€¼çš„æ¦‚ç‡å°äºè®¾å®šçš„æ˜¾è‘—æ€§æ°´å¹³  å°±ä»£è¡¨ ä¸å–xä¸ºæŸå€¼ åˆ™yä½äºpçš„æ¦‚ç‡å‡ºç°å€¼ è¿™ä»£è¡¨å¯ä»¥æ¨ç¿»åŸå§‹å‡è®¾\n",
    "if p <= alpha:\n",
    "    print(\"æ‹’ç»åŸå‡è®¾ï¼Œç»“æœå…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚\")\n",
    "else:\n",
    "    print(\"ä¸æ‹’ç»åŸå‡è®¾ï¼Œç»“æœä¸å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "odf = pd.crosstab(data.data[:,0],data.target)\n",
    "dof = (odf.shape[0] - 1) * (odf.shape[1] - 1)\n",
    "row = odf.sum(axis = 1)\n",
    "column = odf.sum(axis = 0)\n",
    "n = odf.sum().sum()\n",
    "\n",
    "import numpy as np\n",
    "edf = np.zeros(odf.shape)\n",
    "for i in range(0,odf.shape[0]):\n",
    "    for j in range(0,odf.shape[1]):\n",
    "        edf[i,j] = row.iloc[i]*column.iloc[j]/n\n",
    "        \n",
    "chi = ((odf-edf)**2/edf).sum().sum()\n",
    "chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da5e08b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰‹åŠ¨è®¡ç®—çš„å¡æ–¹ç»Ÿè®¡é‡: 711.1902865100636\n",
      "æ‰‹åŠ¨è®¡ç®—çš„På€¼: 2.253773920299271e-14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# å‡è®¾ç‰¹å¾æ˜¯éè´Ÿçš„ï¼ˆå¡æ–¹æ ¡éªŒè¦æ±‚ï¼‰\n",
    "X = np.abs(X)\n",
    "\n",
    "# ä½¿ç”¨ KBinsDiscretizer å¯¹ç‰¹å¾è¿›è¡Œç¦»æ•£åŒ–å¤„ç†\n",
    "binning = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "X_binned = binning.fit_transform(X)\n",
    "\n",
    "\n",
    "# è®¡ç®—åˆ—è”è¡¨å’Œå¡æ–¹ç»Ÿè®¡é‡\n",
    "chi2_values,p,dof,expected = chi2_contingency(X_binned, y)\n",
    "# æ‰“å°ç»“æœ\n",
    "print(\"æ‰‹åŠ¨è®¡ç®—çš„å¡æ–¹ç»Ÿè®¡é‡:\", chi2_values)\n",
    "print(\"æ‰‹åŠ¨è®¡ç®—çš„På€¼:\", p_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b214ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¦»æ•£åŒ–åçš„ç‰¹å¾:\n",
      " [[2. 6. 0. 0.]\n",
      " [1. 4. 0. 0.]\n",
      " [1. 5. 0. 0.]\n",
      " [0. 4. 0. 0.]\n",
      " [1. 6. 0. 0.]\n",
      " [3. 7. 1. 1.]\n",
      " [0. 5. 0. 0.]\n",
      " [1. 5. 0. 0.]\n",
      " [0. 3. 0. 0.]\n",
      " [1. 4. 0. 0.]\n",
      " [3. 7. 0. 0.]\n",
      " [1. 5. 1. 0.]\n",
      " [1. 4. 0. 0.]\n",
      " [0. 4. 0. 0.]\n",
      " [4. 8. 0. 0.]\n",
      " [3. 9. 0. 1.]\n",
      " [3. 7. 0. 1.]\n",
      " [2. 6. 0. 0.]\n",
      " [3. 7. 1. 0.]\n",
      " [2. 7. 0. 0.]\n",
      " [3. 5. 1. 0.]\n",
      " [2. 7. 0. 1.]\n",
      " [0. 6. 0. 0.]\n",
      " [2. 5. 1. 1.]\n",
      " [1. 5. 1. 0.]\n",
      " [1. 4. 1. 0.]\n",
      " [1. 5. 1. 1.]\n",
      " [2. 6. 0. 0.]\n",
      " [2. 5. 0. 0.]\n",
      " [1. 5. 1. 0.]\n",
      " [1. 4. 1. 0.]\n",
      " [3. 5. 0. 1.]\n",
      " [2. 8. 0. 0.]\n",
      " [3. 9. 0. 0.]\n",
      " [1. 4. 0. 0.]\n",
      " [1. 5. 0. 0.]\n",
      " [3. 6. 0. 0.]\n",
      " [1. 6. 0. 0.]\n",
      " [0. 4. 0. 0.]\n",
      " [2. 5. 0. 0.]\n",
      " [1. 6. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 5. 0. 0.]\n",
      " [1. 6. 1. 2.]\n",
      " [2. 7. 1. 1.]\n",
      " [1. 4. 0. 0.]\n",
      " [2. 7. 1. 0.]\n",
      " [0. 5. 0. 0.]\n",
      " [2. 7. 0. 0.]\n",
      " [1. 5. 0. 0.]\n",
      " [7. 5. 6. 5.]\n",
      " [5. 5. 5. 5.]\n",
      " [7. 4. 6. 5.]\n",
      " [3. 1. 5. 5.]\n",
      " [6. 3. 6. 5.]\n",
      " [3. 3. 5. 5.]\n",
      " [5. 5. 6. 6.]\n",
      " [1. 1. 3. 3.]\n",
      " [6. 3. 6. 5.]\n",
      " [2. 2. 4. 5.]\n",
      " [1. 0. 4. 3.]\n",
      " [4. 4. 5. 5.]\n",
      " [4. 0. 5. 3.]\n",
      " [5. 3. 6. 5.]\n",
      " [3. 3. 4. 5.]\n",
      " [6. 4. 5. 5.]\n",
      " [3. 4. 5. 5.]\n",
      " [4. 2. 5. 3.]\n",
      " [5. 0. 5. 5.]\n",
      " [3. 2. 4. 4.]\n",
      " [4. 5. 6. 7.]\n",
      " [5. 3. 5. 5.]\n",
      " [5. 2. 6. 5.]\n",
      " [5. 3. 6. 4.]\n",
      " [5. 3. 5. 5.]\n",
      " [6. 4. 5. 5.]\n",
      " [6. 3. 6. 5.]\n",
      " [6. 4. 6. 6.]\n",
      " [4. 3. 5. 5.]\n",
      " [3. 2. 4. 3.]\n",
      " [3. 1. 4. 4.]\n",
      " [3. 1. 4. 3.]\n",
      " [4. 2. 4. 4.]\n",
      " [4. 2. 6. 6.]\n",
      " [3. 4. 5. 5.]\n",
      " [4. 5. 5. 6.]\n",
      " [6. 4. 6. 5.]\n",
      " [5. 1. 5. 5.]\n",
      " [3. 4. 5. 5.]\n",
      " [3. 2. 5. 5.]\n",
      " [3. 2. 5. 4.]\n",
      " [5. 4. 6. 5.]\n",
      " [4. 2. 5. 4.]\n",
      " [1. 1. 3. 3.]\n",
      " [3. 2. 5. 5.]\n",
      " [3. 4. 5. 4.]\n",
      " [3. 3. 5. 5.]\n",
      " [5. 3. 5. 5.]\n",
      " [2. 2. 3. 4.]\n",
      " [3. 3. 5. 5.]\n",
      " [5. 5. 8. 9.]\n",
      " [4. 2. 6. 7.]\n",
      " [7. 4. 8. 8.]\n",
      " [5. 3. 7. 7.]\n",
      " [6. 4. 8. 8.]\n",
      " [9. 4. 9. 8.]\n",
      " [1. 2. 5. 6.]\n",
      " [8. 3. 8. 7.]\n",
      " [6. 2. 8. 7.]\n",
      " [8. 6. 8. 9.]\n",
      " [6. 5. 6. 7.]\n",
      " [5. 2. 7. 7.]\n",
      " [6. 4. 7. 8.]\n",
      " [3. 2. 6. 7.]\n",
      " [4. 3. 6. 9.]\n",
      " [5. 5. 7. 9.]\n",
      " [6. 4. 7. 7.]\n",
      " [9. 7. 9. 8.]\n",
      " [9. 2. 9. 9.]\n",
      " [4. 0. 6. 5.]\n",
      " [7. 5. 7. 9.]\n",
      " [3. 3. 6. 7.]\n",
      " [9. 3. 9. 7.]\n",
      " [5. 2. 6. 7.]\n",
      " [6. 5. 7. 8.]\n",
      " [8. 5. 8. 7.]\n",
      " [5. 3. 6. 7.]\n",
      " [5. 4. 6. 7.]\n",
      " [5. 3. 7. 8.]\n",
      " [8. 4. 8. 6.]\n",
      " [8. 3. 8. 7.]\n",
      " [9. 7. 9. 7.]\n",
      " [5. 3. 7. 8.]\n",
      " [5. 3. 6. 5.]\n",
      " [5. 2. 7. 5.]\n",
      " [9. 4. 8. 9.]\n",
      " [5. 5. 7. 9.]\n",
      " [5. 4. 7. 7.]\n",
      " [4. 4. 6. 7.]\n",
      " [7. 4. 7. 8.]\n",
      " [6. 4. 7. 9.]\n",
      " [7. 4. 6. 9.]\n",
      " [4. 2. 6. 7.]\n",
      " [6. 5. 8. 9.]\n",
      " [6. 5. 7. 9.]\n",
      " [6. 4. 7. 9.]\n",
      " [5. 2. 6. 7.]\n",
      " [6. 4. 7. 7.]\n",
      " [5. 5. 7. 9.]\n",
      " [4. 4. 6. 7.]]\n",
      "åŸå§‹ç‰¹å¾å½¢çŠ¶: (150, 4)\n",
      "é€‰æ‹©åçš„ç‰¹å¾å½¢çŠ¶: (150, 2)\n",
      "é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•: [2 3]\n",
      "å¡æ–¹ç»Ÿè®¡é‡: [128.50526316  48.85427136 297.02755267 332.84430177]\n",
      "På€¼: [1.24576653e-28 2.46280352e-11 3.17165872e-65 5.29393013e-73]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# å‡è®¾ç‰¹å¾æ˜¯éè´Ÿçš„ï¼ˆå¡æ–¹æ ¡éªŒè¦æ±‚ï¼‰\n",
    "X = np.abs(X)\n",
    "\n",
    "# ä½¿ç”¨ KBinsDiscretizer å¯¹ç‰¹å¾è¿›è¡Œç¦»æ•£åŒ–å¤„ç†\n",
    "binning = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "X_binned = binning.fit_transform(X)\n",
    "\n",
    "# æ£€æŸ¥ç¦»æ•£åŒ–åçš„æ•°æ®åˆ†å¸ƒ\n",
    "print(\"ç¦»æ•£åŒ–åçš„ç‰¹å¾:\\n\", X_binned)\n",
    "\n",
    "# ä½¿ç”¨ SelectKBest å’Œ chi2 è¿›è¡Œç‰¹å¾é€‰æ‹©\n",
    "selector = SelectKBest(score_func=chi2, k=2)\n",
    "X_new = selector.fit_transform(X_binned, y)\n",
    "\n",
    "print(\"åŸå§‹ç‰¹å¾å½¢çŠ¶:\", X.shape)\n",
    "print(\"é€‰æ‹©åçš„ç‰¹å¾å½¢çŠ¶:\", X_new.shape)\n",
    "print(\"é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•:\", selector.get_support(indices=True))\n",
    "print(\"å¡æ–¹ç»Ÿè®¡é‡:\", selector.scores_)\n",
    "print(\"På€¼:\", selector.pvalues_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149e67a",
   "metadata": {},
   "source": [
    "### æœºå™¨å­¦ä¹ \n",
    "###### å››ã€ç®—æ³•åŸç†è§£é‡Šã€é€‚ç”¨é—®é¢˜ä¸SKlearnå†™æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9785539a",
   "metadata": {},
   "source": [
    "ï¼ˆä¸€ï¼‰KNN è¿‘é‚»ç®—æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f9a965",
   "metadata": {},
   "source": [
    "1. å®šä¹‰ä¸é€‚ç”¨æƒ…å†µ   \n",
    "ï¼ˆ1ï¼‰ knn è¿‘é‚»ç®—æ³• é€šè¿‡å„ç§æ•°å­¦å…¬å¼æ‰¾åˆ°æ•°æ®ç‚¹ç›´æ¥çš„è·ç¦» å†æ ¹æ®kä¸ªç›¸è¿‘ç‚¹ æŠ•ç¥¨æ‰¾åˆ°æœªçŸ¥ç‚¹å¯èƒ½çš„åˆ†ç±»   \n",
    "ï¼ˆ2ï¼‰ é€‚åˆæ•°æ®å° ç»´åº¦å°‘ æ•°æ®åˆ†å¸ƒç›¸å¯¹å¹³å‡ ä¸ä¼šå±€éƒ¨ç¨€ç–çš„ æ•°æ®é›† æœ€ä¸ºåŸºç¡€çš„ç®—æ³• ç”¨äºå›å½’ä¹Ÿç”¨äºåˆ†ç±»   \n",
    "ï¼ˆ3ï¼‰ åˆ¤æ–­ç‚¹ä¸ç‚¹ä¹‹é—´çš„è·ç¦»å­˜åœ¨å¤æ•°çš„æ–¹å¼ åŒ…æ‹¬ä½†ä¸é™äºæ¬§å¼è·ç¦»ï¼Œåˆ‡æ¯”é›ªå¤«è·ç¦»ï¼Œæ›¼å“ˆé¡¿è·ç¦»ï¼Œä»¥åŠä¸‰åˆä¸€çš„é—µå¯å¤«æ–¯åŸºè·ç¦»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2ee12",
   "metadata": {},
   "source": [
    "2. æ‰‹åŠ¨å±•ç¤ºknn ä»¥ä¸€ä¸ªåˆ†ç±»é—®é¢˜ä¸ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de629759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. åˆ¤æ–­ä»»åŠ¡ç›®çš„ é€šè¿‡å·²çŸ¥æ•°æ®åˆ†ç±»æœªçŸ¥å®å¯æ¢¦çš„ç±»åˆ« æ‰€ä»¥æ˜¯åˆ†ç±»é—®é¢˜ \n",
    "\n",
    "# 1. æ‹¿åˆ°æ•°æ®æºå¹¶ç®€å•è§‚å¯Ÿ\n",
    "# 1.1 å¼•å…¥æ•°æ®æº \n",
    "import pandas as pd\n",
    "df_known = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\pokemon_knn.xlsx\",\n",
    "                         sheet_name = 'å·²çŸ¥å®å¯æ¢¦' )\n",
    "df_unknown = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\pokemon_knn.xlsx\",\n",
    "                         sheet_name = 'ç¥ç§˜å®å¯æ¢¦' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a5107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.2 è§‚å¯Ÿæ•°æ®æº\n",
    "# åœ¨åªä½¿ç”¨å¯¹æˆ˜æ•°æ®çš„æƒ…å†µä¸‹ ä½“é‡å’Œèº«é«˜çš„ç¼ºå¤±å€¼æ— éœ€å¤„ç† åä¹‹åˆ™éœ€è¦å¤„ç†\n",
    "df_known.info() # 684 non-null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc831c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.æ¼”ç¤ºKNNæ¨¡å‹é€»è¾‘ ç°åœ¨å‡è®¾æˆ‘ä»¬åªéœ€è¦æ²¡æœ‰æœªçŸ¥å€¼çš„å¯¹æˆ˜æ•°æ® æ— ç¼ºå¤±ä¸”ç‰¹å¾éƒ½éœ€è¦ ç›´æ¥å¼€å§‹è·‘æ¨¡å‹\n",
    "# é€‰å–å¯¹æˆ˜çš„ç‰¹å¾ ä½œä¸º xå€¼ åˆ†ç±»çš„å®å¯æ¢¦ä¸ºyå€¼ é€‰å–3æ¡æ•°æ®å‡ºæ¥ \n",
    "r1 = df_known.loc[1,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»']   \n",
    "r2 = df_known.loc[100,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»'] \n",
    "r3 = df_known.loc[3,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»'] \n",
    "\n",
    "# è®¡ç®—2ç‚¹è·ç¦» ç­‰äºè®¡ç®— æ‰€æœ‰ç‰¹å¾çš„è·ç¦» å¹³æ–¹ç›¸åŠ  å¼€æ ¹å·\n",
    "sum( [(r1[i]-r2[i])**2 for i in range(0,len(r1))] ) \n",
    "import math\n",
    "print('r1å’Œr2çš„è·ç¦»æ˜¯ï¼š', math.sqrt(sum( [(r1[i]-r2[i])**2 for i in range(0,len(r1))] ) ) ) # 3.09\n",
    "print('r1å’Œr3çš„è·ç¦»æ˜¯ï¼š', math.sqrt(sum( [(r1[i]-r3[i])**2 for i in range(0,len(r1))] ) ) ) # 3.44\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12684708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. å†™æˆå‡½æ•° å¯¹æ•´ä¸ªæ•°æ®é›†åº”ç”¨ åˆ¤æ–­ä¸æœªçŸ¥çš„è·ç¦»\n",
    "import math\n",
    "def knn(row):\n",
    "    known = row['å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»']\n",
    "    unknown = df_unknown.loc[0,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»'] # å‡è®¾è¿™ä¸ªæ˜¯è®°è½½æœªçŸ¥r1å€¼çš„xå€¼\n",
    "    return  math.sqrt(sum( [(known[i]-unknown[i])**2 for i in range(0,len(known))] ) ) \n",
    "\n",
    "df_known['knn'] = df_known.apply(knn,axis = 1) # ä¸€è¡Œä¸€ä¸ªy å¤æ•°x å¯¹äº æ¯ä¸ªx å³è¡Œçš„æ¯ä¸ªåˆ—åšæ“ä½œ æ‰€ä»¥axis = 1 å¼ºåˆ¶ç†è§£\n",
    "\n",
    "# è·å–åº”ç”¨æ•°æ®é›†çš„index æ‰¾åˆ°ä¸ä»–æœ€è¿‘çš„kä½é‚»å±… k = 10\n",
    "df_known.iloc[df_known['knn'].sort_values().head(7).index, :].groupby('ä¸»åˆ†ç±»').agg(ä¸»åˆ†ç±»è®¡æ•°=('ä¸»åˆ†ç±»', 'count'))\n",
    "                                                                                     # æ–°åˆ—å      ä½¿ç”¨åˆ—å  å‡½æ•°æ“ä½œ\n",
    "# k=10 10ä¸ªè‰ç³» 4ä¸ªè™«ç³» æ‰€ä»¥ç»“æœæ˜¯è‰ç³»\n",
    "# k=7  3ä¸ªè‰ç³»  4ä¸ªè™«ç³» æ‰€ä»¥ç»“æœæ˜¯è™«ç³»\n",
    "\n",
    "df_known.iloc[df_known['knn'].sort_values().head(7).index, :]['ä¸»åˆ†ç±»'].mode()[0] # å–ä¼—æ•°çŸ¥é“ä»€ä¹ˆç³»åˆ—æœ€å¤š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef0c4e",
   "metadata": {},
   "source": [
    "3. é‡‡ç”¨SKlearnåº“æ¥ä½¿ç”¨KNNç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9339d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. æ˜ç¡®ç›®çš„ä¸ºåˆ†ç±»\n",
    "# 2. æ•°æ®é¢„å¤„ç†\n",
    "# 2.1 å¼•å…¥æ•°æ®æº \n",
    "import pandas as pd\n",
    "df_known = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\pokemon_knn.xlsx\",\n",
    "                         sheet_name = 'å·²çŸ¥å®å¯æ¢¦' )\n",
    "df_unknown = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\pokemon_knn.xlsx\",\n",
    "                         sheet_name = 'ç¥ç§˜å®å¯æ¢¦' )\n",
    "# 3 ç‰¹å¾å·¥ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b4afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 é€‰æ‹©kè¿‘é‚»ç®—æ³• \n",
    "# 5 æ¨¡å‹è®­ç»ƒ\n",
    "# 5.1 æ‹†åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†\n",
    "from sklearn.model_selection import train_test_split\n",
    "dfk_train,dfk_test = train_test_split(df_known,         # å·²çŸ¥æ•°æ®é›†\n",
    "                                     test_size = 0.3,   # æµ‹è¯•é›†æ¯”ä¾‹ ä¸€èˆ¬0.2-0.3\n",
    "                                     random_state = 42, # ä¸æƒ³æ¯æ¬¡æŠ½å–éƒ½éšæœº ä»»æ„æ•´æ•°å›ºå®šæ•°æ®é›†åˆ’åˆ†\n",
    "                                     shuffle = True,   # é»˜è®¤True ä»£è¡¨éšæœºæ‰“ä¹±æ•°æ®é›† åä¹‹å›ºå®šé¡ºåºæŠ½å–æ²¡æœ‰éšæœºæ€§ random_state å‚æ•°å¤±å»æ„ä¹‰\n",
    "                                     #stratify = df_known['ä¸»åˆ†ç±»'] #  æ§åˆ¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­å„ç±»çš„æ¯”ä¾‹ä¸å®Œæ•´æ•°æ®é›†ä¸­çš„æ¯”ä¾‹ç›¸åŒ \n",
    "                                                                   # ä¸æ”¯æŒå›ºå®šé¡ºåºæŠ½å– å³shuffle = False\n",
    "                                     ) \n",
    "\n",
    "# train_test_split??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdffbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 ç”Ÿæˆknnç®—æ³•å¯¹è±¡\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "k = 7\n",
    "knn = KNeighborsClassifier(n_neighbors = k) # åç»­ç›´æ¥æ›´æ”¹kå€¼ä¸éœ€è¦å†æ¬¡ç”Ÿæˆknnï¼Œä¹Ÿä¸éœ€è¦é‡æ–°knn.fitï¼Œä»–å­˜æ”¾çš„æ˜¯è·ç¦»å€¼ï¼Œkåªå†³å®šå‡ ä¸ªé‚»å±…æŠ•ç¥¨\n",
    "\n",
    "# 5.3 è·å–éœ€è¦xå’Œyå€¼\n",
    "x = dfk_train.loc[:,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»']\n",
    "y = dfk_train['ä¸»åˆ†ç±»']\n",
    "\n",
    "# 5.4 è¿›è¡Œæ‹Ÿåˆ ç”Ÿæˆæ¨¡å‹\n",
    "k_model = knn.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ba8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 è¯„ä¼°ä¼˜åŒ–\n",
    "# k_modelæ˜¯å·²ç»è·å–çš„æ¨¡å‹ ç”¨æµ‹è¯•é›†æ‹¿å»æµ‹è¯•ä¸‹å‡†ç¡®ç‡\n",
    "\n",
    "# 6.1 æµ‹è¯•é›†é¢„æµ‹\n",
    "y_predicted = k_model.predict(dfk_test.loc[:,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»'])\n",
    "\n",
    "# 6.2 è¯„ä¼°æ¨¡å‹ å³ä¸çœŸå®åˆ†ç±»åšæ¯”è¾ƒ\n",
    "y_real = dfk_test['ä¸»åˆ†ç±»'].values\n",
    "print(f'kå€¼ä¸º{k}æ—¶ï¼Œé¢„æµ‹å‡†ç¡®ç‡æ˜¯ï¼š',round(\n",
    "                                    (y_predicted == y_real).sum() # ç»Ÿè®¡Trueçš„å€¼ä¸º178\n",
    "                                    /y_real.size ,                # å…¨éƒ¨æµ‹è¯•é›†é•¿åº¦ä¸º211\n",
    "                                    4) \n",
    "     )\n",
    "\n",
    "# 6.3 æˆ–è€…ç›´æ¥ä½¿ç”¨skå°è£…çš„è¯„ä¼°å·¥å…·\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score( dfk_test['ä¸»åˆ†ç±»'], y_predicted) # å…ˆå†™y_trueï¼Œå†å†™y_predicted\n",
    "\n",
    "# print('æ¨¡å‹çš„ç²¾ç¡®åº¦æ˜¯ï¼š',round(Accuracy,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65d92e",
   "metadata": {},
   "source": [
    "4. å¤šç§è·ç¦»ç®—æ³•å‚æ•°è¿ç”¨   \n",
    "é»˜è®¤metric ä¸ºé—µå¯å¤«æ–¯åŸºè·ç¦» è°ƒæ•´på€¼ ä½¿å¾— 1ä¸ºæ›¼å“ˆé¡¿ 2ä¸ºæ¬§å¼ æ— ç©·å¤§ä¸ºåˆ‡æ¯”é›ªå¤«è·ç¦»   \n",
    "å…¶ä»–è·ç¦»ç®—æ³• è¯·è°ƒæ•´metricå‚æ•°ï¼Œä»é»˜è®¤çš„å‰å¾€å…¶ä»–ç®—æ³•    \n",
    "    \"euclidean\"  æ¬§æ°è·ç¦»              sqrt(sum((x-y)^2))\n",
    "\n",
    "    \"manhattan\"  æ›¼å“ˆé¡¿è·ç¦»         sum(|x-y|)\n",
    "\n",
    "    \"chebyshev\"  åˆ‡æ¯”é›ªå¤«è·ç¦»       max(|x-y|)\n",
    "\n",
    "    \"minkowski\"  é—µå¯å¤«æ–¯åŸºè·ç¦»      sum(w*|x-y|^p)^(1/p)\n",
    "\n",
    "    \"seuclidean\"  æ ‡å‡†æ¬§æ°è·ç¦»          sqrt(sum((x-y)^2/V))\n",
    "\n",
    "    \"mahalanobis\"  é©¬å“ˆæ‹‰è¯ºæ¯”æ–¯è·ç¦»    sqrt((x-y)'V^-1(x-y))\n",
    "\n",
    "    \"hamming\"    æµ·æ˜ï¼ˆæ±‰æ˜ï¼‰è·ç¦»       sum(|w*(x-y)|^p)^(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa694a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "knn = KNeighborsClassifier(n_neighbors = 2,p=1)\n",
    "\n",
    "# ç”Ÿæˆä¸€ä¸ªç”¨äºè·ç¦»æ¼”ç®—çš„df\n",
    "df1 =  pd.DataFrame( data = [[5,5,5],[5,1,3],[2,2,2]],columns = ['å°„é—¨','é€Ÿåº¦','è¯„åˆ†'])\n",
    "\n",
    "# æ„å»ºæ¨¡å‹ è£…å¡«è®­ç»ƒé›†æ•°æ® åŒ…å«çš„ x y å€¼\n",
    "model = knn.fit(df1.loc[0:1 , 'å°„é—¨':'é€Ÿåº¦'] , df1.loc[0:1 , 'è¯„åˆ†'] )\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹çš„kneighborsæ–¹æ³• è·å–å•ä¸ªç‚¹çš„è·ç¦» è¿”å›è·ç¦»å€¼å’Œ æµ‹è¯•é›†çš„ç´¢å¼•\n",
    "distances, index = model.kneighbors(df1.loc[[2] , 'å°„é—¨':'é€Ÿåº¦']) # è¿”å›å‡ ä¸ªå’Œ n_neighbors å‚æ•°æœ‰å…³ åªè¿”å›ä½ è¦æ±‚çš„æ•°é‡ è€Œéå…¨éƒ¨è®¡ç®—è¿‡çš„\n",
    "print(f'å’Œ\\n{df1.iloc[index[0]]}\\nè·ç¦»åˆ†åˆ«æ˜¯{distances[0]}')\n",
    "\n",
    "# éªŒè¯å‡ºp=1çš„æ—¶å€™ä¸ºæ›¼å“ˆé¡¿è·ç¦»\n",
    "p = 1 \n",
    "x1 = np.array(df1.loc[0:1 , 'å°„é—¨':'é€Ÿåº¦'])\n",
    "x2 = np.array(df1.loc[[2] , 'å°„é—¨':'é€Ÿåº¦'])\n",
    "\n",
    "for _ in x1-x2:\n",
    "    # æ‰‹åŠ¨è®¡ç®—ä¸‹æ›¼å“ˆé¡¿è·ç¦» æ¯ä¸ªç»´åº¦ç›¸å‡å–ç»å¯¹å€¼æœ€åç›¸åŠ \n",
    "    print(sum([abs(x) for x in _ ]))\n",
    "\n",
    "# metric å‚æ•°è®¾ç½®å…¶ä»–è·ç¦»åå­—ï¼Œé»˜è®¤ é—µå¯å¤«æ–¯åŸºè·ç¦»\n",
    "# weights å‚æ•°è®¾ç½®æ¯ä¸ªé‚»å±…è®¡ç®—çš„æƒé‡ å…¸å‹çš„æ˜¯æ±‚å–åæ¯”å€’æ•° weights = distance é»˜è®¤weights = uniform "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e8555",
   "metadata": {},
   "source": [
    "### æœºå™¨å­¦ä¹ \n",
    "###### ä¸‰ã€ç®—æ³•åŸç†è§£é‡Šã€é€‚ç”¨é—®é¢˜ä¸SKlearnå†™æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77794649",
   "metadata": {},
   "source": [
    "ï¼ˆäºŒï¼‰æœ´ç´ è´å¶æ–¯ç®—æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e84081",
   "metadata": {},
   "source": [
    "1. å®šä¹‰ä¸é€‚ç”¨æƒ…å†µ   \n",
    "ï¼ˆ1ï¼‰ æœ´ç´ è´å¶æ–¯ç®—æ³• é€‚åˆé«˜çº¬ç¨€ç–æ•°æ® é«˜çº¬æŒ‡ç»´åº¦ç‰¹å¾å¾ˆå¤š ç¨€ç–æŒ‡å¤§éƒ¨åˆ†çš„ç‰¹å¾å€¼éƒ½æ˜¯ç©ºæˆ–è€…0  \n",
    "ï¼ˆ2ï¼‰ æœ´ç´ åœ¨å‡è®¾æ¯ä¸ªç»´åº¦ä¹‹é—´çš„å…³ç³»æ˜¯ç›¸äº’ç‹¬ç«‹çš„ ä½†æ˜¯ç°å®ä¸­ä¸å¯èƒ½å­˜åœ¨ä¸¤ä¸ªç‰¹å¾ä¹‹é—´æ¯«æ— å…³ç³»  \n",
    "ï¼ˆ3ï¼‰ è´å¶æ–¯ç®—æ³•åŸºäºè´å¶æ–¯å®šç† ä¸‹é¢è¯¦ç»†è§£é‡Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259b2e8",
   "metadata": {},
   "source": [
    "2. è´å¶æ–¯å®šç†  \n",
    "ï¼ˆ1ï¼‰æè¿°ä¸¤ä¸ªæ¡ä»¶æ¦‚ç‡ä¹‹é—´çš„å…³ç³»  å³å½“ä¸€ä»¶äº‹æƒ…ï¼ˆæ¡ä»¶ï¼‰å‘ç”Ÿæ—¶å¦å¤–ä¸€ä»¶äº‹æƒ…ï¼ˆæ¡ä»¶ï¼‰å‘ç”Ÿçš„æ¦‚ç‡ å…ˆå‘ç”Ÿçš„äº‹æƒ…ä½äº|çš„åé¢    \n",
    "ï¼ˆ2ï¼‰å¸¸è§çš„å…¬å¼ä¼šæ˜¯  \n",
    "P(A|B) = P(B|A)* P(A) /P(B)   \n",
    "P(A|B)è¡¨ç¤ºåœ¨Bå‘ç”Ÿçš„æ¡ä»¶ä¸‹Aå‘ç”Ÿçš„æ¦‚ç‡  å³éœ€è¦æ¨ç®—çš„åéªŒæ¦‚ç‡    \n",
    "P(B|A)æ ‡è¯†åœ¨Aå‘ç”Ÿçš„æ¡ä»¶ä¸‹Bå‘ç”Ÿçš„æ¦‚ç‡ å³æ¡ä»¶æ¦‚ç‡ ä¼¼ç„¶ é€šå¸¸æ˜¯ å…³å¿ƒçš„åˆ†ç±»ä¸­å‡ºç°é€‰å®šçš„ç‰¹å¾çš„æ ·æœ¬æ¦‚ç‡  \n",
    "P(A)ä»£è¡¨ç€Aäº‹ä»¶ä¸å—ä»»ä½•å› ç´ å½±å“çš„æƒ…å†µä¸‹å‘ç”Ÿçš„æ¦‚ç‡ å¯ä»¥ç®€æ˜“ç†è§£ä¸º å…³å¿ƒçš„åˆ†ç±»/æ ·æœ¬æ€»æ•°     \n",
    "P(B)ä»£è¡¨ç€è¾¹ç¼˜ä¼¼ç„¶ ä¹Ÿå¯ä»¥è¯´æ˜¯Bäº‹ä»¶ä¸å—å› ç´ å½±å“å‘ç”Ÿçš„æ¦‚ç‡ æœ´ç´ è´å¶æ–¯ä¸­é€šå¸¸è§†å…¶ä¸ºå¸¸æ•° æ— éœ€è®¡ç®—     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700975fa",
   "metadata": {},
   "source": [
    "ä»¥åƒåœ¾é‚®ä»¶ä¸¾ä¾‹ é‚®ä»¶ç”±è‹¥å¹²çš„æ–‡æœ¬ç»„æˆ æ¯ä¸€ä¸ªæ–‡æœ¬å³ä¸ºä¸€ä¸ªç‰¹å¾ å‡è®¾ç°åœ¨ä»…å–å‘ç¥¨å’ŒæŠ˜æ‰£ä¸¤ä¸ªè¯ä½œä¸ºç‰¹å¾ ç”¨äºè¯†åˆ«ä¸€å°é‚®ä»¶æ˜¯å¦ä¸ºå¹¿å‘Š   \n",
    "å°±æ˜¯åœ¨åˆ¤æ–­ é‚®ä»¶æ–‡æœ¬ä¸­åŒ…å«å‘ç¥¨ä¸æŠ˜æ‰£çš„æ—¶å€™ï¼ˆå‰ä¸€ä¸ªäº‹ä»¶å‘ç”Ÿäº†ï¼‰ï¼Œæ˜¯å¹¿å‘Šè¿™ä¸ªäº‹æƒ…å‘ç”Ÿçš„æ¦‚ç‡    \n",
    "å¥—ç”¨è´å¶æ–¯å…¬å¼å†™æˆ   \n",
    "P(æ˜¯å¹¿å‘Š|æ–‡æœ¬ä¸­åŒ…å«å‘ç¥¨ä¸æŠ˜æ‰£) æ­£æ¯”äº P(æ–‡æœ¬åŒ…å«å‘ç¥¨ä¸æŠ˜æ‰£|æ˜¯å¹¿å‘Š)*P(æ˜¯å¹¿å‘Š) / P(æ–‡æœ¬ä¸­åŒ…å«å‘ç¥¨ä¸æŠ˜æ‰£)   \n",
    "æ­¤é¡¹å…¬å¼å¯ä»¥è¿›ä¸€æ­¥ç»†åˆ†ç»´åº¦ å³ å‡è®¾ ä¸¤ä¸ªç‰¹å¾ä¸å­˜åœ¨ç›¸å…³æ€§ ä»è€Œæ¨ç®— æ˜¯å¹¿å‘Š æ–‡æœ¬åŒ…å«å‘ç¥¨ä¸æŠ˜æ‰£ è¿™ä¸€ä¸ªäº‹æƒ…å‘ç”Ÿçš„æ¦‚ç‡çº¦ç­‰äº æ˜¯å¹¿å‘Š æ–‡æœ¬ä¸­åŒ…å«å‘ç¥¨æ¦‚ç‡ * æ˜¯å¹¿å‘Š æ–‡æœ¬ä¸­åŒ…å«æŠ˜æ‰£æ¦‚ç‡   \n",
    "**** ä»…ä»…æ¨¡å‹æ˜¯æœ´ç´ çš„  è´å¶æ–¯å…¬å¼å¯ä¸æ˜¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894d7d8",
   "metadata": {},
   "source": [
    "3. æœ´ç´ è´å¶æ–¯æ¨¡å‹çš„ä¼˜ç¼ºç‚¹  \n",
    "ä¼˜ç‚¹ï¼š  \n",
    "ï¼ˆ1ï¼‰é«˜çº¬ç¨€ç–æ•°æ®ä¹Ÿå¯ä»¥å¤„ç† å› ä¸ºç®€åŒ–ç»´åº¦ä¹‹é—´çš„ç›¸å…³æ€§   \n",
    "ï¼ˆ2ï¼‰é¢å¯¹å¤§æ•°æ®é›†ä¹Ÿèƒ½åšåˆ°é«˜æ•ˆç‡ å› ä¸ºè®¡ç®—é‡éå¸¸çš„å° ä»…ä»…è®¡ç®— å…ˆéªŒæ¦‚ç‡ï¼ˆä½ å…³æ³¨çš„åˆ†ç±»åœ¨æ ·æœ¬ä¸­çš„å‡ºç°æ¦‚ç‡ï¼‰å’Œæ¡ä»¶æ¦‚ç‡ï¼ˆç‰¹å¾/å…³æ³¨åˆ†ç±»ï¼‰  å°±å¯ä»¥æ¨ç®—å‡ºåéªŒæ¦‚ç‡    \n",
    "ï¼ˆ3ï¼‰å¯¹ç¼ºå¤±æ•°æ®ä¸ä¼šå¾ˆæ•æ„Ÿ å³ä½¿æŸä¸ªç»´åº¦æœªå‡ºç° ä¹Ÿå¯ä»¥é€šè¿‡æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘çš„æ–¹å¼åŠ å¸¸æ•°è§„é¿0å€¼    \n",
    "ç¼ºç‚¹ï¼š   \n",
    "ï¼ˆ1ï¼‰è¿‡åº¦ç®€åŒ–ç»´åº¦ç›¸å…³æ€§ å¯¼è‡´æ€§èƒ½çš„æ˜¾è‘—ä¸‹é™ å°¤å…¶åœ¨é¢å¯¹ç»´åº¦æœ¬èº«ç›¸å…³æ€§å¾ˆå¼ºçš„æ•°æ®é›†æ—¶   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16849b",
   "metadata": {},
   "source": [
    "4. ä½¿ç”¨æœ´ç´ è´å¶æ–¯ä¹‹å‰ å¿…é¡»è¦å®Œæˆç‰¹å¾å·¥ç¨‹ å³è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ æˆ‘ä»¬åœ¨è¿™é‡Œä»‹ç»ä¸¤ç§å¤„ç†æ–¹å¼     \n",
    "ï¼ˆ1ï¼‰è¯è¢‹æ¨¡å¼ï¼šå¿½è§†è¯ä¸è¯çš„å‰åé¡ºåº ä»…ç»Ÿè®¡è¯å‡ºç°çš„é¢‘åº¦      \n",
    "ï¼ˆ2ï¼‰N-GRAMæ¨¡å¼ï¼šå…³æ³¨è¯ä¸è¯å‡ºç°çš„å…ˆåé¡ºåº Nä»£è¡¨ç»Ÿè®¡çš„è¯è¿ç»­çš„æ•°é‡      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b6f01",
   "metadata": {},
   "source": [
    "5. æ‰‹åŠ¨å±•ç¤º è‹±æ–‡ ä¸­æ–‡çš„åˆ†è¯å¤„ç† ä¸ä»–å‘é‡åŒ–çš„è¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c681f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5.1 å‡è®¾å­˜åœ¨å¦‚ä¸‹è‹±æ–‡è¯­å¥éœ€è¦åšå¤„ç†\n",
    "\n",
    "import pandas as pd\n",
    "data=[\n",
    "     [\"I love natural language processing\",\"æ­£é¢\" ],\n",
    "     [\"Language processing is fascinating\",\"æ­£é¢\" ],\n",
    "     [\"I dislike artificial intelligence\",\"è´Ÿé¢\"  ],\n",
    "     [\"Machine learning is hard\",         \"è´Ÿé¢\"  ],       \n",
    "     ]\n",
    "df = pd.DataFrame(data,columns = ('language','status'))\n",
    "\n",
    "# 5.2 è°ƒç”¨sklearnä¸­çš„å·¥å…· ä¸ç”¨çš„è¯ å¯ä»¥ä½¿ç”¨ä¸­æ–‡åˆ†è¯å·¥å…· ç»“å·´ åœ¨é¢‘åº¦ç»Ÿè®¡åˆ—è¡¨ä¹Ÿå¯ä»¥ ;è‹±æ–‡ç›´æ¥å¯ä»¥ä½¿ç”¨ å­—ç¬¦ä¸²å¯¹è±¡çš„splitæ–¹æ³•\n",
    "from sklearn.feature_extraction.text import CountVectorizer # ç»Ÿè®¡ å‘é‡åŒ–å™¨\n",
    "\n",
    "# ä¾èµ–äºé¢„è®¾å¯¹è±¡ç”Ÿæˆå®ä¾‹ countvector\n",
    "countvector = CountVectorizer(\n",
    "                        stop_words = None,  # é»˜è®¤ç»Ÿè®¡æ‰€æœ‰åˆ†è¯\n",
    "                        ngram_range = (1,1), # é»˜è®¤ç”Ÿæˆè¯è¢‹æ¨¡å‹ rangeå¦‚æœç­‰äº(1,2)å°±ä»£è¡¨1ä¸ªå•è¯ æˆ–è€…è¿ç»­çš„ä¸¤ä¸ªå•è¯éƒ½è¦ç”¨äºç”Ÿæˆè¯æ±‡è¡¨\n",
    "                        token_pattern = r\"(?u)\\b\\w+\\b\" # é»˜è®¤ä½¿ç”¨çš„r'(?u)\\b\\w\\w+\\b' ä¼šå¯¼è‡´ä¸€ä¸ªå­—æ¯çš„ä¸è¢«ç•™ä¸‹æ¥\n",
    "                        # lowercase = False  # é»˜è®¤True å³å…¨éƒ¨å°å†™åŒ–\n",
    "                        # max_df            # å¤§äºè¿™ä¸ªé¢‘åº¦çš„è¯ä¸ç»Ÿè®¡ æµ®ç‚¹æ•°ä»£è¡¨è¯æ±‡åœ¨æ–‡æ¡£ä¸­çš„æœ€å¤§æ¯”ä¾‹ æ•´æ•°ä»£è¡¨è¯æ±‡åœ¨æ–‡æ¡£ä¸­çš„æœ€å¤§å‡ºç°æ¬¡æ•°\n",
    "                        # min_df            # å°äºè¿™ä¸ªé¢‘åº¦çš„è¯ä¸ç»Ÿè®¡\n",
    "                        # max_features      # æŒ‡å®šæœ€å¤§çš„ç‰¹å¾æ•°é‡ æŒ‰ç…§é¢‘é“æ’åºå–æœ€å¤§çš„ç‰¹å¾\n",
    "                        # vocabulary        # æŒ‡å®šä»…ç»Ÿè®¡è¿™ä¸ªé‡Œé¢çš„å•è¯\n",
    "                        # binary            # äºŒè¿›åˆ¶å‘é‡ æ‰€æœ‰é¢‘åº¦éƒ½å½’1\n",
    "                        # dtype             # æŒ‡å®šè¾“å‡ºæ ¼å¼\n",
    "                        )\n",
    "\n",
    "# è°ƒç”¨countvectorçš„ fitæ–¹æ³• å»è§£æä¸€ä¸ªè¯æ±‡è¡¨ å…¨éƒ¨åˆ†ç±»çš„è¯éƒ½æ‹¿å»è§£æ\n",
    "countvector.fit(df['language'])\n",
    "vocabulary = countvector.get_feature_names_out() # è§£æåæ‰èƒ½è°ƒç”¨è¿™ä¸ªæ–¹æ³•è·å–vocabulary\n",
    "\n",
    "# ä½¿ç”¨transformè½¬åŒ–å‘é‡çŸ©é˜µ è°ƒç”¨toarray()æ–¹æ³•å®ç°è‚‰çœ¼è§‚å¯Ÿ\n",
    "x = countvector.transform(df['language'])\n",
    "x.toarray() \n",
    "\n",
    "# æˆ–è€…ç›´æ¥ä½¿ç”¨fit_transformæ–¹æ³•ä¸€æ¬¡å®ç° èµ‹å€¼CountVectorizerçš„vocabulary å¹¶è¾“å‡ºå‘é‡çŸ©é˜µ\n",
    "x = countvector.fit_transform(df['language'])\n",
    "print('è¯æ±‡åˆ—è¡¨å¦‚ä¸‹ï¼š\\n',countvector.get_feature_names_out())\n",
    "x.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5e8d7",
   "metadata": {},
   "source": [
    "6. åŸºäº5å‘é‡åŒ–çš„ä¸œè¥¿åšä¸€ä¸ªé¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 è®¡ç®—å…ˆéªŒæ¦‚ç‡\n",
    "# ä¿ç•™ä¸‹å‘é‡ç»“æœ ç”¨äºåç»­çš„æ¡ä»¶æ¦‚ç‡/ä¼¼ç„¶è®¡ç®—\n",
    "df['å‘é‡ç»“æœ'] = list(x.toarray())\n",
    "\n",
    "# å¢åŠ ç±»åˆ«èšåˆ è®¡ç®—å¯¹åº”çš„å…ˆéªŒå’Œä¼¼ç„¶çš„å‰ç½®æ•°æ®\n",
    "dfagg = df.groupby('status').agg(ç±»åˆ«æ ·æœ¬æ•° = ('status','count') ,è¯æ±‡é¢‘åº¦ = ('å‘é‡ç»“æœ','sum')     ).reset_index()\n",
    "dfagg['æ€»æ ·æœ¬æ•°'] = df.shape[0]\n",
    "dfagg['å…ˆéªŒæ¦‚ç‡'] = dfagg['ç±»åˆ«æ ·æœ¬æ•°']/dfagg['æ€»æ ·æœ¬æ•°']\n",
    "dfagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 è®¡ç®—ä¼¼ç„¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991b206",
   "metadata": {},
   "source": [
    "å•ä¸ªç‰¹å¾çš„ä¼¼ç„¶æˆ‘ä»¬æ­¥éª¤å¼ç†è§£ä¸‹\n",
    "1. ä¸ºä½•æˆ‘ä»¬è¦è®¡ç®—ä¼¼ç„¶  \n",
    "P(Câˆ£D)=P(Dâˆ£C)â‹…P(C) / P(D)   \n",
    "ç®€åŒ–ä¸ºï¼šP(Câˆ£x)âˆP(C)â‹…P(x1âˆ£C)â‹…P(x 2âˆ£C)* â€¦â€¦ * P(x nâˆ£C)    \n",
    "2. å¸¦å…¥ä¸‹æ¡ˆä¾‹çœ‹çœ‹æˆ‘ä»¬åœ¨ç®—ä»€ä¹ˆ   \n",
    "æ¡ˆä¾‹ä¸­ C å³ä¸ºæ­£é¢æˆ–è€…è´Ÿé¢       \n",
    "æ¡ˆä¾‹ä¸­ å…±4ä¸ªæ ·æœ¬ æ±‚å–çš„ä¼¼ç„¶æ˜¯ æ˜¯æ­£é¢/è´Ÿé¢ åˆ†ç±»çš„æ¡ä»¶å‘ç”Ÿæ—¶ æ¯ä¸ªç‰¹å¾/è¯æ±‡å‘é‡ å‡ºç°çš„æ¦‚ç‡   \n",
    "3. å•ä¸ªç‰¹å¾ä¼¼ç„¶çš„å…¬å¼ï¼ˆw=c éƒ½ä»£è¡¨ç‰¹å¾ï¼‰    \n",
    "å¯¹äºä¸€ä¸ªè¯ ğ‘¤ åœ¨ç±»åˆ« ğ¶ä¸‹çš„æ¡ä»¶æ¦‚ç‡ ğ‘ƒ(ğ‘¤|ğ¶) å…¶è®¡ç®—å…¬å¼åº”è¯¥æ˜¯ï¼š   \n",
    "P(wâˆ£C)= ç±»åˆ« C ä¸­è¯ w çš„è®¡æ•°+Î± / ç±»åˆ« C ä¸­æ‰€æœ‰è¯çš„è®¡æ•°æ€»å’Œ+Î±Ã—è¯æ±‡è¡¨å¤§å°ï¼‰   \n",
    "**è¿™é‡Œçš„åˆ†æ¯ ä¸æ˜¯ç±»åˆ«Cçš„æ ·æœ¬æ•° 2 è€Œæ˜¯æ•´ä¸ªåˆ†ç±»Cä¸‹æ‰€æœ‰è¯çš„è®¡æ•°æ€»å’Œ ä»–ä»¬æ‰æ˜¯è®¡ç®—è¯é¢‘çš„åˆ†æ¯**    \n",
    "4. å¸¦å…¥ä¸‹æ¡ˆä¾‹è®¡ç®—å‡ºlove    \n",
    "æˆ‘ä»¬å…ˆæ±‚å–ç±»åˆ«ï¼šæ­£é¢ä¸­çš„ loveå‡ºç°çš„æ¦‚ç‡    \n",
    "ç±»åˆ« æ­£é¢ ä¸­è¯ love çš„è®¡æ•°ï¼š1   \n",
    "ç±»åˆ« æ­£é¢ ä¸­æ‰€æœ‰è¯çš„è®¡æ•°æ€»å’Œï¼š9   \n",
    "è¯æ±‡è¡¨å¤§å°ï¼š13    \n",
    "æ¦‚ç‡æ­£å¸¸åº”è¯¥æ˜¯ï¼š1/9  = 11.11%    \n",
    "æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ æˆ‘ä»¬é€šå¸¸è®¾å®š Î±= 1 +1å¹³æ»‘ä¹Ÿå¯ä»¥æœ‰+2å¹³æ»‘     \n",
    "æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æ¦‚ç‡ï¼š1+1/9+1*13 = 9.09%    \n",
    "5. é™„å½•æ¡ˆä¾‹çš„ è¯æ±‡è¡¨ å’Œ è¯æ±‡å‘é‡åŒ–çš„çŸ©é˜µ  \n",
    "è¯æ±‡åˆ—è¡¨ï¼š   \n",
    "['artificial' 'dislike' 'fascinating' 'hard' 'i' 'intelligence' 'is'\n",
    " 'language' 'learning' 'love' 'machine' 'natural' 'processing']    \n",
    "è¯æ±‡å‘é‡åŒ–çš„çŸ©é˜µï¼š    \n",
    "array([    \n",
    "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1], # æ­£é¢   \n",
    "       [0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1], # æ­£é¢   \n",
    "       [1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0], # è´Ÿé¢   \n",
    "       [0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0], # è´Ÿé¢    \n",
    "      ],dtype=int64)    \n",
    "æœªå¤„ç†çš„æ ·æœ¬æ•°æ®ï¼š    \n",
    "data=[        \n",
    "     [\"I love natural language processing\",\"æ­£é¢\" ],      \n",
    "     [\"Language processing is fascinating\",\"æ­£é¢\" ],        \n",
    "     [\"I dislike artificial intelligence\",\"è´Ÿé¢\"  ],       \n",
    "     [\"Machine learning is hard\",         \"è´Ÿé¢\"  ],         \n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2477926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2.1 å†™æˆè„šæœ¬è®¡ç®—å‡ºæ‰€æœ‰è¯æ±‡åŒ…å«çš„ç‰¹å¾çš„æ¡ä»¶æ¦‚ç‡ åˆç§°ä¼¼ç„¶\n",
    "# å‰é¢å·²ç»å®Œæˆäº†ä¼¼ç„¶æ‰€éœ€çš„å‘é‡åŒ–çŸ©é˜µ ç°åœ¨å¼€å§‹è®¡ç®—å‰©ä½™çš„éƒ¨åˆ†\n",
    "\n",
    "# ç±»åˆ« æ­£é¢ ä¸­æ‰€æœ‰è¯çš„è®¡æ•°æ€»å’Œ\n",
    "import numpy as np\n",
    "dfagg['ç±»åˆ«ä¸‹è¯æ±‡æ€»è®¡æ•°'] = dfagg['è¯æ±‡é¢‘åº¦'].apply(lambda x : np.sum(x))\n",
    "\n",
    "# è®¡ç®—è¯æ±‡è¡¨çš„å¤§å° \n",
    "vocab_len = len(vocabulary) # 13\n",
    "\n",
    "# å¢åŠ æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å‚æ•° Alpha\n",
    "Alpha = 1\n",
    "\n",
    "# æœ€åæ¥è®¡ç®—æ¯ä¸ªè¯çš„æ¦‚ç‡\n",
    "dfagg['æ¡ä»¶æ¦‚ç‡'] = (dfagg['è¯æ±‡é¢‘åº¦']+1)/(dfagg['ç±»åˆ«ä¸‹è¯æ±‡æ€»è®¡æ•°']+Alpha*vocab_len)\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®\n",
    "dfagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocabulary =  ['artificial', 'dislike', 'fascinating' ,'hard' ,'i' ,'intelligence', 'is',\n",
    " 'language', 'learning', 'love', 'machine', 'natural', 'processing']\n",
    "\n",
    "# æ–°æ ·æœ¬\n",
    "new_sample = \"I love machine learning\"\n",
    "\n",
    "# å°†æ–°æ ·æœ¬è½¬æ¢ä¸ºè¯é¢‘å‘é‡\n",
    "def text_to_vector(text, vocabulary):\n",
    "    vector = np.zeros(len(vocabulary),dtype = 'int64')\n",
    "    words =  [text.lower() for text in text.split()]\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            vector[vocabulary.index(word)] += 1\n",
    "    return vector\n",
    "\n",
    "new_sample_vector = text_to_vector(new_sample, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c506684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_posterior(dfagg, new_sample_vector):\n",
    "    posteriors = []\n",
    "    for _, row in dfagg.iterrows():\n",
    "        prior = row['å…ˆéªŒæ¦‚ç‡']\n",
    "        likelihood = np.prod([\n",
    "            row['æ¡ä»¶æ¦‚ç‡'][i] ** new_sample_vector[i]\n",
    "            for i in range(len(new_sample_vector))\n",
    "        ])\n",
    "        posterior = prior * likelihood\n",
    "        posteriors.append(posterior)\n",
    "    return posteriors\n",
    "\n",
    "posteriors = calculate_posterior(dfagg,new_sample_vector)\n",
    "posteriors/sum(posteriors) # array([[0.29333794, 0.70666206]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83cb0c",
   "metadata": {},
   "source": [
    "7. ä½¿ç”¨sklearnåº“ç®—å‡ºæ¦‚ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5014b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿˜è®°åº“ä¸‹æ¨¡å—ä¸­çš„ç±»åˆ« å¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼\n",
    "import sklearn.naive_bayes\n",
    "import inspect\n",
    "inspect.getfile(sklearn.naive_bayes)\n",
    "\n",
    "dir(sklearn.naive_bayes)\n",
    "help(sklearn.naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "mnb = MultinomialNB( alpha = 1.0, # æŒ‡å®šæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å‚æ•°å€¼\n",
    "                     fit_prior =True,  # æ˜¯å¦å­¦ä¹ æä¾›çš„ç±»çš„å…ˆéªŒæ¦‚ç‡ é»˜è®¤True å¦‚æœæ”¹ä¸ºFalse å…ˆéªŒæ¦‚ç‡å°†å…¨éƒ¨ç›¸ç­‰\n",
    "                     # class_prior = [0.2,0.8], # æŒ‡å®šå ä¸å†å—åˆ°æ•°æ®çš„å½±å“ ä»¥åˆ—è¡¨å½¢å¼å¦‚[0.2,0.8]æä¾› ç±»é¡ºåºè‡ªç„¶æ’åº æ‰€ä»¥éœ€è¦æ£€æŸ¥\n",
    "                     # force_alpha = False   # å¼ºåˆ¶force alpha ä¸ºæµ®ç‚¹æ•° é»˜è®¤default=False\n",
    "                   )\n",
    "\n",
    "# è¿›è¡Œç‰¹å¾å·¥ç¨‹ è¿™ä¸ªå’Œä¸Šé¢ä¿æŒä¸€è‡´\n",
    "import pandas as pd\n",
    "data=[\n",
    "     [\"I love natural language processing\",\"æ­£é¢\" ],\n",
    "     [\"Language processing is fascinating\",\"æ­£é¢\" ],\n",
    "     [\"I dislike artificial intelligence\",\"è´Ÿé¢\"  ],\n",
    "     [\"Machine learning is hard\",         \"è´Ÿé¢\"  ],       \n",
    "     ]\n",
    "df = pd.DataFrame(data,columns = ('language','status'))\n",
    "\n",
    "# è°ƒç”¨sklearnä¸­çš„å·¥å…· ä¸ç”¨çš„è¯ å¯ä»¥ä½¿ç”¨ä¸­æ–‡åˆ†è¯å·¥å…· ç»“å·´ åœ¨é¢‘åº¦ç»Ÿè®¡åˆ—è¡¨ä¹Ÿå¯ä»¥ ;è‹±æ–‡ç›´æ¥å¯ä»¥ä½¿ç”¨ å­—ç¬¦ä¸²å¯¹è±¡çš„splitæ–¹æ³•\n",
    "from sklearn.feature_extraction.text import CountVectorizer # ç»Ÿè®¡ å‘é‡åŒ–å™¨\n",
    "\n",
    "# ä¾èµ–äºé¢„è®¾å¯¹è±¡ç”Ÿæˆå®ä¾‹ countvector\n",
    "countvector = CountVectorizer(\n",
    "                        stop_words = None,  # é»˜è®¤ç»Ÿè®¡æ‰€æœ‰åˆ†è¯\n",
    "                        ngram_range = (1,1), # é»˜è®¤ç”Ÿæˆè¯è¢‹æ¨¡å‹ rangeå¦‚æœç­‰äº(1,2)å°±ä»£è¡¨1ä¸ªå•è¯ æˆ–è€…è¿ç»­çš„ä¸¤ä¸ªå•è¯éƒ½è¦ç”¨äºç”Ÿæˆè¯æ±‡è¡¨ å®¹æ˜“å¯¼è‡´æ¨¡å‹å¤ªå¤æ‚ ä¸€èˆ¬æœ€å¤šå°±(1,2)\n",
    "                        token_pattern = r\"(?u)\\b\\w+\\b\" # é»˜è®¤ä½¿ç”¨çš„r'(?u)\\b\\w\\w+\\b' ä¼šå¯¼è‡´ä¸€ä¸ªå­—æ¯çš„ä¸è¢«ç•™ä¸‹æ¥\n",
    "                        # lowercase = False  # é»˜è®¤True å³å…¨éƒ¨å°å†™åŒ–\n",
    "                        # max_df            # å¤§äºè¿™ä¸ªé¢‘åº¦çš„è¯ä¸ç»Ÿè®¡ æµ®ç‚¹æ•°ä»£è¡¨è¯æ±‡åœ¨æ–‡æ¡£ä¸­çš„æœ€å¤§æ¯”ä¾‹ æ•´æ•°ä»£è¡¨è¯æ±‡åœ¨æ–‡æ¡£ä¸­çš„æœ€å¤§å‡ºç°æ¬¡æ•° \n",
    "                        # min_df            # å°äºè¿™ä¸ªé¢‘åº¦çš„è¯ä¸ç»Ÿè®¡ ä¸€èˆ¬èˆå¼ƒä½é¢‘è¯å’Œè¿‡é«˜é¢‘è¯\n",
    "                        # max_features      # æŒ‡å®šæœ€å¤§çš„ç‰¹å¾æ•°é‡ æŒ‰ç…§é¢‘é“æ’åºå–æœ€å¤§çš„ç‰¹å¾\n",
    "                        # vocabulary        # æŒ‡å®šä»…ç»Ÿè®¡è¿™ä¸ªé‡Œé¢çš„å•è¯\n",
    "                        # binary            # äºŒè¿›åˆ¶å‘é‡ æ‰€æœ‰é¢‘åº¦éƒ½å½’1\n",
    "                        # dtype             # æŒ‡å®šè¾“å‡ºæ ¼å¼\n",
    "                        )\n",
    "\n",
    "# è°ƒç”¨countvectorçš„ fitæ–¹æ³• å»è§£æä¸€ä¸ªè¯æ±‡è¡¨ å…¨éƒ¨åˆ†ç±»çš„è¯éƒ½æ‹¿å»è§£æ\n",
    "countvector.fit(df['language'])\n",
    "vocabulary = countvector.get_feature_names_out() # è§£æåæ‰èƒ½è°ƒç”¨è¿™ä¸ªæ–¹æ³•è·å–vocabulary\n",
    "\n",
    "# ä½¿ç”¨transformè½¬åŒ–å‘é‡çŸ©é˜µ è°ƒç”¨toarray()æ–¹æ³•å®ç°è‚‰çœ¼è§‚å¯Ÿ\n",
    "x = countvector.transform(df['language'])\n",
    "x.toarray() \n",
    "\n",
    "# æˆ–è€…ç›´æ¥ä½¿ç”¨fit_transformæ–¹æ³•ä¸€æ¬¡å®ç° èµ‹å€¼CountVectorizerçš„vocabulary å¹¶è¾“å‡ºå‘é‡çŸ©é˜µ\n",
    "x = countvector.fit_transform(df['language'])\n",
    "# print('è¯æ±‡åˆ—è¡¨å¦‚ä¸‹ï¼š\\n',countvector.get_feature_names_out())\n",
    "x.toarray()\n",
    "y = df['status']\n",
    "\n",
    "# è®­ç»ƒæ•°æ®\n",
    "model = mnb.fit(x,y)\n",
    "print('è®­ç»ƒæ•°æ®ä¸­çš„ç±»åˆ«ï¼š',model.classes_)\n",
    "print('è®­ç»ƒæ•°æ®ä¸­çš„ç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡ï¼š',np.exp(model.class_log_prior_)) # å€¼æ˜¯log(0.5) ä¸Šé¢æ±‚å–è¿‡çš„åˆ†ç±»çš„0.5æ¦‚ç‡çš„å¯¹æ•° ä»£è¡¨å…ˆéªŒæ¦‚ç‡ç®—æ³•ä¸€è‡´\n",
    "# print(model.class_prior) # ç”Ÿæˆçš„æ—¶å€™æ²¡æŒ‡å®šæ‰€ä»¥æ˜¯None\n",
    "print('è®­ç»ƒæ•°æ®ä¸­çš„ç±»åˆ«ä¸‹æ¯ä¸ªç‰¹å¾å¯¹åº”çš„æ¡ä»¶æ¦‚ç‡ï¼š\\n',model.feature_log_prob_) # æ•…æ„ä¿ç•™å¯¹æ•° åé¢ä¼šè§£é‡Šä¸ºä»€ä¹ˆæœ€å¥½éƒ½ç”¨å¯¹æ•°\n",
    "\n",
    "# å°†è®­ç»ƒæ•°æ®å°è£…ä¸ºä¸€ä¸ªdf\n",
    "dfmodel = pd.DataFrame({\n",
    "    'ç±»åˆ«':model.classes_ , \n",
    "    'å…ˆéªŒæ¦‚ç‡':np.exp(model.class_log_prior_), \n",
    "    'ç‰¹å¾çš„æ¡ä»¶æ¦‚ç‡':list(np.exp(model.feature_log_prob_)), \n",
    "             })\n",
    "# å¦‚æœè°ƒç”¨æ—¶å‘ç°è¿‡é•¿æ˜¾ç¤ºä¸å®Œå…¨ ç”¨è¿™ä¸ª\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# è°ƒä¸‹ä¸Šé¢æ‰‹å·¥ä»£ç è®¡ç®—å‡ºæ¥çš„å€¼  å‘ç°ä¸¤è€…å®Œå…¨ä¸€è‡´\n",
    "# print(dfagg['æ¡ä»¶æ¦‚ç‡']ï¼‰\n",
    "print('å¯¹åº”çš„æ‰‹å·¥è‡ªå†™ä»£ç çš„ç»“æœï¼š')\n",
    "print([np.log(i) for i in dfagg['æ¡ä»¶æ¦‚ç‡']])\n",
    "print('ä¸¤è€…å®Œå…¨ä¸€è‡´')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2cdad",
   "metadata": {},
   "source": [
    "8. è¡¥å……è¯´æ˜ ä¸ºä»€ä¹ˆå¿…é¡»ä½¿ç”¨å¯¹æ•° è€ŒéåŸå§‹æ¦‚ç‡å€¼      \n",
    "ï¼ˆ1ï¼‰é¿å…æµ®ç‚¹æ•°ä¸‹æº¢ï¼Œæ¦‚ç‡ç›¸ä¹˜å¯¼è‡´å¾ˆå°å¾ˆå°çš„æ•° æœ€åæº¢å‡ºè¿‘0å€¼             \n",
    "ï¼ˆ2ï¼‰å¯¹æ•°å˜åŒ–åçš„å…¬å¼ è¡¨ç°ä»åŸå§‹çš„ä¹˜æ³•       \n",
    "P(x1,x2,...,xnâˆ£y)=P(x1âˆ£y)â‹…P(x2|y)â‹…...â‹…P(xnâˆ£y) æ”¹ä¸º        \n",
    "logP(x1,x2,...,xnâˆ£y)=logP(x1âˆ£y)+logP(x2âˆ£y)+...+logP(xnâˆ£y) åŠ æ³•æ¯”ä¹˜æ³•æé«˜æ•°å€¼ç¨³å®šæ€§              \n",
    "ï¼ˆ3ï¼‰åœ¨ä¼˜åŒ–å’ŒæŸå¤±å‡½æ•°è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¯¹æ•°å€¼å¯ä»¥ç®€åŒ–å¾ˆå¤šè®¡ç®—ã€‚         \n",
    "ï¼ˆ4ï¼‰ æ›´æ˜“äºç†è§£å’Œè§£é‡Š å¯¹æ•°æ¦‚ç‡çš„èŒƒå›´æ˜¯è´Ÿæ— ç©·å¤§åˆ°0ï¼Œè€Œä¸æ˜¯0åˆ°1           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaff6bd",
   "metadata": {},
   "source": [
    "9. å®é™…é¢„æµ‹ä¸‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92578d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = [\"I love machine learning ye\"]\n",
    "x = countvector.transform(new_sample)\n",
    "x.toarray()\n",
    "\n",
    "model.predict_proba(x)\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c209da75",
   "metadata": {},
   "source": [
    "10. é‚®ä»¶åˆ†ç±»å®æˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0615fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥æ•°æ®é›†\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\trec06cn-1.xlsx\")\n",
    "df2 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\trec06cn-2.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff10aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå¹¶æ¸…ç†ç¼ºå¤±å€¼\n",
    "df = pd.concat([df1,df2],axis = 0)\n",
    "df.dropna(subset = ['é‚®ä»¶æ­£æ–‡'],how = 'any',inplace = True)\n",
    "df.groupby('åˆ†ç±»').count()\n",
    "\n",
    "# é¢„å¤„ç†åˆ†è¯\n",
    "import jieba \n",
    "df['åˆ†è¯'] = df['é‚®ä»¶æ­£æ–‡'].apply(lambda x : ' '.join(jieba.cut(x)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d96f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# å¯¼å…¥ä¸‰æ–¹åº“\n",
    "from sklearn.naive_bayes import MultinomialNB # æ¨¡å‹\n",
    "from sklearn.feature_extraction.text import CountVectorizer # è¯æ±‡ç‰¹å¾å¤„ç†\n",
    "from sklearn.pipeline import Pipeline # ç®¡é“\n",
    "from sklearn.model_selection import train_test_split # è®­ç»ƒæµ‹è¯•åˆ†ç¦»\n",
    "from sklearn.model_selection import GridSearchCV # ç½‘æ ¼æœç´¢\n",
    "from sklearn.preprocessing import LabelEncoder   # ç‰¹å¾é¢„å¤„ç†\n",
    "from sklearn.feature_selection import SelectKBest, chi2 # ç‰¹å¾é€‰æ‹©\n",
    "\n",
    "# é¢„å¤„ç†yå€¼\n",
    "label = LabelEncoder()\n",
    "df['åˆ†ç±»æ•°å€¼åŒ–'] = label.fit_transform(df['åˆ†ç±»'])\n",
    "\n",
    "# å»ºç«‹ç®¡é“ å°†å¤šä¸ªæ­¥éª¤åˆä¸ºä¸€ä½“ ç»Ÿä¸€æœç´¢\n",
    "pipe = Pipeline([\n",
    "    ('vector', CountVectorizer()),\n",
    "    ('select', SelectKBest(chi2)), # æ·»åŠ ç‰¹å¾é€‰æ‹©æ­¥éª¤ æ­¤å¤„é‡‡ç”¨å¡æ–¹æ ¡éªŒ\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# æä¾›å‚æ•° ä»¥æ­¥éª¤åç§°__å‚æ•°åç§° åŒºåˆ†å‚æ•°ä»å±äºå“ªä¸ªæ­¥éª¤\n",
    "param_grid = {  'vector__ngram_range':((1,1),(1,2))   ,   # ä¸€èˆ¬æœ€å¤š(1,2) å¦åˆ™å®¹æ˜“è¿‡æ‹Ÿåˆ æ¨¡å‹è¿‡äºå¤æ‚ ä½†æœ‰æ—¶å€™ç¡®å®éœ€è¦(1,3)\n",
    "                'vector__max_df': (0.75, 1.0),                  # max_df ç”¨äºè¿‡æ»¤é«˜é¢‘è¯\n",
    "                'vector__min_df': (1, 2),                       # min_df ç”¨äºè¿‡æ»¤ä½é¢‘è¯\n",
    "                'select__k': (1000, 2000, 3000),                # å¡æ–¹æ ¡éªŒä¸‹é€‰æ‹©çš„ç‰¹å¾æ•°é‡\n",
    "                'nb__alpha': (0.5, 1, 2),                       # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ å³ä¸ºè´å¶æ–¯æ¨¡å‹ä¸­çš„æ­£åˆ™æƒ©ç½šé¡¹ è°ƒæ•´æ¥é¿å…æ¬ è¿‡æ‹Ÿåˆ        \n",
    "                'nb__fit_prior':(True,False),                  #  å¦‚æœæ ·æœ¬æœ¬èº«å¤ªåç¦»ç°å®çš„æ¦‚ç‡ å»ºè®®èµ‹å€¼ è€Œéå­¦ä¹ æ ·æœ¬ä¸­çš„å…ˆéªŒæ¦‚ç‡\n",
    "             }\n",
    "\n",
    "# è®­ç»ƒ æµ‹è¯•åˆ†ç¦»\n",
    "x_train,x_test,y_train,y_test = train_test_split(df['åˆ†è¯'],df['åˆ†ç±»æ•°å€¼åŒ–'],test_size =0.2,random_state = 42)\n",
    "\n",
    "# ç”Ÿæˆç½‘æ ¼æœç´¢å™¨\n",
    "gscv = GridSearchCV(pipe,param_grid,cv=10,scoring = 'precision')\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "gscv.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è°ƒç”¨æœ€å¥½çš„æ¨¡å‹ å†æ¬¡é¢„æµ‹\n",
    "best_nb = gscv.best_estimator_\n",
    "y_predicit = best_nb.predict(x_test)\n",
    "\n",
    "# æŸ¥çœ‹æœ€å¥½æ¨¡å‹çš„å‚æ•°ç»„åˆ\n",
    "print('æœ€å¥½çš„æ¨¡å‹å‚æ•°',gscv.best_params_)\n",
    "print('æœ€å¥½çš„åˆ†æ•°',gscv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('F1åˆ†æ•°',metrics.f1_score(y_test,y_predicit)         )\n",
    "print('å‡†ç¡®ç‡',metrics.accuracy_score(y_test,y_predicit)   )\n",
    "print('å¬å›ç‡',metrics.recall_score(y_test,y_predicit)     )\n",
    "print('ç²¾ç¡®ç‡',metrics.precision_score(y_test,y_predicit)  )\n",
    "print(metrics.classification_report(y_test,y_predicit)     )\n",
    "# average  = 'macro' 'micro' 'weighted' \n",
    "\n",
    "# è¡¥å……å®Œæ•´æŠ¥å‘Šçš„è§£è¯»æ–¹å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8945b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23901b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88febd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•:\", selector.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843570bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['åˆ†ç±»']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d99e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbox_wt.user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b181b",
   "metadata": {},
   "source": [
    "### æœºå™¨å­¦ä¹ \n",
    "###### å››ã€å¿«é€Ÿè°ƒå‚æ‰‹æ®µ\n",
    "ï¼ˆä¸€ï¼‰KFload äº¤å‰éªŒè¯ï¼ˆäºŒï¼‰ç½‘æ ¼æœç´¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5682da20",
   "metadata": {},
   "source": [
    "ï¼ˆä¸€ï¼‰KFload äº¤å‰éªŒè¯   \n",
    "1.äº¤å‰éªŒè¯çš„ä½œç”¨  2.é‡‡ç”¨SKlearnåº“æ¥å®ç°éšæœºçš„äº¤å‰éªŒè¯ åŸºæœ¬æ¨¡æ¿ 3.kè¿‘é‚»ä¸ºä¾‹å±•ç¤ºè°ƒå‚æ•°æ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf512804",
   "metadata": {},
   "source": [
    "1.äº¤å‰éªŒè¯çš„ä½œç”¨     \n",
    "äº¤å‰éªŒè¯ä¸»è¦æ˜¯ä¸ºäº†è¯„ä¼° æ¨¡å‹åœ¨ä¸åŒçš„å­é›†ä¸Šçš„é¢„æµ‹èƒ½åŠ›å’Œç¨³å®šæ€§ï¼ŒåŒæ—¶é€‰æ‹©æœ€ä¼˜çš„æ¨¡å‹å‚æ•° ï¼Œä»¥åŠæ›´ä¸ºç°å®çš„æ•°æ®é‡ä¸è¶³ï¼Œå¦åˆ™ä¹Ÿå¯ä»¥ç”¨train_test_splitå°†ä¸€ä¸ªå¤§å‹çš„æ•°æ®é›†ç›´æ¥åˆ†ä¸ºè®­ç»ƒï¼ŒéªŒè¯ï¼Œæµ‹è¯•    \n",
    "ä¸€èˆ¬æ—¥å¸¸å·¥ä½œä¸­æˆ‘ä»¬ä¼šé€‰æ‹©10æŠ˜ï¼Œå³n_solits = 10    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e289bfb",
   "metadata": {},
   "source": [
    "2.é‡‡ç”¨SKlearnåº“æ¥å®ç°éšæœºçš„äº¤å‰éªŒè¯ åŸºæœ¬æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61bdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼•å…¥kæŠ˜ç±» ç”Ÿæˆå¯¹è±¡\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 10,shuffle = True, random_state = 42)\n",
    "\n",
    "# è¯»å–ä»»æ„æ•°æ®é›†\n",
    "import pandas as pd\n",
    "df_known = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\pokemon_knn.xlsx\",\n",
    "                         sheet_name = 'å·²çŸ¥å®å¯æ¢¦' )\n",
    "\n",
    "# æ•°æ®é›†é‡ç»„\n",
    "df = df_known.iloc[:20,0:7]\n",
    "kf.split(df) # è¿”å›çš„æ˜¯ è®­ç»ƒé›† éªŒè¯é›† çš„ 2ä¸ªç´¢å¼•æ•°ç»„\n",
    "for train_index,validate_index in kf.split(df_known.iloc[:20]):\n",
    "    # print(type(validate_index))\n",
    "    df_train = df.iloc[train_index,:]\n",
    "    df_validate  = df.iloc[validate_index,:]\n",
    "    print(f'æœ¬è½®çš„è®­ç»ƒé›†æ˜¯ \\n {df_train} \\næœ¬è½®çš„æµ‹è¯•é›†æ˜¯ \\n {df_validate}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dbba5",
   "metadata": {},
   "source": [
    "3. kè¿‘é‚»ä¸ºä¾‹å±•ç¤ºè°ƒå‚æ•°æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa160d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–ä»»æ„æ•°æ®é›†\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_known = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\pokemon_knn.xlsx\",\n",
    "                         sheet_name = 'å·²çŸ¥å®å¯æ¢¦' )    \n",
    "    \n",
    "\n",
    "# å…ˆå®Œæˆæµ‹è¯•é›†çš„åˆ†ç¦»\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_raw_train,df_test = train_test_split(df_known,test_size = 0.2, shuffle = True, random_state = 42)\n",
    "\n",
    "# ä»¥kè¿‘é‚»ä¸ºä¾‹ ä½¿ç”¨äº¤å‰éªŒè¯ä¸»è¦æ˜¯åŠ å¼ºéªŒè¯å–n_neighbors = ? æœ€ä¼˜ é¿å…æ­£å¥½å‡ºç° n_neighbors = k æ—¶ ä»…ä»…å¯¹äºæŸç§æ•°æ®é›†åˆ†ç»„æœ€å‡†ç¡®çš„æƒ…å†µ\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# å¾ªç¯kå€¼ åœ¨ä¸åŒçš„æ‹†åˆ†æ—¶çš„ç»“æœ\n",
    "k_ac_score = {}\n",
    "for k in range(1,11,2):\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    \n",
    "    # è®­ç»ƒé›†10æŠ˜æ‹†åˆ†\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_splits = 10,shuffle = True, random_state = 42)\n",
    "    \n",
    "    # å¼€å§‹10æŠ˜çš„å¾ªç¯\n",
    "    ac_score_list = []\n",
    "    for train_index,validate_index in kf.split(df_raw_train):\n",
    "        # print('å¼€å§‹æœ¬è½®çš„äº¤å‰éªŒè¯')\n",
    "        df_train    = df_raw_train.iloc[train_index,:]\n",
    "        df_validate = df_raw_train.iloc[validate_index,:]\n",
    "        # print(f'æœ¬è½®çš„è®­ç»ƒé›†æ˜¯ \\n {df_train} \\næœ¬è½®çš„æµ‹è¯•é›†æ˜¯ \\n {df_validate}')\n",
    "        # è®­ç»ƒæ¨¡å‹\n",
    "        model = knn.fit(df_train.loc[:,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»'] , df_train['ä¸»åˆ†ç±»'])\n",
    "        ac_score = round(accuracy_score(df_validate['ä¸»åˆ†ç±»'], model.predict(df_validate.loc[:,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»']) ),2)\n",
    "        ac_score_list.append(ac_score)\n",
    "\n",
    "    ac_score = round(np.mean(np.array(ac_score_list))    ,2)\n",
    "    print(f'å½“kå€¼ä¸º{k}æ—¶ï¼Œå‡†ç¡®ç‡ä¸º{ac_score},æ¯ä¸ªæŠ˜çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º{ac_score_list}')\n",
    "    k_ac_score[k] = (ac_score,ac_score_list)\n",
    "\n",
    "# å…¨éƒ¨çš„kå€¼éƒ½åšè¿‡10æŠ˜è¿ç®—å è·å–åˆ°äº†å­—å…¸ åœ¨ç”¨maxå†…ç½®å‡½æ•° keyå‚æ•°å¯¹äºä¼ é€’çš„å€¼åšè§£æåå†æ¯”è¾ƒ è¿”å›maxçš„é”®ä¸ºå€¼çš„èƒ½åŠ›æ‰¾åˆ°æœ€åˆé€‚çš„kå€¼\n",
    "max_key = max(k_ac_score, key=lambda k: k_ac_score[k][0])\n",
    "print(f'æœ€åˆé€‚çš„kå€¼ä¸º{max_key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1e158",
   "metadata": {},
   "source": [
    "ï¼ˆäºŒï¼‰ç½‘æ ¼æœç´¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b0ddf",
   "metadata": {},
   "source": [
    "1. ç½‘æ ¼æœç´¢çš„å®šä¹‰     \n",
    "åŸºäºäº¤å‰éªŒè¯çš„å¤åˆæŠ€æœ¯ï¼Œç”¨äºç³»ç»Ÿåœ°éå†å¤šç§ç®—æ³•çš„å‚æ•°ç»„åˆï¼Œä»è€Œæ‰¾åˆ°æœ€ä½³çš„å‚æ•°è®¾ç½®        \n",
    "ç½‘æ ¼æœç´¢çš„æœ¬è´¨å¯ä»¥è®¤ä¸ºæ˜¯ä¸Šé¢ä»¥knnç®—æ³•ä¸ºä¾‹çš„è¶…å‚æ•°è°ƒä¼˜çš„æ¨¡å—åŒ–è„šæœ¬ æ—¢ç„¶éªŒè¯éƒ½æ˜¯è¦çŸ¥é“åœ¨ä»€ä¹ˆç®—æ³•ä¸‹ åšå“ªäº›å‚æ•° é‡‡å–å‡ æŠ˜çš„éªŒè¯æ–¹å¼ è‡ªç„¶å¯ä»¥å®šä¹‰ä¸ºä¸€ä¸ªå¯¹è±¡ æä¾›å‚æ•° è‡ªåŠ¨çš„å»åš éªŒè¯ï¼Œçœæ‰äº†é‡å¤å†™å¾ªç¯çš„éº»çƒ¦      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99bff70",
   "metadata": {},
   "source": [
    "2. sklearnä¸­çš„GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb1bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# ç”Ÿæˆç½‘æ ¼æœç´¢å¯¹è±¡\n",
    "knn = KNeighborsClassifier()\n",
    "gscv = GridSearchCV(knn,{'n_neighbors':range(1,10,2)},cv = 10)\n",
    "\n",
    "\n",
    "# è¯»å–ä»»æ„æ•°æ®é›†\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_known = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\pokemon_knn.xlsx\",\n",
    "                         sheet_name = 'å·²çŸ¥å®å¯æ¢¦' )    \n",
    "    \n",
    "\n",
    "# å…ˆå®Œæˆæµ‹è¯•é›†çš„åˆ†ç¦»\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_raw_train,df_test = train_test_split(df_known,test_size = 0.2, shuffle = True, random_state = 42)\n",
    "gscv.fit(df_raw_train.loc[:,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»'],df_raw_train['ä¸»åˆ†ç±»'])\n",
    "\n",
    "pd.DataFrame(gscv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe842066",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94020926",
   "metadata": {},
   "source": [
    "### æœºå™¨å­¦ä¹ \n",
    "###### äº”ã€è¯„ä¼°æ¨¡å‹çš„æ–¹å¼\n",
    "ï¼ˆä¸€ï¼‰å‡†ç¡®ç‡ accuracy_score ï¼ˆäºŒï¼‰å¬å›ç‡æŒ‡æ ‡ Recall åˆç§°çµæ•åº¦ (Sensitivity)     \n",
    "ï¼ˆä¸‰ï¼‰ç²¾ç¡®ç‡æŒ‡æ ‡ Precision  ï¼ˆå››ï¼‰F1åˆ†æ•° F1Score\n",
    "ï¼ˆäº”ï¼‰rocæ›²çº¿ ï¼ˆå…­ï¼‰rucæ›²çº¿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b190560",
   "metadata": {},
   "source": [
    "ï¼ˆä¸€ï¼‰å‡†ç¡®æ€§æŒ‡æ ‡ Accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20d673",
   "metadata": {},
   "source": [
    "1. å®šä¹‰ä¸ä½¿ç”¨æƒ…å†µ   \n",
    "Accuracy= TP+TN/TP+TN+FP+FN  ä»»æ„åˆ†ç±»æ­£ç¡®é¢„æµ‹çš„æ•°/æ ·æœ¬æ€»æ•°é‡    \n",
    "è¯„ä¼°æ–¹æ³•é€‚ç”¨äºäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»é—®é¢˜ï¼Œå¹¶ä¸”åœ¨å„ç±»æ ·æœ¬å‡è¡¡çš„æƒ…å†µä¸‹ç‰¹åˆ«æœ‰æ•ˆã€‚    \n",
    "å¯¹äºä¸å¹³è¡¡çš„åˆ†ç±»é—®é¢˜ï¼Œæ¯”å¦‚ä¸€ä¸ªåˆ†ç±»å æ¯”90%çš„æ•°æ®é›†ï¼Œé‚£ä¹ˆæˆ‘åˆ¤æ–­æ‰€æœ‰çš„æ•°æ®éƒ½æ˜¯æŸä¸€ç±»å…¶å‡†ç¡®æ€§å®Œå…¨å¯èƒ½é«˜äºæ¨¡å‹çš„é¢„æµ‹ç»“æœ    \n",
    "æ‰€ä»¥è¿˜è¦å¼•å…¥ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ã€å¬å›ç‡ï¼ˆRecallï¼‰ã€F1 åˆ†æ•°ç­‰ç»¼åˆè¯„ä¼°   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f628bac",
   "metadata": {},
   "source": [
    "2. é‡‡ç”¨SKlearnåº“æ¥ä½¿ç”¨accuracyæŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea840f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c41731",
   "metadata": {},
   "source": [
    "ï¼ˆäºŒï¼‰å¬å›ç‡æŒ‡æ ‡ Recall åˆç§°çµæ•åº¦ (Sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30951671",
   "metadata": {},
   "source": [
    "1. å®šä¹‰ä¸ä½¿ç”¨æƒ…å†µ   \n",
    "Recall = TP/TP+FN æ­£ä¾‹é¢„æµ‹æ­£ç¡®çš„æ•°é‡/å®é™…æ­£ä¾‹çš„æ•°é‡   \n",
    "ä¸å¹³è¡¡æ ·æœ¬åœ¨ç°å®ä¸–ç•Œä¸­è¾ƒå¤š å‡†ç¡®ç‡å¯¼è‡´ä¸¥é‡çš„è¯¯å¯¼ æ•…è€Œè¿›ä¸€æ­¥å¼•å…¥ å¬å›ç‡ å…¸å‹çš„æ¯”å¦‚åˆ†ç±»ç½ªçŠ¯ä¸å¥½äºº åˆ†ç±»ç—…äººå’Œéç—…äºº éƒ½æ˜¯ä½ ä¸å…³æ³¨çš„è´Ÿä¾‹ï¼ˆå¥½äººï¼Œéç—…äººï¼‰å æ¯”æ ·æœ¬ä¸­çš„ç»å¤§å¤šæ•°ï¼Œå¯¼è‡´äº†ä¸å¹³è¡¡çš„å‘ç”Ÿï¼Œä½†æ˜¯é”™è¯¯æ”¾è¿‡é‚£äº›äººçš„é—®é¢˜å¾ˆå¤§ æ‰€ä»¥ä¼šæœ‰å¬å›ç‡ï¼Œä¹Ÿå³å¯¹äºè¯†åˆ«å‡ºçš„æ­£ä¾‹çš„å æ¯”   \n",
    "å¬å›ç‡ä¾§é‡äºæœ€å¤§é™åº¦åœ°è¯†åˆ«å‡ºæ‰€æœ‰æ­£ç±»æ ·æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc57a1",
   "metadata": {},
   "source": [
    "2. é‡‡ç”¨SKlearnåº“æ¥ä½¿ç”¨RecallæŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e9d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score??\n",
    "# average  = 'macro' 'micro' 'weighted' \n",
    "# â€¦â€¦è¿˜æœ‰å¾ˆå¤šä¸åŒçš„å¹³å‡å€¼ç®—æ³•ï¼Œè¿™æ˜¯å‡†ç¡®ç‡æ²¡æœ‰çš„å‚æ•° ä½†æ˜¯è¿™ä¸ªå¿…é¡»æä¾›ï¼Œå¦åˆ™æ— æ³•è§£å†³å¤šåˆ†ç±»æƒ…å†µä¸‹çš„æŒ‡æ ‡å¹³å‡å€¼è®¡ç®—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8844f",
   "metadata": {},
   "source": [
    "ï¼ˆä¸‰ï¼‰ç²¾ç¡®ç‡æŒ‡æ ‡ Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992bd6f9",
   "metadata": {},
   "source": [
    "1. å®šä¹‰ä¸ä½¿ç”¨æƒ…å†µ    \n",
    "Precision = TP/TP+FP è¯†åˆ«è¿‡ç¨‹ä¸­çœŸæ­£ä¾‹ å æ¯” æ¨¡å‹é¢„æµ‹çš„æ­£ä¾‹     \n",
    "ä¸æ–­å¼ºåŒ–æ¨¡å‹å‡†ç¡®çš„è¯†åˆ«å‡ºå®é™…æ­£ä¾‹ ä¹Ÿå¯èƒ½å¯¼è‡´ä¸æ˜¯æ­£ä¾‹çš„è¢«åˆ†ç±»ä¸ºæ­£ä¾‹ å…¸å‹çš„ä¸åº”å¦‚æ­¤çš„åœºæ™¯ä¸ºé‚®ä»¶åˆ†ç±»ï¼Œé‡è¦é‚®ä»¶è¢«åˆ†ç±»è‡³åƒåœ¾å¯¼è‡´é”™è¿‡      \n",
    "ç²¾ç¡®ç‡æŒ‡æ ‡ ä¾§é‡äºæœ€å¤§é™åº¦å‡å°‘è¯¯æŠ¥     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac06c5",
   "metadata": {},
   "source": [
    "2. é‡‡ç”¨SKlearnåº“æ¥ä½¿ç”¨PrecisionæŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c67565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score??\n",
    "# average  = 'macro' 'micro' 'weighted' \n",
    "# â€¦â€¦è¿˜æœ‰å¾ˆå¤šä¸åŒçš„å¹³å‡å€¼ç®—æ³•ï¼Œè¿™æ˜¯å‡†ç¡®ç‡æ²¡æœ‰çš„å‚æ•° ä½†æ˜¯è¿™ä¸ªå¿…é¡»æä¾›ï¼Œå¦åˆ™æ— æ³•è§£å†³å¤šåˆ†ç±»æƒ…å†µä¸‹çš„æŒ‡æ ‡å¹³å‡å€¼è®¡ç®—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9dfbb",
   "metadata": {},
   "source": [
    "ï¼ˆå››ï¼‰F1åˆ†æ•° F1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46280d",
   "metadata": {},
   "source": [
    "1. å®šä¹‰ä¸ä½¿ç”¨æƒ…å†µ    \n",
    "F1Score = 2Ã—ï¼ˆPrecisionÃ—Recallï¼‰ / ï¼ˆPrecision+Recallï¼‰   \n",
    "å¬å›ç‡å’Œç²¾ç¡®ç‡æ˜¯ä¸€å¯¹æ­¤æ¶ˆå½¼é•¿çš„æŒ‡æ ‡ï¼Œè€ŒF1åˆ†æ•°åœ¨ç²¾ç¡®ç‡å’Œå¬å›ç‡ä¹‹é—´æä¾›äº†ä¸€ä¸ªå¹³è¡¡    \n",
    "ç‰¹åˆ«é€‚åˆäºé‚£äº›å¯¹ç²¾ç¡®ç‡å’Œå¬å›ç‡åŒç­‰é‡è§†çš„åœºæ™¯    \n",
    "\n",
    "ä¸Šè¿°ç®—æ³•ä¸ºè°ƒå’Œå¹³å‡å€¼ï¼Œåˆ©ç”¨äº†æœŸå¼ºè°ƒè¾ƒå°æ•°å€¼çš„é‡è¦æ€§æ—¶è¿™ä¸€ä¸ªç‰¹æ€§ï¼Œä»»ä½•ä¸€ä¸ªæŒ‡æ ‡è¿‡å°ï¼Œéƒ½å°†å¯¼è‡´æ•´ä½“å€¼å˜å° å”¯æœ‰ä¸¤è€…éƒ½ææ‰èƒ½ä½¿F1åˆ†æ•°å˜é«˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fd7614",
   "metadata": {},
   "source": [
    "2. é‡‡ç”¨SKlearnåº“æ¥ä½¿ç”¨F1ScoreæŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d981e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score??\n",
    "# average  = 'macro' 'micro' 'weighted' \n",
    "# â€¦â€¦è¿˜æœ‰å¾ˆå¤šä¸åŒçš„å¹³å‡å€¼ç®—æ³•ï¼Œè¿™æ˜¯å‡†ç¡®ç‡æ²¡æœ‰çš„å‚æ•° ä½†æ˜¯è¿™ä¸ªå¿…é¡»æä¾›ï¼Œå¦åˆ™æ— æ³•è§£å†³å¤šåˆ†ç±»æƒ…å†µä¸‹çš„æŒ‡æ ‡å¹³å‡å€¼è®¡ç®—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234bcefc",
   "metadata": {},
   "source": [
    "### æœºå™¨å­¦ä¹   \n",
    "###### é™„å½• è¯¾åä¹ é¢˜   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d13c4",
   "metadata": {},
   "source": [
    "###### ç¬¬5å· æœºå™¨å­¦ä¹  ç¬¬äº”åä¸‰å›   \n",
    "1.è¯·ä½¿ç”¨å¾ªç¯æµ‹è¯•å½“Kçš„å€¼ä¸º 1~19 æ—¶ï¼Œä¸åŒçš„Kå€¼é¢„æµ‹çš„ä¸»åˆ†ç±»å¹¶è§‚å¯Ÿç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. è·å–æ•°æ®\n",
    "import pandas as pd\n",
    "df_known = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\pokemon_knn.xlsx\",\n",
    "                         sheet_name = 'å·²çŸ¥å®å¯æ¢¦' )\n",
    "df_unknown = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\pokemon_knn.xlsx\",\n",
    "                         sheet_name = 'ç¥ç§˜å®å¯æ¢¦' )\n",
    "# 2. åˆ†ä¸€ä¸ªè®­ç»ƒæµ‹è¯•ç”¨äºé¢å¤–çš„ä¹ é¢˜\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train,df_test = train_test_split(df_known,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef783c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.å®šä¹‰ä¸€ä¸ªknnç®—æ³•\n",
    "import math\n",
    "def knn(row,x_unknown):\n",
    "    x_known = row['å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»']\n",
    "    return math.sqrt(sum([(x_unknown[i]-x_known[i])**2 for i in range(0,x_unknown.size)]) )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113edf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. applyäºæ¯ä¸€è¡Œ \n",
    "df_known['è·ç¦»'] = df_known.apply(knn,axis =1,x_unknown = df_unknown.loc[0,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»'].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d719641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.æ’åºå–kå€¼ ç®—å‡ºä¼—æ•°å€¼ è·å¾—åˆ†ç±»\n",
    "df_known.sort_values(by = 'è·ç¦»',axis = 0,ascending = True).head(5)['ä¸»åˆ†ç±»'].mode()\n",
    "\n",
    "# 6.å¾ªç¯å®Œæˆä¹ é¢˜1\n",
    "for k in range(1,20):\n",
    "    classifier = df_known.sort_values(by = 'è·ç¦»',axis = 0,ascending = True).head(k)['ä¸»åˆ†ç±»'].mode()[0]\n",
    "    # print(f'å½“kå€¼ä¸º{k}ï¼Œè¿™ä¸ªå®å¯æ¢¦çš„åˆ†ç±»ä¸º{classifier}')\n",
    "    \n",
    "# 7.é—®é¢˜å‡çº§ï¼Œé¢„æµ‹å¤šä¸ªæœªçŸ¥çš„kå€¼ åœ¨ä¸åŒçš„kå€¼æ—¶çš„å€¼ ä½œä¸ºåˆ—æ”¾è¿›å»æµ‹è¯•é›†\n",
    "classifier_list = []\n",
    "for index,row in df_test.iterrows():\n",
    "    df_train['è·ç¦»'] = df_train.apply(knn,axis =1,x_unknown = row['å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»'])\n",
    "    classifier = [df_train.sort_values(by = 'è·ç¦»',axis = 0 ,ascending = True).head(k)['ä¸»åˆ†ç±»'].mode()[0]  for k in range(1,4)]\n",
    "    classifier_list.append(classifier)\n",
    "\n",
    "df_test['åˆ†ç±»'] = classifier_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a172d4",
   "metadata": {},
   "source": [
    "2.è¯»å–ç«ç„°çº¹ç« æ•°æ®ï¼Œé¢„æµ‹æœªçŸ¥çš„äººç‰©çš„èŒä¸š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afd4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. è·å–æ•°æ®\n",
    "import pandas as pd\n",
    "df_known = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\fire_emblem_1.xlsx\",\n",
    "                         sheet_name = 'å·²çŸ¥è§’è‰²' )\n",
    "df_unknown = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\fire_emblem_1.xlsx\",\n",
    "                         sheet_name = 'æœªçŸ¥è§’è‰²' )\n",
    "\n",
    "# 2. åˆ†ä¸€ä¸ªè®­ç»ƒæµ‹è¯•ç”¨äºé¢å¤–çš„ä¹ é¢˜\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train,df_test = train_test_split(df_known,test_size = 0.3,#stratify = df_known['èŒä¸š']\n",
    "                                   )\n",
    "\n",
    "# 3. è°ƒå–knnç®—æ³•\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "x = df_train.loc[:,'åŠ›é‡':'è¡ŒåŠ¨ç‚¹']\n",
    "y = df_train.loc[:,'èŒä¸š']\n",
    "\n",
    "# 4.ç”Ÿæˆæ¨¡å‹\n",
    "model = knn.fit(x,y)\n",
    "for k in range(1,8,2):\n",
    "    # ä¸‹é¢ä¸¤å¥è¯­å¥é€ æˆçš„kå€¼æ”¹å˜æ˜¯ç­‰ä»·çš„ï¼ŒåŒæ—¶æ”¹å˜knnå’Œmodelçš„ï¼Œä¸”ä»–ä»¬ä¸ºåŒä¸€ç±»å¯¹è±¡\n",
    "    knn.n_neighbors = k \n",
    "    model.n_neighbors = k \n",
    "    \n",
    "    # 5.é¢„æµ‹\n",
    "    y_predicted = model.predict(df_test.loc[:,'åŠ›é‡':'è¡ŒåŠ¨ç‚¹'])\n",
    "    \n",
    "    # 6. æ ¸éªŒå‡†ç¡®æ€§\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    ac_score = accuracy_score(df_test['èŒä¸š'],y_predicted)\n",
    "    print(f'å½“kå€¼ä¸º{k}ï¼Œç²¾ç¡®åº¦ä¸º{round(ac_score,3)*100}%')\n",
    "\n",
    "    # 7. å®é™…é¢„æµ‹ \n",
    "    y_predicted = model.predict(df_unknown.loc[:,'åŠ›é‡':'è¡ŒåŠ¨ç‚¹'])\n",
    "    print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc71b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf03f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb04b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. è·å–æ•°æ®\n",
    "import pandas as pd\n",
    "df_known = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\pokemon_knn.xlsx\",\n",
    "                         sheet_name = 'å·²çŸ¥å®å¯æ¢¦' )\n",
    "df_unknown = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\dataå­˜å‚¨jupyterç”¨çš„ä¸ªæ–‡ä»¶ç±»å‹æ•°æ®\\pokemon_knn.xlsx\",\n",
    "                         sheet_name = 'ç¥ç§˜å®å¯æ¢¦' )\n",
    "# 2. åˆ†ä¸€ä¸ªè®­ç»ƒæµ‹è¯•ç”¨äºé¢å¤–çš„ä¹ é¢˜\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train,df_test = train_test_split(df_known,test_size = 0.2)\n",
    "\n",
    "# 3.å®šä¹‰ä¸€ä¸ªknnç®—æ³•\n",
    "import math\n",
    "def knn(row,x_unknown):\n",
    "    x_known = row['å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»']\n",
    "    return math.sqrt(sum([(x_unknown[i]-x_known[i])**2 for i in range(0,x_unknown.size)]) )   \n",
    "\n",
    "# 4. applyäºæ¯ä¸€è¡Œ \n",
    "df_known['è·ç¦»'] = df_known.apply(knn,axis =1,x_unknown = df_unknown.loc[0,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»'].squeeze())\n",
    "\n",
    "# 5.æ’åºå–kå€¼ ç®—å‡ºä¼—æ•°å€¼ è·å¾—åˆ†ç±»\n",
    "df_known.sort_values(by = 'è·ç¦»',axis = 0,ascending = True).head(5)['ä¸»åˆ†ç±»'].mode()\n",
    "\n",
    "# 6.å¾ªç¯å®Œæˆä¹ é¢˜1\n",
    "for k in range(1,20):\n",
    "    classifier = df_known.sort_values(by = 'è·ç¦»',axis = 0,ascending = True).head(k)['ä¸»åˆ†ç±»'].mode()[0]\n",
    "    # print(f'å½“kå€¼ä¸º{k}ï¼Œè¿™ä¸ªå®å¯æ¢¦çš„åˆ†ç±»ä¸º{classifier}')\n",
    "    \n",
    "# 7.é—®é¢˜å‡çº§ï¼Œé¢„æµ‹å¤šä¸ªæœªçŸ¥çš„kå€¼ åœ¨ä¸åŒçš„kå€¼æ—¶çš„å€¼ ä½œä¸ºåˆ—æ”¾è¿›å»æµ‹è¯•é›†\n",
    "classifier_list = []\n",
    "for index,row in df_test.iterrows():\n",
    "    df_train['è·ç¦»'] = df_train.apply(knn,axis =1,x_unknown = row['å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»'])\n",
    "    classifier = [df_train.sort_values(by = 'è·ç¦»',axis = 0 ,ascending = True).head(k)['ä¸»åˆ†ç±»'].mode()[0]  for k in range(1,4)]\n",
    "    classifier_list.append(classifier)\n",
    "\n",
    "df_test['åˆ†ç±»'] = classifier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['è·ç¦»'] = df_train.apply(knn,axis =1,x_unknown = row[0,'å¯¹é˜µè™«ç³»':'å¯¹é˜µæ°´ç³»'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fbd4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
