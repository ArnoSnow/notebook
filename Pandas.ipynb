{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124f6436",
   "metadata": {},
   "source": [
    "### 一、Pandas，基于Numpy制造\n",
    "###### 存在几个优势，后续篇章将一一解释具体\n",
    "###### 1、灵活索引，按名对齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "深圳销量 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\销量统计.xlsx\",sheet_name ='深圳销量' ,header = 1,index_col = 0 )\n",
    "上海销量 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\销量统计.xlsx\",sheet_name ='上海销量' ,header = 1,index_col = 0 )\n",
    "广州销量 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\销量统计.xlsx\",sheet_name ='广州销量' ,header = 1,index_col = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(上海销量)\n",
    "print(深圳销量)\n",
    "print(广州销量)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 灵活索引 数组必须数维度坐标值才能索引，pandas可以直接用名称索引\n",
    "上海销量[['4月','5月']] # 类切片\n",
    "上海销量.loc['按摩椅']  # 行切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照列名称对齐，进行相加，而非数组的相加逻辑对位计算\n",
    "深圳销量+上海销量+广州销量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c49bca",
   "metadata": {},
   "source": [
    "### 一、Pandas，基于Numpy制造\n",
    "###### 2、时间序列处理能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74178b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前提必须让时间成为序列，表现出来就是乱序时间自动排序了\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\销量统计.xlsx\",sheet_name = '调货明细',header = 0,index_col = 0)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6348c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 切片索引达成某个区间内的订单情况\n",
    "df1.loc['2021-01-01':'2021-01-31'] \n",
    "df1[df1.index.date == '2021-05-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e8668",
   "metadata": {},
   "source": [
    "### 二、Pandas读取excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc973b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_odd(x):\n",
    "    return x%2 == 1\n",
    "\n",
    "import pandas as pd\n",
    "pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house.xlsx\",\n",
    "              sheet_name = '房屋成交信息', # sheet_name = 0 # sheet_name = [0,1] 如果是列表那么返回的是字典，每个值是一个df\n",
    "              engine = 'openpyxl', # 引擎还蛮多的，['xlrd', 'openpyxl', 'odf', 'pyxlsb'] xlrd好像因为版本问题会出错\n",
    "              header = 2, # header = [1,2,3],单int指定一列标题，int列表指定多行为标题，默认为0，可以None\n",
    "              #names = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16'], #注意和usercols无关，遵循原表字段长度\n",
    "              usecols = 'D:K', # usercols = [1,2,3,4,5,6] # 单int只要一列，int列表要多列，字母压缩连续行D:K，默认None\n",
    "              index_col = [0,1],  # index_col = 0  # index_col = '4' names命名后的字段名称\n",
    "              #skiprows = is_odd # 单int等于跳过的行，其实是比较给出的行值与单int是否相等，True就会剔除\n",
    "              skiprows = lambda x : x%2 == 1 # 需要特别注意的是，skiprpws跳过的行，也会导致header的行数变化，header是根据skiprows之后返回的数据条进行判断的\n",
    "             )\n",
    "\n",
    "#参数释义\n",
    "#io         指定读取的文件地址，请加上r避免转义\n",
    "#sheet_name 指定了读取哪张表格，可以int列表读成字典，int为键，df为值\n",
    "#engine     指定读取excel时使用的引擎\n",
    "#header     指定行为标题行，不使用文件自带的标题行写None；单个数字指定一行为标题行；int列表指定多行为标题行，此时name和usercols失效\n",
    "#name       指定了标题行，那么标题行的名称会被替换为这个，写None，则所有行正常使用，额外增加一个标题行；注意usercols无关，遵循原表字段长度，所以我放在usecols上面\n",
    "#usercols   指定使用的列，字母索引切片，优点在于压缩连续列，如H,A:K，如果已经包含不会重复，只会合并后按照原表格顺序呈现\n",
    "#index_col  指定已使用的列哪个为索引列，基于指定后的usercols中的字段名称，和列顺序来判断，所以我觉得应该放在usecols下面\n",
    "#skiprows   指定需要跳过的行，可以接受函数为参数，所以对于相对简单的函数可以采取lambda的方式来处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089ddb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 1. 读取第1号工作表；\n",
    "# 2. 读取名为“资源描述信息”的工作表；\n",
    "# 3. 同时读取前两张工作表，显示第二张工作表的数据\n",
    "import pandas as pd\n",
    "pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_01.xlsx\",sheet_name = '资源描述信息')\n",
    "pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_01.xlsx\",sheet_name = [0,1])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e143fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题2\n",
    "# 请使用read_excel方法按照以下要求读取文件：\n",
    "# 1. 读取第一张工作表，设置第二行为表头；\n",
    "# 2. 读取第二张工作表，设置前三行为表头；\n",
    "# 3. 读取第一张工作表，设置数字列名。\n",
    "import pandas as pd\n",
    "pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_01.xlsx\",sheet_name = 0,header = 1)\n",
    "pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_01.xlsx\",sheet_name = 1,header = [0,1,2])\n",
    "pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_01.xlsx\",sheet_name = 0,header = None)\n",
    "# 无标题又不指示names参数即为默认数字表头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e0992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题3\n",
    "# 读取第二张工作表，将列名按顺序指定为以下中文列名：\n",
    "# 序号 展期 展厅 展品 展览内容\n",
    "import pandas as pd\n",
    "names = ['序号','展期','展厅','展品','展览内容']\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_02.xlsx\",sheet_name = 1,header = 0,names = names)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18db6db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题4\n",
    "#1. 读取第二张工作表中 E列 的所有数据；\n",
    "#2. 读取第二张工作表中 A列~C列、E列 的所有数据；\n",
    "#3. 读取第二张工作表中 第2列、第4列 的数据；\n",
    "#4. 读取第二张工作表中 第2列~第4列 的数据。\n",
    "import pandas as pd\n",
    "pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_02.xlsx\",sheet_name = 1,usecols = 'E')\n",
    "pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_02.xlsx\",sheet_name = 1,usecols = 'A:C,E')\n",
    "pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_02.xlsx\",sheet_name = 1,usecols = [1,3])\n",
    "pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_02.xlsx\",sheet_name = 1,usecols = [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc2362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题5\n",
    "# 请使用read_excel方法按照以下要求读取文件：\n",
    "# 1. 读取第二张工作表，使用 “ID” 列作为索引；\n",
    "# 2. 读取第二张工作表，使用第2列作为索引；\n",
    "# 3. 读取第二张工作表中 第3列、第4列，使用第3列作为索引。\n",
    "import pandas as pd\n",
    "pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_03.xlsx\",sheet_name = 1,index_col = 'ID')\n",
    "pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_03.xlsx\",sheet_name = 1,index_col = 1)\n",
    "pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_03.xlsx\",sheet_name = 1,usecols= [2,3],index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b12ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题6\n",
    "# 1. 读取第二张工作表，设置列名为：自增ID 测站编目 采集时间 时水位\n",
    "# 设置“自增ID”为索引，跳过原始表的第1行表头； \n",
    "import pandas as pd\n",
    "pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_03.xlsx\",\n",
    "              sheet_name = 1,\n",
    "              header = 0,\n",
    "              names = ['自增ID','测站编目','采集时间','时水位'],\n",
    "              index_col = '自增ID')\n",
    "# 2. 读取第二张工作表，设置列名为：自增ID 测站编目 采集时间 时水位\n",
    "# 设置“测站编目”为索引，读取B列~D列，跳过原始表的第1行表头、跳过原始表的第5行~第10行；\n",
    "import pandas as pd\n",
    "pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_03.xlsx\",\n",
    "              sheet_name = 1,\n",
    "              header = 0,\n",
    "              names = ['自增ID','测站编目','采集时间','时水位'],\n",
    "              usecols = 'B:D',\n",
    "              index_col = '测站编目',\n",
    "              skiprows = [0,4,5,6,7,8,9]\n",
    "             )\n",
    "# 3. 读取第二张工作表，设置列名为：自增ID 测站编目 采集时间 时水位，按 40% 随机抽样数据。跳过60%的行\n",
    "import pandas as pd\n",
    "import random\n",
    "pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\python_03_12_03.xlsx\",\n",
    "              sheet_name = 1,\n",
    "              header = 0,\n",
    "              names = ['自增ID','测站编目','采集时间','时水位'],\n",
    "              skiprows = lambda x: random.randint(1,10)>4\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45136be2",
   "metadata": {},
   "source": [
    "### 三、Series数据结构\n",
    "###### 1、dataframe 由若干的Series组成，索引组合所有的Series，由此可知，pd中行与列一样重要，而不像array优先高纬度往低纬度走\n",
    "（1）df[]只能完成列的筛选，不能做行筛选,这代表行列一样重要\n",
    "（2）df[]不能筛选index列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116349e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 先读取需要演示的数据\n",
    "dfhouse = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house.xlsx\",sheet_name = 1,usecols = 'B:D',index_col = 0)\n",
    "dfhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44fe9d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 如何理解每一个df是用若干个Series组成的，切列用dfhouse[]完成，每个切出来的列type都是一个series\n",
    "print(type(dfhouse['社区价值']).__name__) # 数据类型为Series\n",
    "print(dfhouse['社区名称']) # 不会报错\n",
    "print(dfhouse['社区价值']) # 不会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c18376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dfhouse[]无法直接切片行，这是和array一个巨大的区别，array是可以通过索引切片拿出一整行的\n",
    "dfhouse[1111027377718] # 报错 KeyError: 1111027377718\n",
    "dfhouse['社区编号'] # 成为index_col的series也无法被索引出来，没有这个key,也没有这个数组，想拿到他请看下面的写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00771c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 索引的列需要用index打印出来，直接切片也是报错\n",
    "print(type(dfhouse.index).__name__) # 数据类型为Int64Index\n",
    "print(dfhouse.index) # index类型的一个一维列表\n",
    "print(type(dfhouse.values).__name__) # 数据类型为ndarray\n",
    "print(dfhouse.values) # 是个二维数组，1维每增加一个字段，长度就会+1\n",
    "# 这里其实也解释了df的结构还是1个index+1个二维的数组，这个和series是一致的，我们现在回去看series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e227bed",
   "metadata": {},
   "source": [
    "### 三、Series数据结构\n",
    "###### 2、Series类型对象由 “索引对象” 和 “数组对象” 构成，通过values属性与index属性可以获取其中的数据，并简易展示如何修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先读取需要演示的数据\n",
    "dfhouse = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house.xlsx\",sheet_name = 1,usecols = 'B:D',index_col = 0)\n",
    "# 展示index\n",
    "print('社区名称这个series的索引对象:',dfhouse['社区名称'].index           ,end = '\\n\\n')\n",
    "print('index取series的索引对象的类型:',type(dfhouse['社区名称'].index).__name__     ,end = '\\n\\n')\n",
    "print('社区名称这个series的索引对象:',dfhouse['社区名称'].keys            ,end = '\\n\\n')\n",
    "print('keys取series的索引对象的类型:',type(dfhouse['社区名称'].keys).__name__      ,end = '\\n\\n')\n",
    "# 展示values\n",
    "print('社区名称这个series的数组对象:',dfhouse['社区名称'].values           ,end = '\\n\\n')\n",
    "print('取series的数组对象的类型:',type(dfhouse['社区名称'].values).__name__     ,end = '\\n\\n')\n",
    "\n",
    "# 展示dfhouse['社区名称']其中的元素，两种方式，从array来的下标，从类似字典来的index\n",
    "dfhouse['社区名称'][1111027377718] \n",
    "# dfhouse['社区名称'][0] # 报错，因为你的index恰好也是整数，导致了优先按名字查找的情况下，找不到0\n",
    "dfhouse.index = [str(i) for i in dfhouse.index ] # 转化index为数字\n",
    "dfhouse['社区名称']['1111027377718'] \n",
    "dfhouse['社区名称'][0] #不会报错，默认按照第0个元素查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75220ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改index\n",
    "dfhouse['社区名称'].index = ['10','20','30'] #如同字典修改键值对也行\n",
    "print(dfhouse['社区名称'].index) # 成功完成修改\n",
    "print('SSSS',dfhouse['社区名称'][0]) # 成功完成修改\n",
    "print(dfhouse['社区名称'])\n",
    "\n",
    "print(dfhouse.index)# 来源的df的索引是不会变更的，运行两者有不同的编号，因为一个代表行编号，一个代表series元素的编号\n",
    "dfhouse.index = [100,200,300]# 修改dfhouse的索引请用df来调用\n",
    "\n",
    "# 修改values\n",
    "dfhouse['社区名称'].values # 这个只是个展示\n",
    "dfhouse['社区名称'] = dfhouse['社区名称'].fillna('').values + 'ssds' \n",
    "# 但是你要调用它的值你又得去写.fillna填充空值，避免空+有= 空，又有写.values获取值，以免用了series数据类型+字符串数据类型给你空值\n",
    "print(dfhouse['社区名称'])\n",
    "print(dfhouse) # 来源的df也变了，索引允许不同，但是值必须相同\n",
    "\n",
    "# 统一写values吧，指明是用values去做计算，覆盖原值\n",
    "dfhouse['社区价值'] = dfhouse['社区价值'].values*0.7\n",
    "print(dfhouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 补充如何手工生成一个series，提供可迭代的两个容器即可,直接用字典也行，自动调取键为index，值为values\n",
    "import pandas as pd\n",
    "a = range(1,5)\n",
    "b = (3,4,5,6)\n",
    "pd.Series(a,b)\n",
    "age = {'a':90,'b':100,'c':85}\n",
    "pd.Series(age) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 读取Excel文件中“社区编号目录”表的B列和C列\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house.xlsx\",sheet_name = 1,usecols='B:D')\n",
    "print(df1)\n",
    "# 1. 获取所有社区名称的一维数组。\n",
    "df1['社区名称'].values\n",
    "# 2. 获取所有社区编号的一维数组。\n",
    "df1['社区编号'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838cc03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题2\n",
    "# 读取Excel文件中“社区编号目录”表的B列和C列\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = 1,usecols='B:D')\n",
    "print(df1)\n",
    "# 1. 获取前10个社区名称。\n",
    "df1['社区名称'].values[:10]\n",
    "# 2. 获取位置在第 20~30 之间的社区名称，包括第20个和第30个。\n",
    "df1['社区名称'].values[19:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110e108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题3\n",
    "# 读取Excel文件中“房屋成交信息”表的B列、C列、G列，以id列为索引。\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = 0,usecols='B:C,G',index_col = 'id')\n",
    "print(df1)\n",
    "# 1. 求美元总价，结果包含id和总价，总价保留两位小数。美元兑换人民币汇率为：6.54 。\n",
    "df1['总价'] = (df1['总价'].values /6.54).round(2)\n",
    "print(df1)\n",
    "# 2. 求总价在100（万美元）以上的总价数据。\n",
    "df1['总价'][df1['总价'] > 100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a099962a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题4\n",
    "# 创建一个Series对象，\n",
    "# 数据为 0~101 之间所有偶数，\n",
    "# 索引为 0~101 之间的所有奇数。\n",
    "list1 = range(1,102,2)\n",
    "list2 = range(0,101,2)\n",
    "s3 = pd.Series(list2,list1) #先写数据，后写索引\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f6084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题5\n",
    "# 读取Excel文件中“房屋成交信息”表，以id列为索引。\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = 0,index_col = 'id')\n",
    "print(df1)\n",
    "# 获取id为 101102238676 的网址。\n",
    "print(df1['链家网址'][101102238676])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e7762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # 课后习题6\n",
    "# 读取Excel文件中“房屋成交信息”表。\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = 0,index_col = 'id')\n",
    "print(df1)\n",
    "# 1. 获取“链家网址”列的数据；\n",
    "url = df1['链家网址']\n",
    "url.values\n",
    "# 2. 将“所在社区编号”数据设置为“链家网址”数据的索引。\n",
    "url.index = df1['所在社区编号'].values\n",
    "print( df1['链家网址'] )\n",
    "# 3. 获取社区编号为 1111027380051 的网址。\n",
    "df1['链家网址'][1111027380051]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147c389",
   "metadata": {},
   "source": [
    "### 三、Series数据结构\n",
    "###### 3、Series类型对象可以执行的更多操作\n",
    "（1）基于字典相似性特性得以成立的操作：\n",
    "（2）基于数组特性得以成立的操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#（1）基于字典相似性特性得以成立的操作：\n",
    "import pandas as pd\n",
    "dic_age = {'张三': 30, '李四': 25, '王五': 27}\n",
    "s1 = pd.Series(dic_age)\n",
    "print(s1,end = '\\n\\n')\n",
    "\n",
    "# in操作符判断一个元素是否在index其中\n",
    "print('王八' in s1 ,end = '\\n\\n') \n",
    "\n",
    "# 修改键值对\n",
    "s1['张三'] = 35\n",
    "print(s1 ,end = '\\n\\n')\n",
    "\n",
    "# 修改过程其实就是键值对赋值过程，若没有，则插入一条\n",
    "s1 ['王八'] = 40\n",
    "print(s1 ,end = '\\n\\n')\n",
    "\n",
    "# 除了values也可以keys获取所有键;items获取所有键值对\n",
    "print(s1.keys()) # 等价于s1.index\n",
    "print(s1.items()) # zip类型，强转list\n",
    "print(list(s1.items()))\n",
    "\n",
    "# 基于键index的名称来索引出行\n",
    "print(s1['张三'])\n",
    "\n",
    "# 字典键索引也是可以花式索引的\n",
    "s1[['王八','张三']]\n",
    "\n",
    "# 字典键索引的切片索引 他和下标索引的切片索引有个非常本质的差别，那就是他不会舍弃末尾，不遵循左闭右开的原则\n",
    "s1['李四':'王八']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f281e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#（2）基于数组特性得以成立的操作：\n",
    "import pandas as pd\n",
    "dfhouse = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house.xlsx\",sheet_name = 1,usecols = 'B:D',index_col = 0)\n",
    "\n",
    "# values计算，数组特性，整体计算，对位计算，广播原则\n",
    "print(dfhouse['社区价值'].values*0.7) # 基于数组完成计算\n",
    "print(dfhouse['社区价值']*0.7) # 不改变dfhouse['社区价值']原值\n",
    "print(dfhouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values计算，并改变原值\n",
    "dfhouse['社区价值'] = dfhouse['社区价值'].values *0.7 # 改变dfhouse['社区价值']原值\n",
    "print(dfhouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下标索引\n",
    "print(dfhouse['社区名称'].values[0]) # 这是数组本身的下标索引\n",
    "dfhouse.index = [str(i) for i in dfhouse.index] # 转文本避开索引是数字，优先按照名称索引导致找不到0报keyerror错误的问题\n",
    "# dfhouse.index = dfhouse.index.astype('int64') # 也可以用这种方式来转\n",
    "\n",
    "print(dfhouse['社区名称'][0]) # 一样可以和上面的数组本身索引得到相同的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下标索引的花式索引\n",
    "#print(dfhouse['社区价值'][0,2])# 报错，切片的外方框是提供一个操作的含义，里面的内容得打包提供，即使只有一维，因为，在这个里面会被理解为维度，但是你只有一维\n",
    "print(dfhouse['社区价值'][[0,2]]) # 另外后面会学习多重索引，index_col参数允许多个列\n",
    "\n",
    "# dfhouse = dfhouse.set_index(['社区名称'],append = True)\n",
    "# print(dfhouse)\n",
    "# print(dfhouse['社区价值'][0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下标索引的切片索引\n",
    "print(dfhouse['社区名称'][0:2])\n",
    "print(dfhouse['社区名称'].iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b456a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 布尔值，广播运算\n",
    "dfhouse['社区价值'] > 2\n",
    "# 掩码布尔索引\n",
    "dfhouse[dfhouse['社区价值'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e814d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 综上所述，一旦出现index和下标都是数字型的时候，会出现很多问题，比如优先名称导致下标失效，比如左闭右开原则的不同\n",
    "# 我们就采取一个固定的方法来区别两种对行的索引\n",
    "dfhouse.iloc[0:1] #固定下标索引\n",
    "dfhouse.loc['1111027377718'] # 固定按照名称索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60563d37",
   "metadata": {},
   "source": [
    "### 三、Series数据结构\n",
    "###### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04116be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 1. 循环输入三名学生的姓名与年龄保存到字典中，由字典创建Series；\n",
    "import pandas as pd\n",
    "dict1 = {'张三':21,'李四':22,'王五':23,}\n",
    "s1 = pd.Series(dict1)   \n",
    "# 2. 修改最后一名学生的年龄为22；\n",
    "s1['王五'] = 22\n",
    "# 3. 打印每位学生的姓名与对应的年龄，例如：\n",
    "for a,b in list(s1.items()):\n",
    "    print('姓名:',a,'年龄:',b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题2    \n",
    "# 读取文件中的 “社区编号目录” 表，完成以下练习：\n",
    "import pandas as pd\n",
    "dfhouse = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = 1,usecols = 'B:C',index_col = 1)\n",
    "# print(dfhouse)\n",
    "# 1. 获取社区名为：'太阳园', '月坛北街', '大方居' 的社区编号\n",
    "print(dfhouse['社区编号'].loc[['太阳园','月坛北街','大方居']].values)\n",
    "# 2. 获取第8行、第10行的社区编号\n",
    "print(dfhouse['社区编号'].iloc[[7,9]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题3\n",
    "# 读取文件中的 “房屋成交信息” 表，完成以下练习：\n",
    "import pandas as pd \n",
    "dfhouse = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = 0)\n",
    "#print(dfhouse)\n",
    "# 1. 获取关注人数在300以上的数据；\n",
    "#dfhouse[dfhouse['关注人数'] > 300]\n",
    "# 2. 获取挂牌天数在 100~200 之间的数据，包含100、200；\n",
    "#dfhouse[(dfhouse['挂牌天数'] >=100) & (dfhouse['挂牌天数'] <=200)   ]\n",
    "# 3. 获取平米均价在 3万~5万 和 10万~12万 的数据，包括 3万、5万、10万、12万\n",
    "dfhouse[((dfhouse['平米均价'] >=3*10000) & (dfhouse['平米均价'] <=5*10000))|((dfhouse['平米均价'] >=10*10000) & (dfhouse['平米均价'] <=12*10000))  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题4\n",
    "# 读取文件中的 “房屋成交信息” 表，完成以下练习：\n",
    "import pandas as pd \n",
    "dfhouse = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = '房屋成交信息',index_col = 'id')\n",
    "# 1. 以id为索引，获取id为 101101492812 ~ 101101704540 之间的交易时间Series；\n",
    "dfhouse['交易时间'].loc[101101492812:101101704540]\n",
    "# 2. 以社区编号为索引，获取第 5 ~ 10 行的关注人数Series；\n",
    "dfhouse.index = dfhouse['所在社区编号']\n",
    "dfhouse['关注人数'].iloc[4:10]\n",
    "# 3. 以社区编号为索引，获取社区编号为 1111027378900 ~ 1111027380051 之间的平米均价Series。\n",
    "dfhouse['平米均价'].loc[1111027378900:1111027380051]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e19bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题5\n",
    "# 读取文件中的 “房屋成交信息” 表，完成以下练习：\n",
    "import pandas as pd \n",
    "dfhouse = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = '房屋成交信息',index_col = '建成时间')\n",
    "print(dfhouse.index[0])\n",
    "dfhouse.index = pd.to_datetime(dfhouse.index, format=\"%Y\") # 改变列格式的方式，datetime为了避免转的有问题，需要做点格式描述\n",
    "print(dfhouse.index[0])\n",
    "\n",
    "# 1. 以'建成时间'为索引，获取建成时间在2001年的链家网址Series；\n",
    "# dfhouse['链家网址'][dfhouse.index==2001]\n",
    "# dfhouse['链家网址'].loc[2001] # index下当然可以按键名称索引，但是却无法比较，如大于2010年的\n",
    "# dfhouse['链家网址'][dfhouse.index>2010] # 这个告诉我们即使变成了index，无法通过df['建成时间']来判断，也可以写df.index来用\n",
    "# dfhouse[['链家网址','挂牌天数']][dfhouse['挂牌天数']>200]\n",
    "\n",
    "# 2. 以'卧室'为索引，获取卧室数为3的链家网址Series；\n",
    "dfhouse.index = dfhouse['卧室'] # 替换过程会丢失哦\n",
    "dfhouse['链家网址'].loc[3]\n",
    "\n",
    "# 3. 以'关注人数'为索引，获取没人关注的链家网址Series。\n",
    "dfhouse['链家网址'].index = dfhouse['关注人数']\n",
    "dfhouse['链家网址'].loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b9e70",
   "metadata": {},
   "source": [
    "### DataFrame的索引默认情况下不会自动排序。当索引为数字时，取出的值并不是按照索引排序的结果，而是按照索引的原始顺序返回。\n",
    "\n",
    "### 要对DataFrame的索引进行排序，你可以使用sort_index()方法。这将按照索引的顺序对DataFrame进行排序，然后你可以选择取出排序后的特定范围的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 创建一个示例DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=[3, 1, 2])\n",
    "# 打印原始的DataFrame\n",
    "print(df)\n",
    "df.loc[1:3]\n",
    "df.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对索引进行排序\n",
    "df_sorted = df.sort_index()\n",
    "print(df_sorted)\n",
    "df_sorted.loc[1:3]\n",
    "# df_sorted.iloc[0:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4aa88a",
   "metadata": {},
   "source": [
    "### DataFrame的索引默认情况下不会自动排序。当索引为时间型的时候，即使不排序，也能聪明的选择范围内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个示例DataFrame，索引为时间\n",
    "df = pd.DataFrame({'A': [1, 2, 3, 4, 5]}, index=pd.to_datetime(['2023-09-01', '2023-09-07', '2023-09-03', '2023-09-06', '2023-09-02']))\n",
    "df.loc['2023-09-01':'2023-09-06'] # 是执行是否在时间范围内判定的，但是不会改排序顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f12b3",
   "metadata": {},
   "source": [
    "### 四、Dataframe数据结构\n",
    "###### 1、生成dataframe方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9835f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 生成dataframe存在各种方式，后续补充，此处先讲\n",
    "# 1、从series中生成\n",
    "import pandas as pd\n",
    "dfhouse = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_14-15.xlsx\",sheet_name = 0,index_col = 0 )\n",
    "price = dfhouse['总价'][dfhouse['总价']>1000] # 通过筛选使长度不一致\n",
    "bedroom = dfhouse['卧室'][dfhouse['卧室'] >2]\n",
    "pd.DataFrame( {'售价':price,'卧室数量':bedroom}      ) # 自动根据index对齐，无则NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd28fb",
   "metadata": {},
   "source": [
    "### 四、Dataframe数据结构\n",
    "###### 2、从dataframe中获取列的方式 ，iloc，loc也可以说就是获取行的方式\n",
    "（1）df[]切片series出来（2）属性.方法将series作为方法调用出来，不推荐（3）采取iloc，loc来实现对多维度的操作（4）掩码索引仅针对行\n",
    "###### series本身没有列的概念，他是一维的，所以用[]必定就是切行，但是dataframe是有列概念的，[]优先是取列，如果要取行，需要使用iloc，loc；与之产生对比的是array，他有多维度概念，[]默认就是增加，增加维度，不像dataframe必须写iloc才能增加维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af5a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df切片出来\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(   {'A':[1,3,5,7,9],'B':[2,4,6,8,10],'C':[1,2,3,4,5],'D':[5,4,3,2,1]}    )\n",
    "df1[['A','C']] # 支持花式索引 Fancy Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59295c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.series 属性.方法调用\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(   {'A':[1,3,5,7,9],'B':[2,4,6,8,10],'C':[1,2,3,4,5],'D':[5,4,3,2,1]}    )\n",
    "df1.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc可以做到增加一个逗号代表分成行，列两个维度，在列维度上操作也可以选择行\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(   {'A':[1,3,5,7,9],'B':[2,4,6,8,10],'C':[1,2,3,4,5],'D':[5,4,3,2,1]}    )\n",
    "print(df1)\n",
    "df1.loc[0:4,['B','D']]\n",
    "df1.loc[[2,3],['B','D']] # 特别注意loc是指对index键做索引，行列都是一致的，所以这里写的'B','D'\n",
    "df1.loc[2,'B'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22016e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc可以做到增加一个逗号代表分成行，列两个维度，在列维度上操作也可以选择行\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(   {'A':[1,3,5,7,9],'B':[2,4,6,8,10],'C':[1,2,3,4,5],'D':[5,4,3,2,1]}    )\n",
    "print(df1)\n",
    "df1.iloc[0:4,[0,2]]\n",
    "df1.iloc[[2,3],[0,2]] # 特别注意iloc是对下标操作的\n",
    "df1.iloc[2,1:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bf23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 掩码索引只针对行执行\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(   {'A':[1,3,5,7,9],'B':[2,4,6,8,10],'C':[1,2,3,4,5],'D':[5,4,3,2,1]}    )\n",
    "print(df1)\n",
    "df1[(df1['C']>=3) & (df1['D']<3)] # 完全ok，而且也支持多条件，注意位运算符打括号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e20f4",
   "metadata": {},
   "source": [
    "### 四、Dataframe数据结构\n",
    "###### 3、转化为array的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c299527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取df的values即可转化为array，这点和series去values就是一维数组是一个概念\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(   {'A':[1,3,5,7,9],'B':[2,4,6,8,10],'C':[1,2,3,4,5],'D':[5,4,3,2,1]}    )\n",
    "print(df1.values)\n",
    "print(type(df1.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dadba2",
   "metadata": {},
   "source": [
    "### 四、Dataframe数据结构\n",
    "###### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21292a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 读取文件中“房屋成交信息”表 ，按以下要求完成练习：\n",
    "import pandas as pd \n",
    "dfhouse = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\")\n",
    "dfhouse\n",
    "# 1. 利用字典构造新的DataFrame，包括 '链家网址'、'总价'、'平米均价'，合并后的列名为 '网址'、'总价'、'均价'。\n",
    "dfnew = pd.DataFrame(   {'网址':dfhouse['链家网址'],'总价':dfhouse['总价'],'均价':dfhouse['平米均价']}             )\n",
    "# 2. 将挂牌超过3天的天数与第1小题中的数据合并，合并后的列名为 '网址'、'总价'、'均价'、'天数'。\n",
    "dfnew = pd.DataFrame(   {'网址':dfhouse['链家网址'],'总价':dfhouse['总价'],'均价':dfhouse['平米均价'],'天数':dfhouse['挂牌天数'][dfhouse['挂牌天数']>3]}             )\n",
    "dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2ce76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题2\n",
    "# 读取文件中 “房屋成交信息”表 ，按以下要求完成练习：\n",
    "import pandas as pd \n",
    "df1= pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = 0)\n",
    "# 1. 获取 id、网址、社区编号、关注人数等列；\n",
    "df1[['链家网址', 'id', '所在社区编号',  '关注人数']]\n",
    "# 2. 获取 id、交易时间、总价、面积等列。\n",
    "df1.columns\n",
    "df1[['id', '交易时间','总价','面积', ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题3\n",
    "# 读取文件中的 “房屋成交信息”表 ，按以下要求完成练习：\n",
    "import pandas as pd \n",
    "df1= pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = 0)\n",
    "# 1. 获取 id 为 101102452309 的数据；\n",
    "df1[df1['id'] == 101102452309 ]\n",
    "# 2. 获取 id 从 101102134610 ~ 101102196240 之间的数据；\n",
    "df1[(df1['id'] >= 101102134610)  & (df1['id'] <= 101102196240) ]\n",
    "# 3. 获取 id 为 101101711072、101102087081、101102331601 的行数据；\n",
    "df1[df1['id'].isin(  [101101711072,101102087081,101102331601]  ) ]\n",
    "# 4. 获取 第 3~7 行的数据。\n",
    "df1.iloc[2:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd6f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题4\n",
    "# 读取文件中的 “房屋成交信息” 表，完成以下练习：\n",
    "import pandas as pd \n",
    "df1= pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = 0)\n",
    "# 1. 获取没人关注的数据；\n",
    "df1[df1['关注人数'] == 0]\n",
    "# 2. 获取总价在 200~400 之间的数据；\n",
    "df1[(df1['总价'] >= 200) & (df1['总价'] <= 400)]\n",
    "# 3. 获取在2006年建成，且挂牌天数在一周以内或 30~40 天的数据；\n",
    "df1[(df1['建成时间'] == 2006) & ( (df1['挂牌天数'] <= 7) | ((df1['挂牌天数'] >= 30) & (df1['挂牌天数'] <= 40))  )    ]\n",
    "# 4. 获取总价在300以下，卧室和客厅不少于2个的房源网址。\n",
    "df1[(df1['总价'] < 300) & (df1['卧室'] >=2 ) & (df1['客厅'] >= 2)   ]['链家网址']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题5\n",
    "# 读取文件中的 “房屋成交信息” 表，完成以下练习：\n",
    "import pandas as pd \n",
    "df1= pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = 0)\n",
    "# 1. 获取在2009年建成的房源中，建成时间、社区编号、总价、面积、网址等信息；\n",
    "#（1）解法1 先布尔索引整个df再抽取出列\n",
    "df1[df1['建成时间'] == 2009][['建成时间','所在社区编号','总价','面积','链家网址']]\n",
    "new_name = {'链家网址':'网址'} # 旧名做键，新名做值 构成字典\n",
    "df1.rename(columns = new_name) # rename方法，df对给出的键的值替换\n",
    "\n",
    "#（2）解法2 直接用loc的二维切片能力，注意只有loc接受掩码索引，iloc在行的位置上写掩码索引会报错\n",
    "df1.loc[df1['建成时间'] == 2009,['建成时间','所在社区编号','总价','面积','链家网址']]\n",
    "\n",
    "\n",
    "# 2. 获取id编号在 101102111185~101102172813 之间，交易时间、挂牌天数、平米均价、面积等信息；\n",
    "#df1 = df1.set_index(['id'])  # 整个set_index的方法可以不改变原值\n",
    "df1.index = df['id']\n",
    "df2 = df1.loc[101102111185:101102172813,['交易时间','挂牌天数','平米均价','面积']]\n",
    "df2\n",
    "df1\n",
    "\n",
    "# 3. 获取第 3 ~ 10 行的 面积、卧室、客厅、厨房、浴室、楼层等信息；\n",
    "df1.iloc[2:10,8:14]\n",
    "\n",
    "# 4. 获取挂牌天数在90天以内，且关注人数超过100的社区编号、挂牌天数、关注人数、网址。\n",
    "df1.loc[(df1['挂牌天数']<=90) & (df1['关注人数']>100)    ,   ['所在社区编号','挂牌天数','关注人数','链家网址']           ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd240f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\yaoyan\\Downloads\\bquxjob_3673e28_18a972b3ec4.csv\")\n",
    "df.index = df['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2769b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[(df['bundle_id'] == 'com.qxuhappy.qlhouse310') & (df['event_name'].isin (['recharge_order','recharge_success'])) ]\n",
    "paid = df1[df1['event_name'] == 'recharge_success']['f0_']\n",
    "recharge = df1[df1['event_name'] == 'recharge_order']['f0_']\n",
    "dfnew = pd.DataFrame( {'paid':paid,'recharge':recharge}   )\n",
    "dfnew\n",
    "dfnew['rate'] = dfnew['paid']/dfnew['recharge']\n",
    "dfnew['rate'] = [str(round(i*100,2)) + '%' for i in dfnew['rate']]\n",
    "dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a10a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(float(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d4b53",
   "metadata": {},
   "source": [
    "### 五、Dataframe的运算\n",
    "###### 1、dataframe与numpy相似的部分\n",
    "（1）整体计算（2）统计函数（3）对位计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 假设数据为每月每个销售员的销售电视机数量\n",
    "dftest = pd.DataFrame(          {'张三':[50,45,55],'李四':[10,15,25], '王五':[70,65,15]        },index = ['1月','2月','3月'])\n",
    "# （1）整体运算展示：假设每天电视机100元，求取销售额 = 销售电视机数量 * 100\n",
    "dftest * 100\n",
    "# （2）统计函数展示\n",
    "dftest.sum(axis = 0) # 和array 的 axis参数一致，指定消灭维度 但是他永远默认axis = 0至少保留列维度\n",
    "dftest.values.sum() # 利用.values属性制作array，从而调用array的.sum方法解决消灭所有维度的问题\n",
    "\n",
    "# 统计函数有一个非常重要的，就是nunique()统计非重复值，配合group by 使用实现表主键分析\n",
    "# unique()则是序列的方法，可以实现剔除一个series重复值的操作\n",
    "# df.groupby(['订单号','商品编号']).aggregate({'订单号':'unique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （3）对位计算\n",
    "# 使用算术方法替代算术符号\n",
    "#因为pandas的对位计算和numpy的不同，形状不同不可广播，直接就会报错，但是pd却会根据索引列名去使得两者的形状一致，也就导致了会出现很多的NaN\n",
    "#此时用方法可以指定参数fill_value = ，给空值填补值，解决NaN+30 = NaN的问题；但是不会将NaN+NaN = NaN 的结果值替换为0\n",
    "\n",
    "# 此处的运算都遵循前面 运算 后面\n",
    "df1.add(df2) #加\n",
    "df1.sub(df2) #减\n",
    "df1.mul(df2) #乘\n",
    "df1.div(df2) #除\n",
    "df1.mod(df2) #模运算\n",
    "df1.pow(df2) #乘方\n",
    "df1.floordiv(df2) # 整除\n",
    " \n",
    "# 此处的运算都遵循后面 运算 前面 因为如果被运算的对象是个整数的话，数字可没有.add方法也就没有办法指定参数了\n",
    "df1.radd(df2) #加\n",
    "df1.rsub(df2) #减\n",
    "df1.rmul(df2) #乘\n",
    "df1.rdiv(df2) #除\n",
    "df1.rmod(df2) #模运算\n",
    "df1.rpow(df2) #乘方\n",
    "df1.rfloordiv(df2) # 整除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2762eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(          {'张三':[50,45,55],'李四':[10,15,25], '王五':[70,65,15]        },index = ['1月','2月','3月'])\n",
    "df1.rsub(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d458881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe 与 series计算的结果是 将series当做行，这和array是一致的\n",
    "price = pd.Series([2500,1500,1000],['1月','2月','3月'])\n",
    "df1.mul(price,axis = 0) # axis指定计算的维度，手动指定price为和行执行结算，而非默认的和列执行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad15f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame(          {'张三':[50,45,55],'李四':[10,15,25], '王五':[70,65,15]        },index = ['1月','2月','3月'])\n",
    "df1.sum(axis = 1)\n",
    "df1.max(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e076dcef",
   "metadata": {},
   "source": [
    "### 五、Dataframe的运算\n",
    "###### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aac4f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 读取文件中的 “房屋成交信息”表 ，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",sheet_name = 0)\n",
    "#1. 获取 卧室、厨房，卧室与厨房数量上调1；\n",
    "df1[['卧室','厨房']] = df1[['卧室','厨房']]+1\n",
    "#2. 获取 总价、面积、平米均价，各列数据下调10% \n",
    "df1[['总价','面积','平米均价']] = df1[['总价','面积','平米均价']].values* 0.9\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f262471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题2\n",
    "# 读取文件中的 “房屋成交信息”表 ，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",sheet_name = 0)\n",
    "# 1. 统计 模式口01 ~ 龙博苑01 之间，关注人数总和、挂牌天数总和。\n",
    "df1.index = df1['id']\n",
    "df2 = df1.loc['模式口01':'龙博苑01',['关注人数','挂牌天数']]\n",
    "# 写法一，df自己的求和方法\n",
    "df2.sum(axis = 0)\n",
    "\n",
    "# 写法二，整个df当做一个array交给numpy的sum函数\n",
    "import numpy as np\n",
    "np.sum(df2,axis = 0)\n",
    "\n",
    "# 2. 统计总价在500以上的平均总价、平均面积。\n",
    "df3 = df1[df1['总价']>500]\n",
    "df3[['总价','面积']].mean(axis = 0)\n",
    "\n",
    "# 3. 统计每条记录中卧室、厨房、浴室的数量总和。\n",
    "df1[['卧室','厨房','浴室']].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d8659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题3\n",
    "import pandas as pd\n",
    "dict1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\产业产值数据.xlsx\",sheet_name=[0,1],index_col = '年份')\n",
    "# 1. 计算两个地区 2007、2011 两年中金融业、房地产的各年各产业的产值之和。\n",
    "(dict1[0]+dict1[1]).loc[  [2007,2011]  ,  ['金融业','房地产']   ]\n",
    "# 2. 计算B地区 2007~2010 年的运输业、零售业与A地区 2009~2011 年的零售业，房地产之间的产值之差，一边出现缺失值填充0。\n",
    "dfa = dict1[0]\n",
    "dfb = dict1[1]\n",
    "dfa.loc[ 2009:2011 ,  ['零售业','房地产']   ] .rsub( dfb.loc[ 2007:2010 ,  ['运输业','零售业']   ] ,fill_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c18265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题4\n",
    "# 读取文件中的“A地区”工作表，按以下要求完成练习：\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\产业产值数据.xlsx\",sheet_name=0,index_col = '年份')\n",
    "df1.reset_index() # 将索引还回去\n",
    "# 1. 其中2011年与2012年的产值不准确，需要按比例上调，五种产业需要上调的比例分别为：12%、20%、10%、30%、42%，计算上调后的产值数据。\n",
    "rate1 = pd.Series([1.12,1.2,1.1,1.3,1.42],index = ['运输业','零售业','餐饮业','金融业','房地产'])\n",
    "df1reset = df1.mul(rate1,axis = 1)\n",
    "# 2. 其中 2007~2009 年中金融业与房地产的产值不准确，需要按比例上调，2007~2009 年份需要上调的比例分别为：11%、13%、15%，计算上调后的产值数据。\n",
    "rate2 = pd.Series([1.11,1.13,1.15],index= [2007,2008,2009])\n",
    "df2 = df1.loc[2007:2009,['金融业','房地产']]\n",
    "df2 = df2.mul(rate2,axis = 0) \n",
    "df1.loc[2007:2009,['金融业','房地产']] = df2 # 形状相同时直接替换值，这和之前换值的逻辑一致，甚至可以这样挖出缺口填\n",
    "df1\n",
    "# Series数据结构即使有.mul方法也不能指定axis = 1, 因为它只有一个维度，只能默认做列相乘，但是反过来却不是\n",
    "# 另外当dataframe和series做算数运算时，也无法指定fill_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd19829",
   "metadata": {},
   "source": [
    "### 六、dataframe高级函数用法，函数式编程思想\n",
    "###### 0、前言，使用循环快速基于现有数据增加列的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9312e28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 如果没有这个方法，我们想要基于已有数据判断后增加新列，可以采取循环行的方式来解决\n",
    "\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",sheet_name = 0)[['id','总价','卧室']]\n",
    "x = []\n",
    "# 循环行方式一，使用iterrows，另外一个配套循环列的叫iteritems\n",
    "for index,rows in df1.iterrows():\n",
    "    if  rows['总价'] > 500 and rows['卧室'] >2:\n",
    "        x1 = '高级公寓'\n",
    "    else:\n",
    "        x1 = '低级公寓'\n",
    "    x.append(x1)\n",
    "\n",
    "df1['是否高级'] = x\n",
    "print(df1)\n",
    "\n",
    "y = []\n",
    "# 循环行方式二，使用loc定位行\n",
    "for hid in df1.index:\n",
    "    if df1.loc[hid,'总价'] >500 and df1.loc[hid,'卧室']>2:\n",
    "        x1 = '高级公寓'\n",
    "    else:\n",
    "        x1 = '低级公寓'\n",
    "    y.append(x1)\n",
    "df1['是否高级2'] = y\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda表达式回忆\n",
    "# 一种简写函数定义，匿名使用函数的方法\n",
    "def myadd(x,y):\n",
    "    return x+y\n",
    "myadd(1,1)\n",
    "myadd = lambda x,y:x+y\n",
    "myadd(1,1)\n",
    "\n",
    "# map 函数回忆\n",
    "# 通常一个函数一个方法只对一个元素做出作用\n",
    "# 但是map函数的特点是对于给出一个函数，用这个函数对于后面的容器参数中的每一个元素处理\n",
    "list1 = [1,2]\n",
    "list2 = [4,6]\n",
    "list3 = [7,8]\n",
    "list(map(myadd,list1,list2))\n",
    "\n",
    "# 新知识拓展 如何简写判断结构\n",
    "# 如何才能是的if else 判断结构简写为一句话，从而塞入lambda表达式,使用三元运算符即可，又称布尔表达式\n",
    "judge1 = lambda x:'x 是大于10的' if x >10 else 'x是小于或者等于10的'\n",
    "print(judge1(10),end = '\\n\\n')\n",
    "# 如何让多分支判断结构简写为一句话 ，依赖于字典充当判断条件\n",
    "judge2 = lambda x: {x>10:'x 是大于10的' ,x == 10:'x 是等于10的' ,x<10: 'x是小于10的'}[True] # 判断后键会是True Or False，最后一个False的值会覆盖前面的， 因为键绝对唯一\n",
    "print(judge2(10),end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760a87b",
   "metadata": {},
   "source": [
    "### 六、dataframe高级函数用法，函数式编程思想\n",
    "###### 1、Series 的 map方法，本质和python的map一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2010cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 所以我们可以类比，一个series的本质是其中的值是一个数组，也是一个容器，所以当然可以map一下\n",
    "# 案例，基于总价这个series去判断是否高级公寓\n",
    "import pandas as pd \n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",sheet_name = 0,index_col = 'id')\n",
    "df1['是否高级公寓'] = df1['总价'].map(lambda x:'高级公寓' if x >500 else '低级公寓') # 同index生成对应series\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25acacfc",
   "metadata": {},
   "source": [
    "### 六、dataframe高级函数用法，函数式编程思想\n",
    "###### 2、dataframe的apply方法，对每一行/每一列的元素执行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b22825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",\n",
    "                    sheet_name = 0,\n",
    "                    index_col = 'id',\n",
    "                    # usecols = lambda columns: columns != '交易时间' 这个写法似乎可以用，但我看不出错误在哪\n",
    "                   )\n",
    "# 多列同行apply演示\n",
    "df1['是否高级公寓'] = df1.apply(\n",
    "    lambda house :'高级公寓' \n",
    "    if house['总价']>500 and house['卧室']>2 \n",
    "    else '低级公寓',axis = 1)\n",
    "# 多行同列apply演示\n",
    "df1['交易时间']= df1['交易时间'].astype('int64') # timestamp不能+，为了演示先干掉他\n",
    "df1.loc['总计'] = df1.apply(lambda columns: columns['逸成东01']+columns['天通苑05'],axis = 0)\n",
    "\n",
    "df1\n",
    "# 本质是按行/列抽取一整条的数据，然后对于整条数据中的index获取你想要执行操作的元素出来，做各种不同运算\n",
    "# 最终还是会获取到一个同index的一整条数据，从而可以进一步的添加到原来的dataframe数据框架中，\n",
    "# 用df1.loc[]赋值，df1['']给与行/列新行或者新列\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf7591",
   "metadata": {},
   "source": [
    "### 六、dataframe高级函数用法，函数式编程思想\n",
    "###### 3、dataframe的applymap方法，对df的每个元素做处理，返回一个新的dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e90778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",\n",
    "                    sheet_name = 0,\n",
    "                    index_col = 'id',\n",
    "                    # usecols = lambda columns: columns != '交易时间' 这个写法似乎可以用，但我看不出错误在哪\n",
    "                   )[['挂牌天数','关注人数','总价','平米均价']]\n",
    "df1.applymap(lambda x : str(x)[0]+ 'xx') # 处理每个元素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886e8b9",
   "metadata": {},
   "source": [
    "### 六、dataframe高级函数用法，函数式编程思想\n",
    "###### 4、dataframe的定位方法\n",
    "（1）argmax (2)idxmin  # 实际众多统计函数能找到单个数值的都可以拉去做坐标/索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回忆numpy中的where函数\n",
    "import numpy as np\n",
    "# arr1 = np.random.randint(10,100,(4,8))\n",
    "# s = np.where(arr1>50) # 坐标轴获取\n",
    "# list1 = list(zip(s[0],s[1]))\n",
    "# for _ in list1:\n",
    "#     print(arr1[_])\n",
    "import pandas as pd\n",
    "# 然后学习df的定位能力\n",
    "df1 = pd.DataFrame(\n",
    "    np.array([13,22,36,47,58,69,70,89,99]).reshape(3,3),\n",
    "    index = ['张三','李四','王五'],\n",
    "    columns = ['语文','数学','英语']\n",
    "    ) # 其实还有参数为dtype指定数据类型，字典类自带了列名，如果是数组，也可以手动增加列名参数\n",
    "# df1.max(axis = 0) # 除了交给numpy做处理之外，也可以直接df的基础统计方法计算，也可以指定axis\n",
    "df1.iloc[df1['英语'].argmax(axis = 0)]\n",
    "df1.loc [df1['英语'].idxmin(axis = 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc707fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 延伸思考在需要在df所有数据中找打最大值并定位行列时的操作方式\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 假设df1是英语这门考试5张试卷，5个同学的考试，试问复数次考试中考分最高的一次是由哪张试卷哪个同学创造的\n",
    "arr1 = np.random.randint(5,100,(5,5))\n",
    "df1 = pd.DataFrame(arr1,index=['张三','李四','王五','瓜六','田七',],columns = ['试卷1','试卷2','试卷3','试卷4','试卷5',])\n",
    "print(df1)\n",
    "# 因为df不能求出全部值的最大值，此时只能调用numpy出来计算\n",
    "print(np.max(df1.values))\n",
    "# 找到最大值后拿去定位，定位会给出坐标轴，轴需要用zip将可能多个坐标轴拼合起来\n",
    "坐标=list(zip(np.where(df1.values == np.max(df1.values))[0],np.where(df1.values == np.max(df1.values))[1]))\n",
    "print(坐标)\n",
    "# 根据坐标轴采取index同技术方式切片，就能搞到结果\n",
    "print(\n",
    "    '复数次英语考试中，最大值由' \n",
    "    + str(df1.index[坐标[0][0]])\n",
    "    +'制造，被考试卷为'\n",
    "    +str(df1.columns[坐标[0][1]])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ffa64e",
   "metadata": {},
   "source": [
    "###### 补充Series/DataFrame支持的统计方法与numpy的方法相似，由axis参数设置DataFrame的运算方向，默认为0：\n",
    "\n",
    "    count()     非空值的总个数\n",
    "    sum()       全部数值加总\n",
    "    mean()      平均值\n",
    "    median()    中位数\n",
    "    mode()      众数\n",
    "    std()       标准差\n",
    "    min()       最小值\n",
    "    max()       最大值\n",
    "    abs()       绝对值\n",
    "    prod()      全部元素相乘之积\n",
    "    cumsum()    从第1个元素至本元素的累加之和\n",
    "    cumprod()   从第1个元素至本元素的累乘之积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示累加函数的用法\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.DataFrame(\n",
    "    np.array([13,22,36,47,58,69,70,89,99]).reshape(3,3),\n",
    "    index = ['张三','李四','王五'],\n",
    "    columns = ['语文','数学','英语']\n",
    "    )\n",
    "print(df1)\n",
    "df1.cumsum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756b7a52",
   "metadata": {},
   "source": [
    "### 六、dataframe高级函数用法，函数式编程思想\n",
    "###### 5、dataframe的corr 相关性分析，此处只展示函数，后面才是真分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7a6661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>总价</th>\n",
       "      <th>平米均价</th>\n",
       "      <th>卧室</th>\n",
       "      <th>厨房</th>\n",
       "      <th>浴室</th>\n",
       "      <th>客厅</th>\n",
       "      <th>面积</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>总价</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.508209</td>\n",
       "      <td>0.531304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.587018</td>\n",
       "      <td>0.222403</td>\n",
       "      <td>0.723970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>平米均价</th>\n",
       "      <td>0.508209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.110584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.094161</td>\n",
       "      <td>-0.118455</td>\n",
       "      <td>-0.155493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>卧室</th>\n",
       "      <td>0.531304</td>\n",
       "      <td>-0.110584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.405796</td>\n",
       "      <td>0.329354</td>\n",
       "      <td>0.753997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>厨房</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>浴室</th>\n",
       "      <td>0.587018</td>\n",
       "      <td>0.094161</td>\n",
       "      <td>0.405796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.485899</td>\n",
       "      <td>0.657351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>客厅</th>\n",
       "      <td>0.222403</td>\n",
       "      <td>-0.118455</td>\n",
       "      <td>0.329354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.485899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.414809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>面积</th>\n",
       "      <td>0.723970</td>\n",
       "      <td>-0.155493</td>\n",
       "      <td>0.753997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.657351</td>\n",
       "      <td>0.414809</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            总价      平米均价        卧室  厨房        浴室        客厅        面积\n",
       "总价    1.000000  0.508209  0.531304 NaN  0.587018  0.222403  0.723970\n",
       "平米均价  0.508209  1.000000 -0.110584 NaN  0.094161 -0.118455 -0.155493\n",
       "卧室    0.531304 -0.110584  1.000000 NaN  0.405796  0.329354  0.753997\n",
       "厨房         NaN       NaN       NaN NaN       NaN       NaN       NaN\n",
       "浴室    0.587018  0.094161  0.405796 NaN  1.000000  0.485899  0.657351\n",
       "客厅    0.222403 -0.118455  0.329354 NaN  0.485899  1.000000  0.414809\n",
       "面积    0.723970 -0.155493  0.753997 NaN  0.657351  0.414809  1.000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",\n",
    "                    sheet_name = 0,\n",
    "                    index_col = 'id',\n",
    "                    # usecols = lambda columns: columns != '交易时间' 这个写法似乎可以用，但我看不出错误在哪\n",
    "                   )[['总价','平米均价','卧室','厨房','浴室','客厅','面积']]\n",
    "df1.corr()\n",
    "# 看法是，基于某一列/某一行，查看其他字段和本字段之间的比值，从而比值越大相关性越大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935a834",
   "metadata": {},
   "source": [
    "### 六、dataframe高级函数用法，函数式编程思想\n",
    "###### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c354cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 读取文件中“房屋成交信息”表，以第一列为索引，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",\n",
    "                    sheet_name = 0,\n",
    "                    index_col = 'id',\n",
    "                    # usecols = lambda columns: columns != '交易时间' 这个写法似乎可以用，但我看不出错误在哪\n",
    "                   )[['总价','平米均价','卧室','厨房','浴室','客厅','面积','建成时间']]\n",
    "# 1. 获取卧室信息，将卧室数量 1、2、3 转换为中文数量，例如：\n",
    "# （1）对series操作\n",
    "# 与其写函数判断式，不如直接字典映射\n",
    "df1['卧室'].map({1:'一卧',2:'二卧',3:'三卧'}) \n",
    "# 函数判断式这样写直接切片字典会报错，用get方法\n",
    "series1 = df1['卧室'].map(lambda x :{x == 1:'一卧',x == 2:'二卧',x == 3:'三卧'}.get(True)) \n",
    "series1.name = '卧室中文'\n",
    "pd.DataFrame([df1['卧室'],series1]).transpose() # series转化过来的会有行列倒转的情况\n",
    "# （2）对dataframe操作\n",
    "df1['卧室中文化'] = df1.apply(lambda house: {house['卧室'] == 1:'一卧',house['卧室'] == 2:'二卧',house['卧室'] == 3:'三卧'}.get(True),axis = 1)\n",
    "# 常规操作，正常的定义一个函数出来\n",
    "def judge (house):\n",
    "    if house['卧室'] == 1:\n",
    "        return '一卧'\n",
    "    elif house['卧室'] == 2:\n",
    "        return '二卧'\n",
    "    elif house['卧室'] == 3:\n",
    "        return '三卧'\n",
    "df1['卧室中文化2'] = df1.apply(judge,axis = 1)\n",
    "df1\n",
    "\n",
    "# 2. 获取建成时间，将2000年之前建成的转换为 “2000年之前”，其余的（包含2000）替换为 “2000年之后” 。\n",
    "df1['建成时间'].map(lambda x: '2000年之前' if x <2000 else '2000年之后')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题2\n",
    "# 读取文件中“房屋成交信息”表，以第一列为索引，根据以下条件判断房屋的户型：\n",
    "# 面积小于90，且卧室和客厅数量不低于1的为 '小户型'；面积在 90~140 之间的为 '中户型'；面积在 140 以上的为 '大户型'；其余的标记为 '-'。\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",\n",
    "                    sheet_name = 0,\n",
    "                    index_col = 'id',\n",
    "                   )\n",
    "def judge(house):\n",
    "    if (house['面积'] < 90) & (house['卧室']>=1)  & (house['客厅']>=1):\n",
    "        return '小户型'\n",
    "    elif (house['面积']>= 90) & (house['面积']<= 140):\n",
    "        return '中户型'\n",
    "    elif (house['面积']> 140):\n",
    "        return '大户型'\n",
    "    else:\n",
    "        return '-'\n",
    "\n",
    "pd.set_option('display.max_rows',None)\n",
    "df1.apply(judge,axis = 1)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题3\n",
    "# 读取文件中“房屋成交信息”表，以第一列为索引，按要求完成以下练习：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",\n",
    "                    sheet_name = 0,\n",
    "                    index_col = 'id',\n",
    "                   )\n",
    "# 1. 求2000年之后建成的房屋数据中，总关注人数、平均总价、平均面积；\n",
    "dfs = df1.loc[df1['建成时间']>2000,['关注人数','总价','面积']]\n",
    "print(df1['关注人数'].sum(),round(df1['总价'].mean(),2),round(df1['面积'].mean(),2))\n",
    "# 2. 求 2010~2012 年之间建成的房屋数据中，最大面积、最低总价。\n",
    "dfs = df1.loc[(df1['建成时间']>=2010) & (df1['建成时间']<=2012),['建成时间','关注人数','总价','面积']]\n",
    "print(dfs['面积'].max(),dfs['总价'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题4\n",
    "# 读取文件中“房屋成交信息”表，以第一列为索引，按要求完成以下练习：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",\n",
    "                    sheet_name = 0,\n",
    "                    index_col = 'id',\n",
    "                   )\n",
    "# 1. 获取关注人数最多的房屋的 社区编号、关注人数、链家网址；\n",
    "df1.iloc[df1['关注人数'].argmax(axis =0)][['所在社区编号','关注人数','链家网址']].transpose()\n",
    "# 2. 获取面积最小的房屋的 社区编号、面积、总价。\n",
    "df1.loc[df1['面积'].idxmin()][['所在社区编号','面积','总价']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题5\n",
    "#读取文件中“房屋成交信息”表，以第一列为索引，\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",\n",
    "                    sheet_name = 0,\n",
    "                    index_col = 'id',\n",
    "                   )\n",
    "# 1. 获取2000年之后建成的房屋信息中 挂牌天数、关注人数、面积、总价之间的相关性\n",
    "df1 = df1.loc[df1['建成时间']>2000,['挂牌天数','关注人数','面积','总价']]\n",
    "# 2. 获取总价相关性最高的那一行数据。\n",
    "print(type(df1.corr())) # DataFrame\n",
    "dfc = df1.corr()\n",
    "dfc = dfc.drop('总价',axis =0)\n",
    "print('相关性最高的是:',   dfc['总价'].idxmax(), round( dfc.loc[dfc['总价'].idxmax(),'总价'],2) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97368c2",
   "metadata": {},
   "source": [
    "### 七、dataframe了解信息与增删改查\n",
    "###### 1、了解dataframe的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d44d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 了解全局信息\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",\n",
    "                    sheet_name = 0,\n",
    "                    index_col = 'id',\n",
    "                    # usecols = lambda columns: columns != '交易时间' 这个写法似乎可以用，但我看不出错误在哪\n",
    "                   )[['总价','平米均价','卧室','厨房','浴室','客厅','面积']]\n",
    "df1.info() # 查看df的信息，可以看到行列大小，可以看到字段结构，看到占用内存\n",
    "df1.describe() # 基础统计描述下整个df结构\n",
    "df1.shape # 和array一样快速查看行列大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 了解明细信息\n",
    "df1.head() # 默认前5条，可以手动设定值\n",
    "df1.head(2)\n",
    "df1.tail() # 默认后5条，可以手动设定值\n",
    "df1.tail(2)\n",
    "pd.set_option('display.max_rows',None) # None代表不做限制，因此可以完整展示整个表格全部数据\n",
    "\n",
    "pd.reset_option('display.max_rows') # 还原option的设置\n",
    "df1\n",
    "\n",
    "# 更多的可设置选项，请前往官网调取httpss://pandas.pydata.org/docs/reference/api/pandas.set_option.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd421ee",
   "metadata": {},
   "source": [
    "### 七、dataframe了解信息与删 增 改\n",
    "###### 2、删除dataframe的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af6727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop方法搞定行列删除，不修改原先的df，需要用新变量接收\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\")\n",
    "df1 = df1[['id','挂牌天数','关注人数','平米均价','总价']]\n",
    "df1.index = df1['id']\n",
    "print(df1)\n",
    "# 行删除\n",
    "df2 = df1.drop('逸成东01',axis = 0) # 按照行索引走，基于index删除\n",
    "df2.drop(df2.index[1],axis = 0) # 按照行索引走，基于下标删除，本质是取了index的索引\n",
    "# 列删除\n",
    "df3 = df1.drop('关注人数',axis = 1) # 按照列索引走，基于index删除\n",
    "df3.drop(df3.columns[0],axis = 1) # 按照列索引走，基于columns，竖着的index的索引来删除\n",
    "df3.index\n",
    "# 不论行，列删除，index都可以按照花式索引的方式来删除多行,index的索引都可以切片\n",
    "df1\n",
    "df2 = df1.drop(['平米均价','总价'],axis = 1) # 花式索引\n",
    "df2.drop(df2.index[0:5:2],axis = 0) # 下标切片\n",
    "# 布尔索引删选数据然后拿出index的值交给drop完成布尔drop\n",
    "df1.drop(df1[df1['关注人数']>100].index,axis = 0) # 这样关注人数只有100以下的房子了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea2a546",
   "metadata": {},
   "source": [
    "### 七、dataframe了解信息与增删改查\n",
    "###### 3、增加dataframe的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.DataFrame(\n",
    "    np.random.randint(40,80,(3,3)),\n",
    "    index = ['张三','李四','王五'],\n",
    "    columns = ['语文','数学','英语'] )\n",
    "\n",
    "# 增加行数据 一种是字典键值对增加，一种是采取合并多个df\n",
    "df1.loc['瓜六'] = [57,98,70] # 走的字典逻辑，给出键给出值自然增加\n",
    "df1\n",
    "df2 = pd.DataFrame(\n",
    "    np.random.randint(40,80,(3,3)),\n",
    "    index = ['www','xxx','sss'],\n",
    "    columns = ['语文','数学','英语'] )\n",
    "pd.concat([df1,df2]) # 使用这个函数替代下append方法\n",
    "# df1.append(df2)\n",
    "\n",
    "# 增加列数据 一种是字典键值对增加 ，一种是insert方法\n",
    "df1['物理'] = 33 # 重复知识点，广播原则，一个数字也可以等于\n",
    "\n",
    "df1.insert(3,'化学',np.random.randint(10,100,(4))) # 三个参数，3代表插入的位置是第四个，'化学'是列名，最后一个是提供数据\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18600a61",
   "metadata": {},
   "source": [
    "### 七、dataframe了解信息与增删改查\n",
    "###### 4、改变dataframe的行列顺序，本质是移动index，带着数据值一起走"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef510258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改变值的方式可太多了，之前已经学过了直接df.loc[行，列] = 其他容器，形状一致即可替换\n",
    "# 这里只讲如何改变行列顺序\n",
    "\n",
    "# 花式索引改变列顺序\n",
    "df1 = pd.DataFrame(\n",
    "    np.random.randint(40,80,(3,3)),\n",
    "    index = ['张三','李四','王五'],\n",
    "    columns = ['语文','数学','英语'] )\n",
    "df1 = df1[['数学','英语','语文']]\n",
    "df1\n",
    "\n",
    "# 正规的改变行，列顺序的方法，reindex方法\n",
    "# 改变行调用reindex的index参数，如果新的序列有原df无的index自动空值，但是可以用fill_value填充空值\n",
    "df1.reindex(index=['王五','张三','李四','瓜六'],fill_value = 1) # 顺便还有个填充值的参数\n",
    "df1.reindex(columns = ['物理','英语','语文','数学']) \n",
    "\n",
    "# 排序改变行列顺序\n",
    "df1.sort_values(['英语','数学'],ascending = [True,False]) #支持列表表达多字段排序，同理声明降序/升序的也可以"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c0801e",
   "metadata": {},
   "source": [
    "### 七、dataframe了解信息与增删改查\n",
    "###### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be852ab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 读取文件中“房屋成交信息”表，以第一列为索引，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "# 1. 获取房源数据集的基本信息，包括行索引范围、每一列的数量、数据类型、内存占用等；\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\")\n",
    "df1.info()\n",
    "# 2. 获取房源数据集的描述性统计指标，仅包括：平均值、标准差、最小值、最大值、中位数；\n",
    "df1.describe().loc[['mean','std','min','max','50%']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a415d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题2\n",
    "# 读取文件中“房屋成交信息”表，以第一列为索引，按以下要求完成练习：\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",index_col = 0)\n",
    "# 1. 获取数据集的前5行；\n",
    "df1.head(5)\n",
    "# 2. 获取数据集的后3行；\n",
    "df1.tail(3)\n",
    "# 3. 设置不允许换行显示；\n",
    "pd.set_option('expand_frame_repr',False)\n",
    "# 4. 设置最大行数为无限制，显示数据集，重置最大行数；\n",
    "pd.set_option('display.max_rows',None)\n",
    "df1\n",
    "pd.reset_option('display.max_rows')\n",
    "# 5. 设置小数的小数点后保留1位，显示数据集，重置精度。\n",
    "pd.reset_option('display.precision',1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题3\n",
    "import pandas as pd\n",
    "# 读取文件中“房屋成交信息”表，以第一列为索引，按以下要求完成练习：\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",index_col = 0)\n",
    "# 1. 删除'天通苑05'所在行的数据；\n",
    "df1 =df1.drop('逸成东01',axis = 0) # 注意都不会改变原df\n",
    "\n",
    "# 2. 删除'链家网址'列；\n",
    "df1 =df1.drop('链家网址',axis = 1) # 注意都不会改变原df\n",
    "\n",
    "# 3. 删除'逸成东01', '汽南小01', '门头沟01' 所在行的数据；\n",
    "list1 = ['逸成东01','汽南小01','门头沟01']\n",
    "for x in list1:\n",
    "    if x not in df1.index:\n",
    "        list1.remove(x)\n",
    "df1 =df1.drop(list1,axis = 0) # 注意都不会改变原df\n",
    "\n",
    "# 4. 删除'平米均价', '楼层信息', '链家网址' 三列；\n",
    "list1 = ['平米均价','楼层信息','链家网址']\n",
    "for x in list1:\n",
    "    if x not in df1.columns:\n",
    "        list1.remove(x)\n",
    "df1 =df1.drop(list1,axis = 1)\n",
    "\n",
    "# 5. 删除第2行~第5行；\n",
    "df1.drop(df1.index[1:5],axis = 0)\n",
    "# 6. 删除有人关注的房源数据。\n",
    "df1 = df1.drop(df1[df1['关注人数'] > 0].index,axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6eb8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题4\n",
    "# 读取文件中“房屋成交信息”表，以第一列为索引，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "import random\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",index_col = 0)\n",
    "# 1. 为数据集新增'销售状态'列，值为1，表示已出售。\n",
    "df1['销售状态'] = 1 # 直接改变原df\n",
    "df1\n",
    "# 2. 为数据集新增'承泽苑01'行：# 社区编号为 1211027380051，交易时间为 2017-5-1，建成时间为 2017，# 卧室、客厅、厨房、浴室都是2，其余数值都为0，字符串都为 '-' 。\n",
    "df1.loc['承泽苑01'] = [1211027380051,pd.to_datetime('2017-5-1'),0,0,0,0,0,2,2,2,2,'-',2017,'-',1]\n",
    "df1\n",
    "\n",
    "# 3. 在数据集中的第2列插入'单元'列，值为一单元、二单元、三单元中随机的一个单元。\n",
    "list2 = []\n",
    "list1 = ['一单元','二单元','三单元']\n",
    "i = 0\n",
    "while i< len(df1.index):\n",
    "    x = random.choice(list1)\n",
    "    list2.append(x)\n",
    "    i = i+1\n",
    "\n",
    "df1.insert(1,'单元',[random.choice(list1) for _ in df1.index]) # 这个列表生成式相当于将上面list2的代码给压缩了\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce8160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题5\n",
    "# 读取文件中“房屋成交信息”表，以第一列为索引，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "import random\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_16.xlsx\",index_col = 0)\n",
    "\n",
    "# 1. 修改列的顺序为：挂牌天数、交易时间、所在社区编号、销售状态，销售状态为 0 ；\n",
    "df1['销售状态'] = 0\n",
    "df1[['挂牌天数','交易时间','所在社区编号','销售状态']]\n",
    "df1.reindex(columns = ['挂牌天数','交易时间','所在社区编号','销售状态'])\n",
    "# 2. 修改行的顺序为：新街口01、模式口01、霁月园01、逸成东01，霁月园01的数据为 '-' 。\n",
    "df1.loc['霁月园01'] = '-'\n",
    "df1.reindex(index = ['新街口01','模式口01','霁月园01','逸成东01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ed33d",
   "metadata": {},
   "source": [
    "### 八、dataframe的合并操作\n",
    "###### 1、pd.concat函数，dataframe.append方法同款操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91381749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "arr1,arr2 = [np.random.randint(0,101,(3,5)) for _ in range(0,2)]\n",
    "df1 = pd.DataFrame(arr1,index = ['李四','张三','王五'],columns = ['语文','数学','英语','物理','化学'])\n",
    "df2 = pd.DataFrame(arr1,index = ['李四','瓜六','王五'],columns = ['数学','语文','物理','体育','英语'])\n",
    "print(df1)\n",
    "print(df2)\n",
    "# 第一种链接方式df.append方法\n",
    "# 默认增加行，不能指定axis，遵循重行不重列；也无法指定链接方式，默认outer，不同列者保留之\n",
    "df1.append(df2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb260758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一种链接方式的升级版本pd.contact函数，沿用上面的df1,df2，请先运行上面代码\n",
    "# 升级点在于可以指定concat的axis（默认0），可以指定join（默认outer）\n",
    "pd.concat([df1,df2]) #默认与上面完全一致\n",
    "pd.concat([df1,df2],join = 'inner',axis = 0) # 指定axis = 0行时，inner会遵循重行不重列，多个拼接对象共有的列保留\n",
    "pd.concat([df1,df2],join = 'outer',axis = 1) # 指定axis = 1列时，会变成列拼接，行自动对齐不允许重行的状态\n",
    "pd.concat([df1,df2],join = 'inner',axis = 1) # 再改变join为inner时，多个对象共有的行才会保留"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50211fa3",
   "metadata": {},
   "source": [
    "### 八、dataframe的合并操作\n",
    "###### 2、pd.merge函数，sql同款链接大法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f32252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据源\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "武功秘籍 = pd.DataFrame(\n",
    "    {\n",
    "    '武功':['降龙十八掌','降龙十八掌','降龙十八掌','黯然销魂掌','黯然销魂掌','玉女剑法','碧海潮生曲','阴阳术'],\n",
    "    '招式':['亢龙有悔','飞龙在天','龙战于野','六神不安','杞人忧天','双剑合璧','碧海潮生曲','迟钝符'],\n",
    "    '伤害':[100,200,300,400,250,1000,500,200]    \n",
    "    }\n",
    "            )\n",
    "人物名册 = pd.DataFrame({'人物':['黄老邪','杨过','杨过','小龙女','郭靖'],\n",
    "                         '武功':['碧海潮生曲','黯然销魂掌','玉女剑法','玉女心经','降龙十八掌']\n",
    "                    }\n",
    "                   ) \n",
    "print(武功秘籍)\n",
    "print(人物名册)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f6ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 常用参数解析\n",
    "# left,right 指定需要链接的2个df，似乎不能大于2个\n",
    "# on,left on ,right on 这些字段指定on的字段，如果不写，自动识别出df中相同名称的列，否则必须需要指定on\n",
    "# left_index = True right_index = True 如果使用的columns变成了index那么可以使用和这个指定使用index列连表\n",
    "# how inner ，outer ，left ，right和sql里面的join是完全一致的逻辑 \n",
    "\n",
    "pd.merge(人物名册,武功秘籍,how = 'right' ,left_index = True,right_on = '武功') \n",
    "pd.merge(人物名册,武功秘籍,how = 'how' ,left_index = True,right_on = '武功')\n",
    "pd.merge(人物名册,武功秘籍,how = 'right' ,left_index = True,right_on = '武功') \n",
    "\n",
    "# DataFrame.rename方法重命名列名，参数 inplace=True 表示原地修改。例如将'秘技'修改为'绝招'，'招式'修改为'动作'：\n",
    "# df.reanme(columns={'秘技':'招式', '绝招':'动作'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 补充类知识，关于ipython和jupyter 可以通过代码调整display，之前mumpy中已经展示过展示图片的\n",
    "# 通过修改jupyter的网页显示样式可以使DataFrame数据并排显示，例如并排显示df1、df2：\n",
    "\n",
    "#     import pandas as pd\n",
    "#     from IPython.display import display, HTML\n",
    "#     CSS = \"\"\".output{flex-direction:row;}\"\"\"\n",
    "#     HTML('<style>{}</style>'.format(CSS))\n",
    "#     # display(df1, df2)\n",
    "#     display(df1) \n",
    "#     display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04b654",
   "metadata": {},
   "source": [
    "### 八、dataframe的合并操作\n",
    "###### 3、pd.merge函数后，会造成索引丢失问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据源\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.DataFrame(np.random.randint(1,101,(2,3)),columns = ['张三','李四','瓜六'],index = ['语文','英语'] )\n",
    "df2 = pd.DataFrame( {'数字编号':[117,118],'科目名称':['英语','语文']} )\n",
    "# 合并后，索引自动归零，df1原始数据集中的index列丢失\n",
    "pd.merge(df1,df2,left_index = True,right_on = '科目名称')\n",
    "# 操作方式 和变更索引有关，方式一重名的方式，手动增加一列等于索引列\n",
    "df1['科目名称'] = df1.index\n",
    "# 方式二，更加合理，还原索引为一列\n",
    "df1 = df1.drop('科目名称',axis = 1) # 不可插入同名列，所以先删除\n",
    "df1.index.name = '科目名称' # 增加下name则自动选择name作为列名，不加的话下面也有个参数可以配置\n",
    "df1.reset_index(inplace = True,names = '科目') # 原值替换，加上给出index的别名\n",
    "df1\n",
    "# 两种方式备份，完成后再merge\n",
    "pd.merge(df1,df2,left_on = '科目',right_on = '科目名称')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52021b",
   "metadata": {},
   "source": [
    "### 八、dataframe的合并操作\n",
    "###### 4、pd.merge函数后，排序顺序会发生变更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa5f1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行排序操作来解决变更问题\n",
    "# 排序方式一，sort_values(之前已经展示过)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.DataFrame(np.random.randint(1,101,(2,3)),columns = ['张三','李四','瓜六'],index = ['语文','英语'] )\n",
    "df2 = pd.DataFrame( {'数字编号':[117,118],'科目名称':['英语','语文']} )\n",
    "# 常用参数\n",
    "# axis 一样的axis参数让你行列都可排序\n",
    "# by 排序依据，可列表元组提供\n",
    "# ascending 排序升降序指定，排序依据，可列表元组提供\n",
    "# inplace 是否直接替换原df\n",
    "# kind 排序方式种类，有很多，默认quick\n",
    "# na_position 如果出现空值放在末尾还是首行，默认末尾\n",
    "# ignore_index 是否重新建立index，0到n-1，默认false\n",
    "# kye  允许使用函数callable是否允许呼叫引入的，function是典型的callable 学不会使用方式，采取sort的用法是不正确的，目前来看迂回解决\n",
    "# df1.sort_values(by = ['语文','英语'],axis = 1,ascending = [False,True],inplace = True,na_position = 'last') \n",
    "\n",
    "# 排序方式二，sort_index（配套操作index）,这个不需要指定列，只需要指定axis说明根据列索引排，还是行\n",
    "df1.sort_index(axis = 0,level = [0,1],ascending = [True,False] ,inplace = True,na_position = 'last') \n",
    "df1.sort_index??\n",
    "# axis 指定行index或者列columns排序\n",
    "# level 仅当index为多级别时才需要使用，平时请默认全部排序即可，多重索引时可以指定哪些排序，哪些不排序。但是由于多重索引重度依赖顺序，不如全部排序\n",
    "# ascending 指定是否升序\n",
    "# inplace 指定是否替换原df\n",
    "# na_position 指定空值所处位置\n",
    "\n",
    "# 回忆下列表排序方式\n",
    "# hax = lambda x: {'张三':1,'李四':3,'瓜六':2,}.get(x)\n",
    "# hax('张三')\n",
    "# list1 = ['张三','李四','瓜六']\n",
    "# list1.sort(key = hax)\n",
    "# print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35549de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为pandas的排序对中文的排序方式并非按照首字母，所以需要引入pinyin库还原中文为拼音，这样即可ASCII编码排序\n",
    "# !pip install pinyin -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# 控制台语句需要!运行\n",
    "import pinyin\n",
    "pinyin.get('三方',format = 'strip')\n",
    "df1.columns = [pinyin.get(i,format = 'strip') for i in df1.columns]\n",
    "df1.sort_index(ascending = True,axis = 1) # 成功解决问题\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bddbac",
   "metadata": {},
   "source": [
    "### 八、dataframe的合并操作\n",
    "###### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 读取文件中“A地区服务类”、“B地区食品类”表，以“年份”列为索引，拼接 A地区 2003~2006 年、B地区 2007~2010 年的总指数同比指数。\n",
    "# concat写法\n",
    "import pandas as pd\n",
    "dfa,dfb = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\居民价格消费指数.xlsx\",sheet_name = [0,1],index_col = '年份').values()\n",
    "pd.concat([dfa.loc[2003:2006,'总指数同比指数'],dfb.loc[2007:2010,'总指数同比指数']],axis = 1,join = 'outer')\n",
    "# append写法\n",
    "dfa.loc[2003:2006,'总指数同比指数'].append(dfb.loc[2007:2010,'总指数同比指数'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415790f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题2\n",
    "# 读取文件中“A地区服务类”、“B地区食品类”表，以“年份”列为索引，按以下要求完成练习：\n",
    "dfa,dfb = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\居民价格消费指数.xlsx\",sheet_name = [0,1],index_col = '年份').values()\n",
    "# 1. 合并两张工作表中的所有行列数据，保留全部索引，获取2007年的指数数据；\n",
    "pd.concat([dfa,dfb]).loc[2007]\n",
    "# 2. 合并两张工作表中的所有行列数据，忽略原始索引，获取 总指数同比指数、总指数定基指数。\n",
    "pd.concat([dfa,dfb],ignore_index = True)[['总指数同比指数','总指数定基指数']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题3\n",
    "# 读取文件中“A地区服务类”、“B地区食品类”表，以“年份”列为索引，按以下要求完成练习：\n",
    "dfa,dfb = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\居民价格消费指数.xlsx\",sheet_name = [0,1],index_col = '年份').values()\n",
    "# 1. 合并A、B两个地区的数据，年份不重复，获取全部服务类同比指数、食品类同比指数；\n",
    "pd.concat([dfa,dfb],axis = 1)[['服务类同比指数','食品类同比指数']]\n",
    "# 2. 合并A、B两个地区的数据，指数项不重复，获取 2007、2009、2011 年份中的总指数同比指数、总指数定基指数。\n",
    "pd.concat([dfa,dfb],axis = 0).loc[[2007,2009,2011],['总指数同比指数','总指数定基指数']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题4\n",
    "# 读取文件中“A地区服务类”、“B地区食品类”表，以“年份”列为索引，按以下要求完成练习：\n",
    "dfa,dfb = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\居民价格消费指数.xlsx\",sheet_name = [0,1],index_col = '年份').values()\n",
    "# 1. 合并A、B两个地区的数据，获取两个地区中都包含的年份中的数据；\n",
    "pd.concat([dfa,dfb],axis = 1,join = 'inner')\n",
    "# 2. 合并A、B两个地区的数据，获取两个地区中都包含的指数项中的数据。\n",
    "pd.concat([dfa,dfb],axis = 0,join = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题5\n",
    "# 读取文件中“课程”、“教师”、“学生”、“分数”表，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "dfsd,dfcourse,dfte,dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school.xlsx\",sheet_name = [0,1,2,3]).values()\n",
    "# 1. 按教师编号合并 “课程”、“教师” 数据集。\n",
    "dfcourse = dfcourse.set_index('课程编号')\n",
    "dfte = dfte.set_index('教师编号')\n",
    "pd.merge(left = dfcourse,right=dfte,left_on= '教师编号',right_index = True,how = 'outer')\n",
    "# 2. 按学生编号合并 “学生”、“分数” 数据集。\n",
    "pd.merge(dfsd,dfscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f5042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题6\n",
    "# 读取文件中“课程”、“教师”、“学生”、“分数”表，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "dfsd,dfcourse,dfte,dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school2.xlsx\",sheet_name = [0,1,2,3]).values()\n",
    "# 1. 按照评价合并课程，教师数据集；\n",
    "pd.merge(dfcourse,dfte,on = '评价')\n",
    "# 2. 按照教师编号合并课程，教师数据集\n",
    "# pd.merge(dfsd,dfscore)\n",
    "pd.merge(dfcourse,dfte,on = '教师编号')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d361e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题7\n",
    "# 读取文件中“学生”、“课程”、“分数”表，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "dfsd,dfcourse,dfte,dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school3.xlsx\",sheet_name = [0,1,2,3]).values()\n",
    "dfsd\n",
    "# 合并 分数、学生、课程 三个数据集，获取学生姓名、课程、分数等数据。\n",
    "dfnew = pd.merge(dfsd,dfscore,left_on = '编号',right_on = '学生编号' )[['学生姓名','课程编号','分数']]\n",
    "dfnew = pd.merge(dfnew,dfcourse,left_on = '课程编号',right_on = '编号' )[['学生姓名','课程','分数']]\n",
    "dfnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea59af9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题8\n",
    "# 读取文件中“学生”、“分数”表，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "dfsd,dfcourse,dfte,dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school4.xlsx\",sheet_name = [0,1,2,3]).values()\n",
    "# 1. 合并学生与分数数据集，获取两个表都有的学生的数据；\n",
    "pd.merge(dfsd,dfscore,how = 'inner')\n",
    "# 2. 合并学生与分数数据集，获取学生表中所有学生与分数数据；\n",
    "pd.merge(dfsd,dfscore,how = 'left')\n",
    "# 3. 合并学生与分数数据集，获取分数表中所有分数与学生数据；\n",
    "pd.merge(dfsd,dfscore,how = 'right')\n",
    "# 4. 合并学生与分数数据集，获取学生表所有学生与分数表所有分数数据。\n",
    "pd.merge(dfsd,dfscore,how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a182ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题9\n",
    "# 读取文件中“课程”、“教师”、“学生”、“分数”表，以'编号'为索引，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "dfsd,dfcourse,dfte,dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school5.xlsx\",\n",
    "                                           sheet_name = [0,1,2,3],\n",
    "                                           index_col = '编号'\n",
    "                                          ).values()\n",
    "# 1. 合并教师与课程数据集，合并依据为教师索引、教师编号；\n",
    "pd.merge(dfte,dfcourse,left_index = True ,right_on = '教师编号')\n",
    "#pd.merge(dfte,dfcourse)\n",
    "# 2. 合并分数与学生数据集，合并依据为学生编号、学生索引；\n",
    "pd.merge(dfsd,dfscore,left_index = True ,right_on = '学生编号')\n",
    "# 3. 获取学生'裴冬雪'的考试数据，包括学生姓名、课程、分数、教师姓名。\n",
    "dfnew = pd.merge(dfsd,dfscore,left_index = True , right_on = '学生编号' )[['学生姓名','课程编号','分数']]\n",
    "dfnew = pd.merge(dfnew,dfcourse,left_on = '课程编号',right_index = True )[['学生姓名','课程','分数','教师编号']]\n",
    "dfnew = pd.merge(dfnew,dfte,left_on = '教师编号',right_index = True )[['学生姓名','课程','分数','教师姓名']]\n",
    "dfnew.loc[dfnew['学生姓名'] == '裴冬雪']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题10\n",
    "# 读取文件中的课程和教师数据，课程数据以'课程'为索引。\n",
    "import pandas as pd\n",
    "df1,df2 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school21.xlsx\",sheet_name = [1,2]).values()\n",
    "df1.set_index('课程',inplace = True)\n",
    "# 按教师编号合并课程与教师数据集，根据课程数据集的索引重新指定合并后的索引为'课程名称'。\n",
    "df1['课程名称'] = df1.index\n",
    "dfn = pd.merge(df1,df2,left_on = '教师编号',right_on = '教师编号')\n",
    "dfn.index = dfn['课程名称']\n",
    "dfn.drop('课程名称',axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题11\n",
    "# 读取文件中的课程和教师数据。按'教师编号'合并数据集。\n",
    "import pandas as pd\n",
    "df1,df2 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school21.xlsx\",sheet_name = [1,2]).values()\n",
    "df1.rename(columns = {'评价':'课程评价'},inplace = True)\n",
    "df2.rename(columns = {'评价':'教师评价'},inplace = True)\n",
    "dfn = pd.merge(df1,df2,left_on = '教师编号',right_on = '教师编号')\n",
    "dfn\n",
    "# 1. 按'课程编号'升序排序数据；\n",
    "dfn.sort_values(by = '课程编号', ascending = True)\n",
    "# 2. 按'教师编号'降序排序数据；\n",
    "dfn.sort_values(by = '教师编号', ascending = False)\n",
    "# 3. 按'教师评价'降序排序数据\n",
    "dfn.sort_values(by = '教师评价', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题12\n",
    "# 读取文件中的课程和教师数据。按'教师编号'合并数据集。\n",
    "import pandas as pd\n",
    "df1,df2 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school21.xlsx\",sheet_name = [1,2]).values()\n",
    "df1.rename(columns = {'评价':'课程评价'},inplace = True)\n",
    "df2.rename(columns = {'评价':'教师评价'},inplace = True)\n",
    "dfn = pd.merge(df1,df2,left_on = '教师编号',right_on = '教师编号')\n",
    "dfn\n",
    "# 1. 按'教师编号'升序排列，相同教师按'课程评价'从高到低排列；\n",
    "dfn.sort_values(by = ['教师编号','课程评价'],ascending = [True,False])\n",
    "# 2. 按'课程评价'降序排列，相同评价按'教师评价'从高到低排列。\n",
    "dfn.sort_values(by = ['课程评价','教师评价'],ascending = [False,False],inplace = True)\n",
    "dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02475e7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题13\n",
    "# 读取文件中的学生和分数数据。按'学生编号'合并数据集。\n",
    "import pandas as pd\n",
    "import pinyin as py\n",
    "df1,df2 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school21.xlsx\",sheet_name = [0,3]).values()\n",
    "dfn = pd.merge(df1,df2)\n",
    "dfn\n",
    "# 按照学生姓名的汉字拼音升序，分数降序排列。\n",
    "dfn['姓名拼音'] = dfn['学生姓名'].map(lambda x: py.get(x,format = 'strip'))\n",
    "#dfn['姓名拼音'] = dfn['学生姓名'].apply(py.get,format = 'strip') # apply多一个能力，就是可以将其他的参数直接写在里面解决，不像map必须得lambda加工\n",
    "dfn.sort_values(by = ['姓名拼音','分数'],ascending = [True,False],inplace = True,ignore_index =True)\n",
    "dfn.drop('姓名拼音',inplace = True,axis = 1)\n",
    "dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题14\n",
    "# 读取文件中的学生、分数、课程数据。合并三个数据集。\n",
    "import pandas as pd\n",
    "df1,df2,df3 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school21.xlsx\",sheet_name = [0,1,3]).values()\n",
    "dfn = pd.merge(df1,df3,how='outer')\n",
    "dfn = pd.merge(dfn,df2,how='outer')\n",
    "dfn\n",
    "# 1. 以'分数'为索引，根据分数降序排序，缺失值排到最前面。\n",
    "dfn = dfn.set_index('分数')\n",
    "dfn.sort_index(ascending = False,na_position = 'first')\n",
    "# 2. 获取参加'地理'考试的 学生姓名、课程、分数。\n",
    "dfgeo = dfn.loc[dfn['课程'] == '地理',['学生姓名','课程']]\n",
    "# 3. 计算'地理'课程的 总分，平均分，小数点后保留一位。\n",
    "dfgeo.reset_index(inplace = True)\n",
    "round(dfgeo['分数'].sum(),1),dfgeo['分数'].mean().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd58aa",
   "metadata": {},
   "source": [
    "### 九、dataframe的数据透视，数据分组能力\n",
    "###### 1、group by 分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee88325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1,df2 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house_22.xlsx\",sheet_name = ['房屋成交信息','社区编号目录']).values()\n",
    "dfhouse = pd.merge(df1,df2,left_on = '所在社区编号',right_on = '社区编号')\n",
    "dfhouse.groupby(['卧室','总价'])[['总价','卧室','平米均价','面积']].sum()['面积']\n",
    "# 解析上面的案例\n",
    "# 第一步，dfhouse.groupby('卧室') \\ dfhouse.groupby(['卧室','总价'])，完成分组，需要分组的列写在后面，多列请采用列表打包\n",
    "# 第二步，找到需要执行聚合的列，以切片的写法放在group by 后面，未被选中的列不参与聚合\n",
    "# 第三步，指定聚合方式.sum(),.mean(),.max()代表对多选中字段一个聚合方式，如要指定不同字段不同聚合方式请采用如下写法\n",
    "dfhouse.groupby(['卧室']).aggregate({'面积':'mean','卧室':'sum'})\n",
    "# 第四步，单一聚合字段查看也可以继续写['面积']\n",
    "\n",
    "#filter筛选器，对于分组后的值，只要分组后的一条记录算出的mean大于500那么整个分组内所有值都会被保留\n",
    "dfhouse.groupby('社区名称').filter( lambda g: g['总价'].mean()>500 )\n",
    "# dfhouse.drop(dfhouse[dfhouse['总价']<=1000].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 分组对象的常用统计方法：\n",
    "\n",
    "#    count       返回DataFrame，各组计数（非空值）\n",
    "#    cumprod     各组从第1个元素至本元素的累乘之积\n",
    "#    cumsum      各组从第1个元素至本元素的累加之和\n",
    "#     head        各组的前n条数据\n",
    "#     max         最大值\n",
    "#     mean        平均值\n",
    "#     median      中位数\n",
    "#     min         最小值\n",
    "#     std         标准差\n",
    "#     sum         求和\n",
    "#     size        返回Series，各组计数（含空值）\n",
    "#     var         方差\n",
    "#     tail        各组的最后n条数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d519ef2",
   "metadata": {},
   "source": [
    "深入理解，如果分组字段是values的话，groupby('年级') 需要的是列名\n",
    "       如果分组字段是index的话，可以给出axis = 0来聚合，如果多重索引，请用level指定参数来聚合，具体内容见多重索引篇章"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4697c53",
   "metadata": {},
   "source": [
    "### 九、dataframe的数据透视，数据分组能力\n",
    "###### 2、数据透视表制造方式\n",
    "（1）group by 分组能力+unstack多重索引构成透视表 （2）pivot_table专门生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#（1）group by 分组能力+unstack多重索引构成透视表\n",
    "# 多重索引概念解释\n",
    "# 使的dataframe这种二维结构也可以容纳高纬数据，后面会详细解释\n",
    "dfhouse.groupby(['卧室','社区名称'])[['总价','卧室','平米均价','面积']].sum().unstack()\n",
    "dfhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#（2）pivot_table专门生成 另外还有个pd.pivot的弱化制造透视表函数\n",
    "# pd.pivot_table() or df.pivot_table 区别仅在于不需要指定一个参数data\n",
    "# data: 指定要使用的数据集，通常是一个 DataFrame 对象。\n",
    "# values: 指定要聚合的列或列列表，用于计算数据透视表的值。\n",
    "# index: 指定要作为行索引的列或列列表，用于分组数据。\n",
    "# columns: 指定要作为列索引的列或列列表，用于分组数据。\n",
    "# aggfunc: 指定如何聚合数据，默认为 'mean'。可以使用内置的聚合函数（如 'sum'、'count'、'max'、'min'、'mean' 等），也可以传递自定义的聚合函数。\n",
    "# fill_value: 指定用于填充缺失值的值。\n",
    "# margins: 指定是否添加行和列的汇总，默认为 False。如果设置为 True，则会在结果中添加汇总行和列。\n",
    "# dropna: 指定是否丢弃包含缺失值的行或列，默认为 True。\n",
    "# margins_name: 指定添加的汇总行和列的名称。\n",
    "# observed: 仅在使用分类数据时有效。指定是否仅使用观察到的因子值，默认为 False。\n",
    "pd.pivot_table(data = dfhouse,\n",
    "         values = ['总价','平米均价','面积'],\n",
    "         index='卧室',columns = '社区名称',\n",
    "         aggfunc = ['mean','sum','max'],\n",
    "         fill_value = 0,\n",
    "         margins = True,\n",
    "         dropna = False,\n",
    "         margins_name = '总计'\n",
    "        ).round(1) \n",
    "# DataFrame.round(n)方法对DataFrame整体设置小数保留的位数，例如设置所有小数保留2位：house.round(2) 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c352d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建示例数据透视表\n",
    "pivot_table = pd.pivot_table(data=df, index='City', columns='Year', values='Population', aggfunc='sum')\n",
    "\n",
    "# 使用 pd.melt() 还原透视表\n",
    "melted_df = pd.melt(pivot_table.reset_index(), id_vars='City', var_name='Year', value_name='Population')\n",
    "melted_df = melted_df.sort_values(['City', 'Year']).reset_index(drop=True)\n",
    "\n",
    "print(melted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35b239",
   "metadata": {},
   "source": [
    "### 九、dataframe的数据透视，数据分组能力\n",
    "###### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf71228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 读取文件中的分数数据，按要求完成以下练习：\n",
    "import pandas as pd\n",
    "dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school22.xlsx\",sheet_name = 3)\n",
    "# 1.统计每位学生的总分，显示学生编号、总分；\n",
    "dfscore.groupby('学生编号')['分数'].sum()\n",
    "# 2.统计各课程的平均分，显示课程编号、平均分。\n",
    "dfscore.groupby('课程编号')['分数'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643bb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题2\n",
    "# 读取文件中的数据，按以下要求完成练习：\n",
    "import pandas as pd\n",
    "dfcourse,dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school22.xlsx\",sheet_name = [1,3]).values()\n",
    "# 1. 合并分数与课程，统计各课程的最高分，按最高分从高到低排序，显示课程、最高分。\n",
    "dfn = pd.merge(dfcourse,dfscore,how = 'outer')\n",
    "\n",
    "dfn.groupby('课程')['分数'].max().sort_values(ascending=False)\n",
    "# 2. 统计各课程中参加考试的人数，分数是缺失值的表示未参加考试，按人数升序排序，显示课程、人数。\n",
    "dfn.groupby('课程')['分数'].count().sort_values(ascending = True) # 统计完成后给的是series，一样有排序方法，但是不需要指定by参数了，只有一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题3\n",
    "# 读取文件中的分数、课程、教师数据，统计每位学生考试的平均分、课程评价中位数、最低教师评价。\n",
    "import pandas as pd\n",
    "dfcourse,dfte,dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school22.xlsx\",sheet_name = [1,2,3]).values()\n",
    "dfn = pd.merge(dfscore,dfcourse)\n",
    "dfn = pd.merge(dfn,dfte,on = '教师编号')\n",
    "dfn.groupby('学生编号').aggregate({'分数':'mean','评价_x':'median','评价_y':'min',})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题4\n",
    "# 读取文件中的数据，按要求完成以下练习：\n",
    "import pandas as pd\n",
    "file_name = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school22.xlsx\"\n",
    "score = pd.read_excel(file_name, sheet_name = '分数')\n",
    "student = pd.read_excel(file_name, sheet_name = '学生')\n",
    "teacher = pd.read_excel(file_name, sheet_name = '教师')\n",
    "course = pd.read_excel(file_name, sheet_name = '课程')\n",
    "\n",
    "# 1. 获取学生平均分在72以上的学生姓名、分数、年龄；\n",
    "data = pd.merge(score, student)\n",
    "res1 = data.groupby('学生编号').filter(lambda x: x['分数'].mean()>72)\n",
    "res1 = res1[['学生姓名', '分数', '年龄']]\n",
    "# 2. 获取最高课程评价不低于8分，且授课数在2门以下的教师姓名、课程、课程评价。\n",
    "data = pd.merge(course, teacher, on='教师编号')\n",
    "res2 = data.groupby('教师姓名').filter(lambda g: (g['评价_x'].max()>=8) & (g['课程编号'].count()<2)) #位运算符和普通逻辑运算符都可以?\n",
    "res2 = res2[['教师姓名', '课程', '评价_x']]\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题5\n",
    "# 读取文件中的学生数据，统计男、女学生在每个年龄段的人数。\n",
    "import pandas as pd\n",
    "dfstudent = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 0)\n",
    "s1 = dfstudent.groupby(['性别','年龄'])['学生姓名'].count()\n",
    "s1.name = '人数'\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf76be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题6\n",
    "# 读取文件中的学生、分数数据，通过分组统计各年龄段中，各门课程的平均分，生成透视表，结果保留1位小数。\n",
    "import pandas as pd\n",
    "dfstudent = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 0)\n",
    "dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 3)\n",
    "dfn = pd.merge(dfstudent,dfscore,on = '学生编号')\n",
    "dfn.groupby(['年龄','课程编号'])['分数'].mean().round(1).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9c3e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题7\n",
    "# 读取文件中的学生、分数数据，展示每位学生每门课程的分数透视表，缺失值显示'缺考'。\n",
    "import pandas as pd\n",
    "dfstudent = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 0)\n",
    "dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 3)\n",
    "dfcourse = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 1)\n",
    "dfn = pd.merge(dfstudent,dfscore,on = '学生编号')\n",
    "dfn = pd.merge(dfn,dfcourse,on='课程编号')\n",
    "dfs = pd.pivot_table(data = dfn,values = '分数',index = '课程',columns = '学生姓名',fill_value ='缺考') \n",
    "dfs\n",
    "# 透视表制造维度本质是改变shape，如果聚合的维度已经是最明细层，那么你写sum，mean本身就都是一个值，比如和这个案例数据集里面每个学生每个课程一个分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a972e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题8\n",
    "# 1、读取文件中的分数、学生数据，统计每门课程中男、女学生的总分，\n",
    "import pandas as pd\n",
    "dfstudent = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 0)\n",
    "dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 3)\n",
    "dfcourse = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 1)\n",
    "dfn = pd.merge(dfstudent,dfscore,on = '学生编号')\n",
    "dfn = pd.merge(dfn,dfcourse,on='课程编号')\n",
    "dfn\n",
    "dfs = pd.pivot_table(data = dfn,values = '分数',index = '课程',columns = '性别',aggfunc = 'sum')\n",
    "# 2、展示女生总分200以上，且男生总分60以上的数据透视表。\n",
    "dfs[(dfs['女']>200) & ((dfs['男']>60))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题9\n",
    "# 读取文件中的学生、分数数据，统计男、女学生中各个年龄段各门课程的最低分数，缺失值显示0。\n",
    "import pandas as pd\n",
    "dfstudent = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 0)\n",
    "dfscore = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 3)\n",
    "dfcourse = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\school23.xlsx\",sheet_name = 1)\n",
    "dfn = pd.merge(dfstudent,dfscore,on = '学生编号')\n",
    "dfn = pd.merge(dfn,dfcourse,on='课程编号')\n",
    "dfs= pd.pivot_table(data = dfn,values = '分数',index = '性别',columns = ['年龄','课程'],fill_value= 0,aggfunc = 'min')\n",
    "# 留下一个疑问，多重索引如何正常切片处理？？？？？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba2a4f",
   "metadata": {},
   "source": [
    "### 十、dataframe的字符串处理能力\n",
    "###### 1、str转化series为字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1976b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 思考找到社区名称='天通苑东一区'\n",
    "import pandas as pd\n",
    "df1,df2 = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house原始.xlsx\",sheet_name = [0,1]).values()\n",
    "dfhouse=pd.merge(df1,df2,left_on = '所在社区编号',right_on = '社区编号')[['id','总价','面积','社区名称']]\n",
    "dfhouse[dfhouse['社区名称'] == '天通苑东一区']\n",
    "# 但是要找到所有名字中带天通苑的就有点麻烦了，因为之所以能直接写== '天通苑东一区'是基于广播原则复制了97份和dfhouse['社区名称']每一行做对比的\n",
    "# 字符操作不能直接用于series，所以引入.str办法，将series视作字符串处理，这样切片处理都能做了\n",
    "dfhouse[dfhouse['社区名称'].str[:3] == '天通苑']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46e9cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pandas支持的字符串对象str的处理方法：\n",
    "\n",
    "# split()       拆分字符串\n",
    "# replace()     替换字符串\n",
    "# strip()       去掉首位空白字符\n",
    "# rstrip()      去掉右侧空白字符\n",
    "# lstrip()      去掉左侧空白字符\n",
    "# upper()       转换为大写\n",
    "# lower()       转换为小写\n",
    "# len()         字符串长度\n",
    "# contains()    是否包含指定内容\n",
    "# startswith()  是否以指定文字开头\n",
    "# isalnum()     是否全为数字和字母\n",
    "# isalpha()     是否全为字母\n",
    "# isdigit()     是否全为数字\n",
    "# islower()     是否全部小写\n",
    "# isupper()     是否全部大写\n",
    "# match()       是否符合指定正则式\n",
    "# findall()     找出全部匹配结果\n",
    "# extract()     匹配并提取捕获组 \n",
    "\n",
    "#print('王 国'.split(sep = ' '))# 默认就是空格做分隔符\n",
    "# dfhouse['社区名称新'] = [i[:3]+' '+i[3:] for i in dfhouse['社区名称']]\n",
    "# print(dfhouse['社区名称新'].str.split()) # object指代对象，代表多种数据类型混合在一列，比如开裂成列表\n",
    "\n",
    "# print('王国'.replace('王','神')) 普通替换，下面有升级的正则版本\n",
    "# print(dfhouse['社区名称'].str.replace('天通苑', 'sssssse区'))\n",
    "\n",
    "# dfhouse['社区名称'] = ['六六'+' '+i+ ' '+'sss'for i in dfhouse['社区名称']]\n",
    "# dfhouse['社区名称'] = dfhouse['社区名称'].str.strip('六六') # 指定内容去除，不指定默认去除两端空格\n",
    "# dfhouse\n",
    "\n",
    "# print(len('王 国')) # 有些用法是函数不是方法，但是df统一了\n",
    "# print(dfhouse['社区名称'].str.len()) # object指代对象，代表多种数据类型混合在一列\n",
    "\n",
    "'国' in '王国' # 字符串没有contains方法，但是用in可以判断\n",
    "dfhouse['社区名称'].str.contains('区')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df40c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最特别的应该是正则，原本需要import re 来使用，这边甚至不需要引入\n",
    "import pandas as pd\n",
    "dfhouse = pd.read_excel(io = r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\house正则.xlsx\",sheet_name = 0)\n",
    "\n",
    "# match() 是否符合指定正则式 参数pat写正则表达式；case写区分大小写；na指定缺失值的表示方式\n",
    "dfhouse[dfhouse['社区名称'].str.match(pat = r'(\\w*?)[东南]?[一二三]$' ,case = True,na = 'NaN')] # match返回的是True or False，所以可以拿去布尔掩码\n",
    "\n",
    "# findall() 找出全部匹配结果\n",
    "dfhouse['社区名称'].str.findall(pat = r'(\\w*?)[东南]?[一二三]$') # 找到所有的东西并提出捕获组内容作为一个列表 \n",
    "\n",
    "# extract() 匹配并提取所有捕获组，多个组多个列 参数pat写正则表达式；expand指定返回的形式，可以使dataframe，可以是series\n",
    "dfhouse['社区名称'].str.extract(pat = r'(\\w*?)[东南]?[一二三]$',expand = True) \n",
    "\n",
    "# 通过str.replace()方法设置正则表达式与替换后的字符串（支持捕获组），在旧版本的pandas中replace方法默认使用正则替换，\n",
    "# 在2021年之后的新版本中默认不使用正则替换，当参数 regex=True 时使用正则替换。\n",
    "dfhouse['社区本名'] = dfhouse['社区名称'].str.replace(r'(\\w*?)[东南]?[一二三]$', r'\\1', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb82f02",
   "metadata": {},
   "source": [
    "### 十、dataframe的字符串处理能力\n",
    "###### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 读取文件中的数据，按照以下要求获取数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data24.xlsx\")\n",
    "\n",
    "# 1. 获取监测点在'华侨城'的数据；\n",
    "df1[df1['监测点'] == '华侨城' ]\n",
    "# 2. 获取监测点在'盐田'或'观澜'的数据；\n",
    "df1[df1['监测点'].isin( ['盐田','观澜']) ]\n",
    "# 3. 在'指数级别'后新增'指数等级'列，根据指数级别生成对应的中文数字，例如'一级'对应'一'。\n",
    "df1.insert(5,'指数等级',df1['指数级别'].str[0:1])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d31ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题2\n",
    "# 读取文件中的数据，按照以下要求获取数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data24.xlsx\")\n",
    "# 1. 获取指数颜色以'空气质量'为开头的数据；\n",
    "df1[df1['指数颜色'].str.startswith('空气质量')  ]\n",
    "# 2. 获取健康影响状况'令人满意'的数据。\n",
    "df1[df1['健康影响状况'].str.contains('令人满意')  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2cbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题3\n",
    "# 读取文件中的数据，按照以下要求获取数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data24.xlsx\")\n",
    "# 1. 获取首要污染物全部为字母或汉字的数据；\n",
    "df1[df1['首要污染物'].str.isalpha()]\n",
    "# 2. 获取首要污染物为四个字，且健康影响状况'令人满意'的数据。\n",
    "df1[ (df1['首要污染物'].str.len() == 4) & (df1['健康影响状况'].str.contains('令人满意'))   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题4\n",
    "# 读取文件中的数据，按照以下要求获取数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data24.xlsx\")\n",
    "# 1. 获取健康影响状况以'空气质量'为开头的的数据；\n",
    "df1[df1['健康影响状况'].str.match(pat = r'^空气质量')]\n",
    "# 2. 新建'状况'列，根据健康影响状况中对空气质量的描述替换为'令人满意'或'可接受'。\n",
    "df1['状况']= df1['健康影响状况'].str.replace( r'空气质量(\\w*)，\\w*',    r'\\1',     regex = True)\n",
    "df1['健康影响状况'].str.extract( r'(空气)质量(\\w*)，\\w*') # 多一个捕获组多返回一列\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd92b89",
   "metadata": {},
   "source": [
    "### 十一、dataframe的时间处理能力\n",
    "###### 1、读入时间 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e87932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （1） 读入的时候不一定就是时间类型，需要转化\n",
    "import pandas as pd \n",
    "df1 = pd.read_csv(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\trans.csv\")\n",
    "print(type(df1.loc[0,'交易日期'])) # 读入的是字符串，需要先进行时间转化处理\n",
    "\n",
    "# python的时间类型格式\n",
    "# python 自带的datetime，可以毫秒精确，但是numpy自制了datetime64，可以飞秒精确，但是方法太少，于是python包装出了pandas.Timestamp pandas.DatetimeIndex\n",
    "\n",
    "# （2）转化方式 to_datetime\n",
    "# 既适配字符串处理\n",
    "pd.to_datetime('20070101')\n",
    "print(type(pd.to_datetime('2007/01/01 15:37:27.000000001')))\n",
    "\n",
    "# 也适配一整列数据，容器\n",
    "df1['交易日期'] = pd.to_datetime(df1['交易日期'])\n",
    "df1.loc[0,'交易日期']-df1.loc[1,'交易日期'] # Timedelta('0 days 14:21:16') 这个请看下面的回忆delta\n",
    "\n",
    "# （3）完整的参数如下，常用的给出解释\n",
    "# pd.to_datetime(arg, format=None, errors='raise', dayfirst=False, yearfirst=False, utc=None, box=True, exact=True, unit=None, infer_datetime_format=False, origin='unix')\n",
    "# arg 表示待转换的字符串\n",
    "# format表示待转换的字符串的格式描述 ，和普通的datetime.datetime.strptime 一样的描述结构\n",
    "    # %Y表示四位数的年份，\n",
    "    # %m表示两位数的月份，\n",
    "    # %d表示两位数的日期，\n",
    "    # %H表示小时数（24小时格式），\n",
    "    # %M表示分钟数，\n",
    "    # %S表示秒数。\n",
    "# errors='raise' 无法解析的字符串如何处理 raise表示引发异常，coerce表示转换为NaT（Not-a-Time）对象，ignore表示返回原始值。\n",
    "# unit 参数用于指定输入数据的时间单位。\n",
    "\n",
    "# （4）其中unit参数应用于转化时间戳\n",
    "# 13位时间戳是毫秒单位；10秒时间戳是秒单位\n",
    "import time\n",
    "print(time.time())# 1696129092.371583\n",
    "print(pd.to_datetime(time.time(),unit = 's')) # 10位所以给个s，万一不正常转化了，就手动指定下时间单位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b33ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696406315.9921103\n",
      "<class 'float'>\n",
      "2005\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 9, 26, 0, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进一步学习pandas的时间类型前，我们先回忆下普通python的时间知识\n",
    "# 使用的库有两个\n",
    "import time\n",
    "import pytz\n",
    "print(time.time()) # 时间戳\n",
    "# time.sleep(3) #睡个几秒钟\n",
    "import pandas as pd\n",
    "timestamp = pd.Timestamp.now().timestamp() # 时间戳 ，pd.Timestamp生成的更接近与传统的某个日期点，而非秒数的那种表达，所以才.timestamp()\n",
    "# pd.Timestamp()支持转化字符串/时间戳浮点数格式为正常的datetime64类型\n",
    "indtz = pytz.timezone('Asia/Kolkata')\n",
    "indtoday = pd.Timestamp.today(tz = indtz).normalize() #生成无小时的时间，但是带有tz信息，必须用.tz_localize(None)去除时区信息，不然就无法和无时区的timestamp对比\n",
    "\n",
    "print(type(timestamp)) # 时间戳是float\n",
    "import datetime\n",
    "time = datetime.datetime(2005,6,8) #和sql一样的构造时间法\n",
    "print(time.year) # 构造的日期具有.year,.month.……的方法可以拿取任意一个时间段\n",
    "\n",
    "# 时间转字符串，指定格式\n",
    "print(type(datetime.datetime.strftime(time ,format = '%Y-%m-%d'))) \n",
    "# 字符串转时间，指定格式\n",
    "s = '2023-08-27'\n",
    "\n",
    "time2 = datetime.datetime.strptime(s,'%Y-%m-%d')\n",
    "# datetime.timedelta 制造时间差，但是dateutil.relativedelta(years = 500)三方更强，提供了更多的差值算法，比如year\n",
    "time2 + datetime.timedelta(days =30,seconds = 360) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590e7da",
   "metadata": {},
   "source": [
    "### 十一、dataframe的时间处理能力\n",
    "###### 2、依据时间查询数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fe9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （1）将时间作为index，并排序，虽然不拍应该也没问题，但还是保险点\n",
    "df1.set_index('交易日期',inplace = True)\n",
    "df1.sort_index(inplace = True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （2）使用索引切片等方式按时间获取\n",
    "df1.loc['1997/12/31 23:59:49'] #字符串比较的话，改格式肯定识别不了，但是datetime可以\n",
    "df1.loc['1996-01-01'] # 允许日期段落处理\n",
    "df1.loc['1996-01-01 15:08'] # 允许分钟 段落处理\n",
    "df1.loc['1996-01-01':'1996-05-08'] # 即使给出任意时间段，即使切片的时间段不存在于df中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663bea56",
   "metadata": {},
   "source": [
    "### 十一、dataframe的时间处理能力\n",
    "###### 3、转化时间格式  .dt让数据列变为datetime处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df1 = pd.read_csv(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\trans.csv\")\n",
    "print(type(df1.loc[0,'交易日期'])) # 读入的是字符串，需要先进行时间转化处理\n",
    "df1['交易日期'] = pd.to_datetime(df1['交易日期'])\n",
    "df1['星期几']=df1['交易日期'].dt.dayofweek # 一天是一周中的周几，0代表星期一，数字6代表星期日\n",
    "df1['月份']=df1['交易日期'].dt.month # 12个月\n",
    "pd.pivot_table(data = df1 ,values = '金额',index = '星期几',aggfunc= 'sum') # 一列也可以是dataframe，请注意哦，这个和下面基本等价\n",
    "df1.groupby('星期几')['金额'].sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035fab1",
   "metadata": {},
   "source": [
    "### 十一、dataframe的时间处理能力\n",
    "###### 4、df和series都支持的平移数据条能力，sql需要开窗；另一个是时间差算法，制造时间差方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70685208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 准备数据集 \n",
    "import pandas as pd \n",
    "df1 = pd.read_csv(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\trans.csv\")\n",
    "df1['交易日期'] = pd.to_datetime(df1['交易日期'])\n",
    "dfn = df1.sort_values(by = '交易日期',ignore_index = 1)\n",
    "dfn = dfn.loc[(dfn['账户编号'] == 2504) & (dfn['交易类型'] =='信用卡取款')]\n",
    "dfn.reset_index(inplace = True)\n",
    "dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7d603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 题目，帮我们计算交易间隔时间\n",
    "# （1）解法一，循环搞定\n",
    "# dfn['信用卡取款间隔'] = list([dfn.iloc[i+1,3]-dfn.iloc[i,3] for i in range(0,dfn.index[-1])]).append['']\n",
    "# print(type(list([dfn.iloc[i+1,3]-dfn.iloc[i,3] for i in range(0,dfn.index[-1])])))\n",
    "# (2) 解法二，平移方法搞定\n",
    "# 平移方法同时适用于于series和df \n",
    "# periods 指定平移的距离，默认向下和右，写负数代表向上和左\n",
    "# freq \n",
    "# axis 指定列平移还是行平移\n",
    "# fill_value 填充空值\n",
    "dfn['上次交易日期'] = dfn['交易日期'].shift(1)\n",
    "dfn['交易间隔'] = dfn['交易日期']-dfn['上次交易日期']\n",
    "# dfn.info() # 可以看到时间差的数据类型时timedelta64[ns]ns代表NanoSecond 纳秒，所以怎么转化为小时呢\n",
    "\n",
    "#（3）延伸，如何改变时间差展示格式，按照小时？其实就是转化数据类型\n",
    "dfn['交易间隔'].astype('timedelta64[s]') # 秒\n",
    "dfn['交易间隔'].astype('timedelta64[h]') # 小时\n",
    "# Y M D W h m s ms ns\n",
    "# 年月日周时分秒毫秒纳秒\n",
    "\n",
    "#(4）pd生成时间差 pd.Timedelta\n",
    "pd.Timedelta(3,'D')\n",
    "pd.Timedelta('3D5h')\n",
    "dfn['新交易日期'] = dfn['交易日期'] + pd.Timedelta('3D5h')\n",
    "\n",
    "# D  W  h  T/m  S   L     U    N/ns\n",
    "#日 周 时  分  秒 毫秒  微秒  纳秒   \n",
    "\n",
    "# (5)时间系列索引迁移能力,仅在数据索引上使用\n",
    "# This method is only implemented for DatetimeIndex, PeriodIndex and TimedeltaIndex; Got type Index\n",
    "dfn.index = dfn['交易日期']\n",
    "dfn.sort_index(inplace = True)\n",
    "dfn.shift(3,axis = 0,freq = 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0cced1",
   "metadata": {},
   "source": [
    "### 十一、dataframe的时间处理能力\n",
    "###### 5、分组/重采样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741ff59",
   "metadata": {},
   "source": [
    "### 十一、dataframe的时间处理能力\n",
    "###### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b00f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 读取文件中的数据，按照以下要求处理数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data25.xlsx\")\n",
    "# 1. 将时间数据转换为pandas时间类型生成'时间对象'列；\n",
    "print(type(df1.loc[0,'时间']))\n",
    "df1['时间1'] = pd.to_datetime(df1['时间'])\n",
    "print(type(df1.loc[0,'时间1']))\n",
    "df1['时间2'] = df1['时间'].astype('datetime64[ns]')\n",
    "print(type(df1.loc[0,'时间2']))\n",
    "print(type(df1.loc[0,'时间']))\n",
    "# 2. 根据'时间对象'列生成时间索引并排序。\n",
    "df1.index = df1['时间']\n",
    "df1.sort_index(inplace = True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153361ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题2\n",
    "# 读取文件中的数据，按照以下要求获取数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data25.xlsx\")\n",
    "# 1. 生成时间索引，获取2019年7月7日的监测数据；\n",
    "df1['时间1'] = pd.to_datetime(df1['时间'])\n",
    "df1.index = df1['时间1']\n",
    "df1.sort_index(inplace = True)\n",
    "\n",
    "df1.loc['2019-07-07']\n",
    "# 2. 获取2020年3月的监测数据；\n",
    "df1.loc['2019-03']\n",
    "# 3. 获取2017年第一季度'南澳'地区的监测数据。\n",
    "df1['时间1'].dt.to_period('Q') # 利用to_period的能力手动制造辅助列，Q季度 Y年 M月\n",
    "dfn = df1.loc[(df1['时间1']>='2017-01') & (df1['时间1']<'2017-04') & (df1['监测点'] == '南澳')] # 就是那个你要理解2017-01-01是等于2017-01的\n",
    "dfs = (df1.loc['2017-01': '2017-03']).loc[(df1.loc['2017-01': '2017-03'])['监测点'] == '南澳'] # 更加安全\n",
    "\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c20a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题3\n",
    "# 读取文件中的数据，按照以下要求获取数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data25.xlsx\")\n",
    "df1['时间1'] = pd.to_datetime(df1['时间'])\n",
    "df1.index = df1['时间1']\n",
    "df1.sort_index(inplace = True)\n",
    "# 1. 拆分时间中的年、月、日，并生成三个新列；\n",
    "df1['年'] = df1['时间1'].dt.year\n",
    "df1['月'] = df1['时间1'].dt.month\n",
    "df1['日'] = df1['时间1'].dt.day\n",
    "df1\n",
    "# 2. 统计2019年的平均污染指数，结果保留两位小数。\n",
    "df1.loc['2019','污染指数'].mean().round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题4\n",
    "# 读取文件中的数据，按照以下要求获取数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data25.xlsx\")\n",
    "df1['时间1'] = pd.to_datetime(df1['时间'])\n",
    "df1.index = df1['时间1']\n",
    "df1.sort_index(inplace = True)\n",
    "# 1. 统计各年的平均污染指数，结果保留两位小数；\n",
    "df1['年'] = df1['时间1'].dt.year\n",
    "df1.groupby('年')['污染指数'].mean().round(2)\n",
    "# 2. 统计各年、各月的污染指数总和，生成透视表，缺失值填充0。\n",
    "df1['月'] = df1['时间1'].dt.month\n",
    "df2 = pd.pivot_table(data = df1,values = '污染指数',index = '年',columns = '月',aggfunc = 'sum',fill_value = 0 )\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9945ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题5\n",
    "# 读取文件中的数据，按照以下要求处理数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data25.xlsx\")\n",
    "df1\n",
    "#1. 转换'时间'列为时间对象；\n",
    "df1['时间'] = pd.to_datetime(df1['时间'])\n",
    "df1.index = df1['时间']\n",
    "df1.sort_index(inplace = True)\n",
    "# 2. 获取'南澳'地区的监测数据，将监测时间向上平移1行作为新列，缺失日期为2016年1月1日；\n",
    "dfna = df1.loc[df1['监测点'] == '南澳']\n",
    "dfna['时间'].shift(-1,fill_value = pd.Timestamp('2016-01-01'))\n",
    "# 3. 获取'盐田'地区的监测数据，将监测时间向下平移3行作为新列，缺失日期为2017年1月1日\n",
    "dfyt = df1.loc[df1['监测点'] == '盐田']\n",
    "dfna['时间'].shift(3,fill_value = pd.Timestamp('2017-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题6\n",
    "# 读取文件中的数据，按照以下要求获取数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data25.xlsx\")\n",
    "df1['时间'] = pd.to_datetime(df1['时间'])\n",
    "df1.index = df1['时间']\n",
    "df1.sort_index(inplace = True)\n",
    "# 计算'洪湖'地区每次监测距离上次的时间间隔天数。\n",
    "dfhh = df1.loc[df1['监测点'] == '洪湖']\n",
    "dfhh.insert(3,'上次监测时间',dfhh['时间'].shift(1))\n",
    "dfhh.insert(5,'时间差',dfhh['时间']-dfhh['上次监测时间'])\n",
    "dfhh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题7\n",
    "# 读取文件中的数据，按照以下要求获取数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data25.xlsx\")\n",
    "df1['时间'] = pd.to_datetime(df1['时间'])\n",
    "df1.index = df1['时间']\n",
    "df1.sort_index(inplace = True)\n",
    "# 1. 计算'西乡'地区每次监测距离上次的时间间隔天数；\n",
    "dfxx = df1.loc[df1['监测点'] == '西乡']\n",
    "dfxx.insert(3,'上次监测时间',dfxx['时间'].shift(1))\n",
    "dfxx.insert(5,'时间差',dfxx['时间']-dfxx['上次监测时间'])\n",
    "dfxx\n",
    "# 2. 根据间隔天数获取间隔小时数作为新列；\n",
    "dfxx.insert(6,'小时差',dfxx['时间差'].astype('timedelta64[m]')/60)\n",
    "# 3. 根据间隔天数获取间隔周数，周数以小数精确表示，保留2位小数。\n",
    "dfxx.insert(6,'星期差',(dfxx['时间差'].astype('timedelta64[D]')/7).round(2))\n",
    "dfxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff92eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题8\n",
    "# 读取文件中的数据，按照以下要求获取数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data25.xlsx\")\n",
    "df1['时间'] = pd.to_datetime(df1['时间'])\n",
    "df1.index = df1['时间']\n",
    "df1.sort_index(inplace = True)\n",
    "# 1. 筛选指数类别为'优'，且指数颜色为'绿色'，且污染指数小于16的数据；\n",
    "dfx = df1.loc[(df1['指数类别'] == '优') &  (df1['指数颜色'] == '绿色') & (df1['污染指数']<16 )]\n",
    "# 2. 将筛选数据中的监测时间延后1天半作为新时间列；\n",
    "timedelta = pd.Timedelta('1D12h')\n",
    "dfx.insert(6,'时间1',dfx['时间'].values+timedelta )\n",
    "# 3. 将新时间列设置为索引，将时间索引延后6小时。\n",
    "dfx.index = dfx['时间1']\n",
    "dfx.index = dfx.index.shift(6,freq = 'h')\n",
    "dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fe351",
   "metadata": {},
   "source": [
    "### 十二、dataframe的数据清洗\n",
    "###### 1、数据类型转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采用astype能力\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\进货.xlsx\")\n",
    "df1['进货总价'] = df1['进货总价'].str.extract(pat = r'[^\\d]*(\\d+)[^\\d]*') # 正则表达式返回的是文本\n",
    "df1['进货总价'] = df1['进货总价'].astype('int64') # 所以先转化一次数据类型为整数\n",
    "\n",
    "df1['品级'] = df1['品级'].astype('str')+'级'\n",
    "df1\n",
    "# 常用的astype转换数据类型\n",
    "# object        各种对象\n",
    "# str           字符串\n",
    "# int64         整数\n",
    "# float64       浮点数\n",
    "# bool          逻辑值\n",
    "# datetime64    时间日期\n",
    "# timedelta[ns] 时间差\n",
    "\n",
    "# 常见需求示例，数据格式规范化\n",
    "#1、正则式去除无用字符\n",
    "#2、astype转换数据类型\n",
    "#3、复杂情况是用apply自定义函数逐个解决\n",
    "def int_to_float(s):\n",
    "    if isinstance(s,int):\n",
    "        return float(s)\n",
    "    else:\n",
    "        return -1\n",
    "df1['数量'].apply(int_to_float)\n",
    "\n",
    "# NaN是用astype转化时居然不是False ，0 被转化时是False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080993f7",
   "metadata": {},
   "source": [
    "### 十二、dataframe的数据清洗\n",
    "###### 2、缺失值处理 \n",
    "（1）定位（2）删除（3）替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （1）定位缺失值\n",
    "# df/series.isnull()\n",
    "# df/series.notnull()\n",
    "import numpy as np\n",
    "df1.loc[0,'数量'] = np.nan # 生成缺失值的方式\n",
    "df1.isnull() # 返回的也是一个同形状的df逻辑值，可以布尔索引\n",
    "df1[df1['数量'].isnull()]\n",
    "df1[df1['数量'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c1e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （2）删除缺失值\n",
    "#df1.dropna()\n",
    "# how 参数 = 'all' #全空值才删除 ; 'any' # 只要某行或某列中包含缺失值，就删除该行或\n",
    "# thresh 参数 = 2 # 一条记录一行2个极其以上缺失值则删除  threshold  # n. 起点，门槛，阈值\n",
    "# axis 参数  # 按照行还是列代表一条记录\n",
    "# inplace参数\n",
    "\n",
    "# 案例\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df12 = pd.DataFrame(np.random.randint(1,100,(3,5)))\n",
    "df12.loc[0,1] = np.nan\n",
    "df12.loc[1,1] = np.nan\n",
    "df12.loc[1,2] = np.nan\n",
    "df12.loc[2,0] = np.nan\n",
    "df12.loc[2,1] = np.nan\n",
    "df12.loc[2,2] = np.nan\n",
    "df12.dropna(thresh = 3) # 这个域值是对非空值而言的，所以要想保留几个空值活着，\n",
    "# 得用列数-保留次数 5-3 = 2只会保留小于等于2的空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8772a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3)替换填充缺失值\n",
    "# 机器学习专题介绍缺失值处理的思维，目前已知的方法是可以选择采用平均值，或者照抄前后\n",
    "#df1.fillna()\n",
    "# limit参数 = 限制填充个数\n",
    "# method 参数 = 2 # 一条记录向前向左向后向右就近填充值\n",
    "# axis 参数  # 按照行还是列执行填充\n",
    "# inplace参数\n",
    "\n",
    "df1.fillna(0) # 替换为一个值\n",
    "df1.fillna({'进货总价':df1['进货总价'].mean(),'数量':0,'品级':'待定'}) # 按列替换值\n",
    "df1.fillna(method = 'bfill') #ffill forward 向前抄  bfill backward 向后抄 nearest就近抄一个非缺失值\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 创建示例数据帧\n",
    "df = pd.DataFrame({'A': [1, np.nan, np.nan, 4],\n",
    "                   'B': [np.nan, 2, np.nan, 5],\n",
    "                   'C': [10, 20, np.nan, 40]})\n",
    "\n",
    "# 按列填充不同的值或方法,向前面或者后面对单列处理可以如下嵌套\n",
    "df.fillna(\n",
    "    {'A': df['A'].fillna(method = 'pad'), \n",
    "     'B': df['B'].fillna(method = 'backfill'), \n",
    "     'C': df['C'].mean()} \n",
    ")\n",
    "# pad另外一种向前抄；backfill另外一种向后抄\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb86ad",
   "metadata": {},
   "source": [
    "### 十二、dataframe的数据清洗\n",
    "###### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题1\n",
    "# 按照以下要求处理数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"C:\\Users\\yaoyan\\Downloads\\data27.xlsx\")\n",
    "df1\n",
    "# 1. 对长度列进行规范化，要求数据为浮点数；\n",
    "df1['长度'] = df1['长度'].str.replace(r'[^\\d]*(\\d+.(\\d+)?)[^\\d]*',r'\\1',regex = 1).astype('float64')\n",
    "#  2. 长度列的缺失值填充为0。\n",
    "df1.fillna({'长度':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f67aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 课后习题2\n",
    "# 读取文件中的数据，按照以下要求处理数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data27.xlsx\")\n",
    "\n",
    "# 1. 筛选出长度是缺失值的数据；\n",
    "df1[df1['长度'].isnull()]\n",
    "# 2. 筛选出简称、等级、长度都是缺失值的数据；\n",
    "df1[(df1['简称'].isnull()) & (df1['等级'].isnull()) & (df1['长度'].isnull())]\n",
    "# 3. 筛选出街道是缺失值，但等级不是缺失值的数据。\n",
    "df1[(df1['街道'].isnull()) & (df1['等级'].notnull()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "975efc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>名称</th>\n",
       "      <th>简称</th>\n",
       "      <th>等级</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>省立绿道2号线</td>\n",
       "      <td>省绿道网</td>\n",
       "      <td>省绿道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>绿道深圳5号线（石樟坑水库-银湖森林公园）龙岗段</td>\n",
       "      <td>省绿道网</td>\n",
       "      <td>省绿道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>求水岭绿道</td>\n",
       "      <td>郊野绿道，低碳城配套绿道。</td>\n",
       "      <td>社区绿道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>屋角头绿道</td>\n",
       "      <td>郊野绿道，低碳城配套绿道。</td>\n",
       "      <td>社区绿道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>西坑绿道</td>\n",
       "      <td>省绿道网</td>\n",
       "      <td>省绿道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>求水岭绿道</td>\n",
       "      <td>郊野绿道，低碳城配套绿道。</td>\n",
       "      <td>社区绿道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>屋角头绿道</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>西坑绿道</td>\n",
       "      <td>省绿道网</td>\n",
       "      <td>省绿道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>谭仙庙绿道</td>\n",
       "      <td>省绿道网</td>\n",
       "      <td>省绿道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>保安绿道</td>\n",
       "      <td>接驳园山和大运公园</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          名称             简称    等级\n",
       "0                    省立绿道2号线           省绿道网   省绿道\n",
       "1   绿道深圳5号线（石樟坑水库-银湖森林公园）龙岗段           省绿道网   省绿道\n",
       "2                      求水岭绿道  郊野绿道，低碳城配套绿道。  社区绿道\n",
       "3                      屋角头绿道  郊野绿道，低碳城配套绿道。  社区绿道\n",
       "4                       西坑绿道           省绿道网   省绿道\n",
       "..                       ...            ...   ...\n",
       "75                     求水岭绿道  郊野绿道，低碳城配套绿道。  社区绿道\n",
       "76                     屋角头绿道            NaN   NaN\n",
       "77                      西坑绿道           省绿道网   省绿道\n",
       "78                     谭仙庙绿道           省绿道网   省绿道\n",
       "79                      保安绿道      接驳园山和大运公园   NaN\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 课后习题3\n",
    "# 读取文件中的数据，按照以下要求处理数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data27.xlsx\")\n",
    "# 1. 删除所有包含缺失值的行，得到新的数据集；\n",
    "pd.set_option('display.max_rows',None)\n",
    "df1.dropna(thresh = 5,axis = 0)\n",
    "pd.reset_option('display.max_rows')\n",
    "# 2. 获取简称、等级、长度数据，从中删除三列上都是缺失值的行；\n",
    "df1[['简称','等级','长度']].dropna(how = 'all')\n",
    "\n",
    "# 3. 删除数据，保留至少有77个非缺失值的列。\n",
    "df1.dropna(thresh = 77,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da4df592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>名称</th>\n",
       "      <th>简称</th>\n",
       "      <th>等级</th>\n",
       "      <th>长度</th>\n",
       "      <th>街道</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>省立绿道2号线</td>\n",
       "      <td>省绿道网</td>\n",
       "      <td>省绿道</td>\n",
       "      <td>2751</td>\n",
       "      <td>横岗街道</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>绿道深圳5号线（石樟坑水库-银湖森林公园）龙岗段</td>\n",
       "      <td>省绿道网</td>\n",
       "      <td>省绿道</td>\n",
       "      <td>公里数5.25</td>\n",
       "      <td>吉华</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>求水岭绿道</td>\n",
       "      <td>郊野绿道，低碳城配套绿道。</td>\n",
       "      <td>社区绿道</td>\n",
       "      <td>1.1017千米</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>屋角头绿道</td>\n",
       "      <td>郊野绿道，低碳城配套绿道。</td>\n",
       "      <td>社区绿道</td>\n",
       "      <td>len1.81881</td>\n",
       "      <td>坪地</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>西坑绿道</td>\n",
       "      <td>省绿道网</td>\n",
       "      <td>省绿道</td>\n",
       "      <td>全长5.68公里</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>求水岭绿道</td>\n",
       "      <td>郊野绿道，低碳城配套绿道。</td>\n",
       "      <td>社区绿道</td>\n",
       "      <td>1.1017</td>\n",
       "      <td>坪地</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>屋角头绿道</td>\n",
       "      <td>省绿道网</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.1017</td>\n",
       "      <td>坪地</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>西坑绿道</td>\n",
       "      <td>省绿道网</td>\n",
       "      <td>省绿道</td>\n",
       "      <td>1.1017</td>\n",
       "      <td>园山</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>谭仙庙绿道</td>\n",
       "      <td>省绿道网</td>\n",
       "      <td>省绿道</td>\n",
       "      <td>3.58</td>\n",
       "      <td>园山</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>保安绿道</td>\n",
       "      <td>接驳园山和大运公园</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.58</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          名称             简称    等级          长度    街道\n",
       "0                    省立绿道2号线           省绿道网   省绿道        2751  横岗街道\n",
       "1   绿道深圳5号线（石樟坑水库-银湖森林公园）龙岗段           省绿道网   省绿道     公里数5.25    吉华\n",
       "2                      求水岭绿道  郊野绿道，低碳城配套绿道。  社区绿道    1.1017千米   NaN\n",
       "3                      屋角头绿道  郊野绿道，低碳城配套绿道。  社区绿道  len1.81881    坪地\n",
       "4                       西坑绿道           省绿道网   省绿道    全长5.68公里   NaN\n",
       "..                       ...            ...   ...         ...   ...\n",
       "75                     求水岭绿道  郊野绿道，低碳城配套绿道。  社区绿道      1.1017    坪地\n",
       "76                     屋角头绿道           省绿道网   NaN      1.1017    坪地\n",
       "77                      西坑绿道           省绿道网   省绿道      1.1017    园山\n",
       "78                     谭仙庙绿道           省绿道网   省绿道        3.58    园山\n",
       "79                      保安绿道      接驳园山和大运公园   NaN        3.58   NaN\n",
       "\n",
       "[80 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 课后习题4\n",
    "# 读取文件中的数据，按照以下要求处理数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data27.xlsx\")\n",
    "# 1. 替换简称的缺失值为'缺失'，等级的缺失值为'暂定'，长度的缺失值为0，街道的缺失值为'街道'，作为新数据集1；\n",
    "df1.fillna({'简称':'缺失','等级':'暂定','长度':0,'街道':'街道'})\n",
    "# 2. 替换简称的缺失值为后面非缺失值的值，替换长度为前面非缺失值的值，作为新数据集2。\n",
    "df1.fillna({'简称':df1['简称'].fillna(method = 'bfill'),'长度':df1['长度'].fillna(method = 'ffill')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98295b",
   "metadata": {},
   "source": [
    "### 十三、dataframe的多重索引\n",
    "###### 1、多重索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02fdf179",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yaoyan\\AppData\\Local\\Temp\\ipykernel_36528\\560340809.py:61: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  dfx.sum(level = 1,axis = 0) # 此方法即将被移除,含义是按照index的1也就是地区，进行一次聚合统计操作，axis指定按行操作\n",
      "C:\\Users\\yaoyan\\AppData\\Local\\Temp\\ipykernel_36528\\560340809.py:64: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  dfx.sum(level = 0,axis = 1)# 此方法即将被移除,含义是按照index的0也就是结婚离婚来聚合统计\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>离婚率</th>\n",
       "      <th>结婚率</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>省份</th>\n",
       "      <th>地区</th>\n",
       "      <th>随机数</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>吉林省</th>\n",
       "      <th>大东北</th>\n",
       "      <th>30</th>\n",
       "      <td>26.28</td>\n",
       "      <td>85.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>辽宁省</th>\n",
       "      <th>大东北</th>\n",
       "      <th>16</th>\n",
       "      <td>32.69</td>\n",
       "      <td>120.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>黑龙江省</th>\n",
       "      <th>大东北</th>\n",
       "      <th>18</th>\n",
       "      <td>38.41</td>\n",
       "      <td>118.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>上海市</th>\n",
       "      <th>华东</th>\n",
       "      <th>56</th>\n",
       "      <td>14.14</td>\n",
       "      <td>46.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>安徽省</th>\n",
       "      <th>华东</th>\n",
       "      <th>2</th>\n",
       "      <td>45.93</td>\n",
       "      <td>277.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>山东省</th>\n",
       "      <th>华东</th>\n",
       "      <th>14</th>\n",
       "      <td>52.70</td>\n",
       "      <td>259.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>江苏省</th>\n",
       "      <th>华东</th>\n",
       "      <th>47</th>\n",
       "      <td>54.91</td>\n",
       "      <td>278.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>江西省</th>\n",
       "      <th>华东</th>\n",
       "      <th>13</th>\n",
       "      <td>21.29</td>\n",
       "      <td>118.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>浙江省</th>\n",
       "      <th>华东</th>\n",
       "      <th>29</th>\n",
       "      <td>30.48</td>\n",
       "      <td>142.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>福建省</th>\n",
       "      <th>华东</th>\n",
       "      <th>16</th>\n",
       "      <td>20.22</td>\n",
       "      <td>121.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>河南省</th>\n",
       "      <th>华中</th>\n",
       "      <th>20</th>\n",
       "      <td>59.41</td>\n",
       "      <td>367.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>湖北省</th>\n",
       "      <th>华中</th>\n",
       "      <th>74</th>\n",
       "      <td>38.24</td>\n",
       "      <td>197.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>湖南省</th>\n",
       "      <th>华中</th>\n",
       "      <th>3</th>\n",
       "      <td>40.11</td>\n",
       "      <td>191.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>内蒙古自治区</th>\n",
       "      <th>华北</th>\n",
       "      <th>76</th>\n",
       "      <td>19.93</td>\n",
       "      <td>77.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>北京市</th>\n",
       "      <th>华北</th>\n",
       "      <th>41</th>\n",
       "      <td>18.64</td>\n",
       "      <td>63.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>天津市</th>\n",
       "      <th>华北</th>\n",
       "      <th>18</th>\n",
       "      <td>12.41</td>\n",
       "      <td>38.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>山西省</th>\n",
       "      <th>华北</th>\n",
       "      <th>67</th>\n",
       "      <td>15.87</td>\n",
       "      <td>117.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>河北省</th>\n",
       "      <th>华北</th>\n",
       "      <th>5</th>\n",
       "      <td>45.26</td>\n",
       "      <td>211.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>广东省</th>\n",
       "      <th>华南</th>\n",
       "      <th>80</th>\n",
       "      <td>43.22</td>\n",
       "      <td>308.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>广西壮族自治区</th>\n",
       "      <th>华南</th>\n",
       "      <th>36</th>\n",
       "      <td>23.28</td>\n",
       "      <td>154.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>海南省</th>\n",
       "      <th>华南</th>\n",
       "      <th>25</th>\n",
       "      <td>3.44</td>\n",
       "      <td>30.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>宁夏回族自治区</th>\n",
       "      <th>西北</th>\n",
       "      <th>20</th>\n",
       "      <td>4.17</td>\n",
       "      <td>24.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>新疆维吾尔自治区</th>\n",
       "      <th>西北</th>\n",
       "      <th>21</th>\n",
       "      <td>16.80</td>\n",
       "      <td>79.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>甘肃省</th>\n",
       "      <th>西北</th>\n",
       "      <th>55</th>\n",
       "      <td>10.44</td>\n",
       "      <td>83.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>陕西省</th>\n",
       "      <th>西北</th>\n",
       "      <th>8</th>\n",
       "      <td>21.02</td>\n",
       "      <td>126.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>青海省</th>\n",
       "      <th>西北</th>\n",
       "      <th>25</th>\n",
       "      <td>3.16</td>\n",
       "      <td>24.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>云南省</th>\n",
       "      <th>西南</th>\n",
       "      <th>81</th>\n",
       "      <td>24.64</td>\n",
       "      <td>169.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>四川省</th>\n",
       "      <th>西南</th>\n",
       "      <th>98</th>\n",
       "      <td>60.79</td>\n",
       "      <td>283.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>西藏自治区</th>\n",
       "      <th>西南</th>\n",
       "      <th>48</th>\n",
       "      <td>0.75</td>\n",
       "      <td>12.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贵州省</th>\n",
       "      <th>西南</th>\n",
       "      <th>71</th>\n",
       "      <td>25.55</td>\n",
       "      <td>171.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>重庆市</th>\n",
       "      <th>西南</th>\n",
       "      <th>60</th>\n",
       "      <td>28.98</td>\n",
       "      <td>108.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    离婚率     结婚率\n",
       "省份       地区  随机数               \n",
       "吉林省      大东北 30   26.28   85.56\n",
       "辽宁省      大东北 16   32.69  120.90\n",
       "黑龙江省     大东北 18   38.41  118.60\n",
       "上海市      华东  56   14.14   46.78\n",
       "安徽省      华东  2    45.93  277.44\n",
       "山东省      华东  14   52.70  259.30\n",
       "江苏省      华东  47   54.91  278.32\n",
       "江西省      华东  13   21.29  118.22\n",
       "浙江省      华东  29   30.48  142.24\n",
       "福建省      华东  16   20.22  121.20\n",
       "河南省      华中  20   59.41  367.74\n",
       "湖北省      华中  74   38.24  197.18\n",
       "湖南省      华中  3    40.11  191.42\n",
       "内蒙古自治区   华北  76   19.93   77.08\n",
       "北京市      华北  41   18.64   63.54\n",
       "天津市      华北  18   12.41   38.66\n",
       "山西省      华北  67   15.87  117.56\n",
       "河北省      华北  5    45.26  211.36\n",
       "广东省      华南  80   43.22  308.84\n",
       "广西壮族自治区  华南  36   23.28  154.68\n",
       "海南省      华南  25    3.44   30.82\n",
       "宁夏回族自治区  西北  20    4.17   24.46\n",
       "新疆维吾尔自治区 西北  21   16.80   79.60\n",
       "甘肃省      西北  55   10.44   83.40\n",
       "陕西省      西北  8    21.02  126.60\n",
       "青海省      西北  25    3.16   24.18\n",
       "云南省      西南  81   24.64  169.78\n",
       "四川省      西南  98   60.79  283.32\n",
       "西藏自治区    西南  48    0.75   12.84\n",
       "贵州省      西南  71   25.55  171.36\n",
       "重庆市      西南  60   28.98  108.90"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# （1）excel多行列作为表头的表如何读入，index_col指定索引名字   header指定头部标题数量\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dfx = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\婚姻统计.xlsx\",header = [0,1],index_col = [0,1])\n",
    "dfc = pd.read_csv(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\婚姻统计.csv\",encoding = 'gb2312')\n",
    "dfx.index\n",
    "# （2）多维数据集索引排序方式\n",
    "# 链式调用同时完成排序行列，上面写了indexheader是0,1，说明这个df就是2层的，level也就只有2层，而ascending分别为他们指定升序降序\n",
    "dfx = dfx.sort_index(axis = 0,level = [0,1],ascending = [1,1]).sort_index(axis=1,ascending = [1,1])\n",
    "\n",
    "# （3）多重索引使用方式，注意必须先完成（2）的排序，不然就会报错,而且只能排升序\n",
    "#多层索引的本质为元祖组合多个层级level\n",
    "# 最外层索引调用\n",
    "dfx.loc['东北','结婚率'] \n",
    "\n",
    "# 内外交叉调用\n",
    "dfx.loc[('华东','上海市'),'结婚率'] # 行像内层调用一次，元祖形式 ,数据类型为序列\n",
    "\n",
    "# 切片大法\n",
    "dfx.loc[('东北','辽宁省'):('华东','江西省'),('离婚率','2017年'):('结婚率','2016年')]\n",
    "\n",
    "# 花式索引\n",
    "dfx.loc[[('东北','辽宁省'),('华东','江西省'),('华中','湖北省')],[('结婚率','2016年'),('离婚率','2017年')]]\n",
    "\n",
    "# 布尔索引\n",
    "dfx.loc[dfx[('离婚率','2017年')] < 10]\n",
    "dfx.set_index\n",
    "# 下标调用并不受多重索引的影响，你正常的写，他会正常的给值，索引的部分依旧不参与列数统计\n",
    "#dfx.iloc[1:6,2:3]\n",
    "\n",
    "# （4）多重索引如何修改索引值\n",
    "# a.set_index方法\n",
    "    # dfx.set_index(keys = ,drop=True , append = True,inplace = False, verify_integrity = True)\n",
    "    # keys 指定用于当做index的新列\n",
    "    # drop 被设定为索引的列是否从列中删除，默认删除\n",
    "    # append 为True时会不删除原索引只做增加，默认是否\n",
    "    # inplace 原df是否被替换\n",
    "    # verify_integrity 是否检索索引为唯一值，默认为否，检索的话当不为唯一值将会抛出异常\n",
    "    \n",
    "    #加一维\n",
    "dfx.set_index(keys = np.random.randint(1,99,31),append = True,inplace = True) # 如果不替换自动降低为非多重维度的行索引\n",
    "    #替换为两个维\n",
    "dfx.set_index(keys = [np.random.randint(1,99,31),np.random.randint(1,99,31)]) #多重维度请使用列表装入keys\n",
    "\n",
    "# b.重命名index的name请使用dfx.index.set_names\n",
    "dfx.index.set_names(names = ['地区','省份','随机数'],level = [0,1,2],inplace = True) # 遵循从外向内维度降低，0维度，1维度，2维度\n",
    "# 单个索引修改的话，可以直接用rename的方式\n",
    "dfx.rename(index = {'东北':'大东北'},inplace = True)\n",
    "dfx.rename(index = {('大东北','吉林省',dfx.index[0][2]):('东北','吉林',dfx.index[0][2])}) # rename算是一个特例，不遵守由外向内传递的降维度过程也能完成查找替换\n",
    "dfx.rename(index = {'吉林省':'吉林'}) # 一样能替换，绕开了外内的关系\n",
    "\n",
    "# c.reset_index还原index为列\n",
    "dfx.reset_index(names = [('地区','地区'),('省份','省份')],level = [0,1])\n",
    "dfx.reset_index(names = [('地区','地区')],level = 0) # names纵然只有一个也要用list给出\n",
    "#dfx.set_index( ('离婚率','2017年'),level = 1   )\n",
    "\n",
    "# （5）多重索引如何交换索引的level\n",
    "dfx = dfx.swaplevel(i = 0,j=1,axis = 0) # i,j各代表一个交换元素，axis代表交换的是行索引还是列索引\n",
    "\n",
    "# （6）多重索引中的统计方法\n",
    "dfx.sum(level = 1,axis = 0) # 此方法即将被移除,含义是按照index的1也就是地区，进行一次聚合统计操作，axis指定按行操作\n",
    "dfx.groupby('地区').sum() # 替换为groupby的方式，可以直接识别出index名\n",
    "\n",
    "dfx.sum(level = 0,axis = 1)# 此方法即将被移除,含义是按照index的0也就是结婚离婚来聚合统计\n",
    "dfx[('结婚率','总数')] = dfx[('结婚率','2017年')]+dfx[('结婚率','2016年')]\n",
    "dfx.groupby(level = 0,axis = 1).sum() # 请选用groupby来聚合内容\n",
    "\n",
    "\n",
    "\n",
    "# （7）行列互换\n",
    "# stack 列index转换行index 一行变多行，所以 堆栈了\n",
    "# unstack 行名转换列名\n",
    "#dfx = dfx.stack(level = 0)\n",
    "#dfx.index.set_names(names = '婚姻操作',level = 3,inplace = 1)\n",
    "# dfx.reset_index(names = ['1','2','3','婚姻']) # 最好还是上面给出名字，还原的时候给名字需要给四个\n",
    "#dfx.stack(level = 0)\n",
    "# dfx.unstack(level = 3)\n",
    "# dfx.stack(level = )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa14211",
   "metadata": {},
   "source": [
    "### 十三、dataframe的多重索引\n",
    "###### 2、MultiIndex 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3639e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name   Age Height\n",
      "           years     cm\n",
      "0    Alice    25    165\n",
      "1      Bob    30    180\n",
      "2  Charlie    35    175\n"
     ]
    }
   ],
   "source": [
    "# 上述学习的多重索引其实有个专门的 对象\n",
    "# print(type(dfx.index)) # <class 'pandas.core.indexes.multi.MultiIndex'>\n",
    "# 构造他的方式\n",
    "import pandas as pd\n",
    "columns = pd.MultiIndex.from_tuples([('Name', ''), ('Age', 'years'), ('Height', 'cm')]) #看起来是坐标系的写法\n",
    "# 创建包含多层次列索引的DataFrame\n",
    "data = [['Alice', 25, 165], ['Bob', 30, 180], ['Charlie', 35, 175]]\n",
    "df = pd.DataFrame(data,columns=columns)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1ab98",
   "metadata": {},
   "source": [
    "### 十三、dataframe的多重索引\n",
    "###### 课后习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6ca0baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>语文</th>\n",
       "      <th>数学</th>\n",
       "      <th>英语</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>赵婉清</th>\n",
       "      <td>64</td>\n",
       "      <td>84</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贰曼珠</th>\n",
       "      <td>58</td>\n",
       "      <td>96</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>裴冬雪</th>\n",
       "      <td>46</td>\n",
       "      <td>71</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     语文  数学  英语\n",
       "赵婉清  64  84  93\n",
       "贰曼珠  58  96  60\n",
       "裴冬雪  46  71  42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 课后习题1\n",
    "# 读取文件中的数据生成多层索引，按照以下要求处理数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data28.xlsx\",\n",
    "             header = [0,1],\n",
    "             index_col = [0,1]\n",
    "             )\n",
    "df1\n",
    "#  1. 读取期末考试数据；\n",
    "df1['期末']\n",
    "#  2. 读取三班的考试数据；\n",
    "df1.loc['三班']\n",
    "#  3. 读取一班期中考试数据。\n",
    "df1.loc['一班','期末']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b1930f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 课后习题2\n",
    "# 读取文件中的数据生成多层索引，按照以下要求处理数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data28.xlsx\",\n",
    "             header = [0,1],\n",
    "             index_col = [0,1]\n",
    "             )\n",
    "df1\n",
    "# 1. 读取一班期末的语文成绩；\n",
    "df1.sort_index(axis=0).sort_index(axis = 1)\n",
    "df1.loc['一班',('期末','语文')]\n",
    "# 2. 读取二班赵雪凝的期中成绩；\n",
    "df1.loc[('二班','赵雪凝'),'期中']\n",
    "# 3. 读取三班白飞兰期末的英语成绩。\n",
    "df1.loc[('三班','白飞兰'),('期末','英语')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89f7abbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>语文</th>\n",
       "      <th>数学</th>\n",
       "      <th>英语</th>\n",
       "      <th>语文</th>\n",
       "      <th>数学</th>\n",
       "      <th>英语</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>期中</th>\n",
       "      <th>期中</th>\n",
       "      <th>期中</th>\n",
       "      <th>期末</th>\n",
       "      <th>期末</th>\n",
       "      <th>期末</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>一班</th>\n",
       "      <td>223</td>\n",
       "      <td>210</td>\n",
       "      <td>235</td>\n",
       "      <td>168</td>\n",
       "      <td>251</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>三班</th>\n",
       "      <td>274</td>\n",
       "      <td>328</td>\n",
       "      <td>328</td>\n",
       "      <td>326</td>\n",
       "      <td>251</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>二班</th>\n",
       "      <td>205</td>\n",
       "      <td>210</td>\n",
       "      <td>188</td>\n",
       "      <td>220</td>\n",
       "      <td>228</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     语文   数学   英语   语文   数学   英语\n",
       "     期中   期中   期中   期末   期末   期末\n",
       "一班  223  210  235  168  251  195\n",
       "三班  274  328  328  326  251  272\n",
       "二班  205  210  188  220  228  219"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 课后习题3\n",
    "# 读取文件中的数据生成多层索引，按照以下要求处理数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data28.xlsx\",\n",
    "             header = [0,1],\n",
    "             index_col = [0,1]\n",
    "             )\n",
    "df1\n",
    "# 1. 交换列索引得到新数据集；\n",
    "df1 = df1.swaplevel(i = 0,j=1, axis = 1)\n",
    "\n",
    "# 2. 对新数据集中三个班分别统计期中、期末、各科目的总成绩；\n",
    "df1.groupby(level=0,axis=0).sum()\n",
    "\n",
    "# 3. 对新数据集中语文、数学、英语各科分别统计总成绩；\n",
    "df1.groupby(level = 0,axis = 1).sum()\n",
    "\n",
    "# 4. 对新数据集中各名学生在期中、期末分别统计平均成绩。\n",
    "df1.groupby(level = 1,axis = 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "421c6394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">三班</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>数学</th>\n",
       "      <th>英语</th>\n",
       "      <th>语文</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>期中</th>\n",
       "      <th>李俟侬</th>\n",
       "      <td>49.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>期末</th>\n",
       "      <th>李俟侬</th>\n",
       "      <td>62.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>期中</th>\n",
       "      <th>李诗桃</th>\n",
       "      <td>94.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>期末</th>\n",
       "      <th>李诗桃</th>\n",
       "      <td>49.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          三班            \n",
       "          数学    英语    语文\n",
       "期中 李俟侬  49.0  82.0  58.0\n",
       "期末 李俟侬  62.0  60.0  65.0\n",
       "期中 李诗桃  94.0  56.0  67.0\n",
       "期末 李诗桃  49.0  81.0  82.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 课后习题4\n",
    "# 读取文件中的数据生成多层索引，按照以下要求处理数据：\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\data28.xlsx\",\n",
    "             header = [0,1],\n",
    "             index_col = [0,1]\n",
    "             )\n",
    "df1\n",
    "# 1. 交换外层行索引班级与列索引期中期末，且保持索引的层级，得到新数据集。\n",
    "df1 = df1.stack(level = 0)\n",
    "df1 = df1.unstack(level = 0)\n",
    "df1 = df1.swaplevel(axis = 0).swaplevel(axis = 1)\n",
    "\n",
    "# 2. 从新数据集获取前4行数据，删除都是缺失值的列。\n",
    "df1.head(4).dropna(how = 'all',axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9e3c1",
   "metadata": {},
   "source": [
    "### 十四、Pandas与其他软件的交互，数据输入输出方式\n",
    "###### 1、excel与csv最基础的交互\n",
    "（1）输入是read_excel，read_csv\n",
    "（2）输出是to_excel，to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5512510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其中read_excel的方式已经在上文详细的探讨过了\n",
    "# 此处介绍to_excel\n",
    "df = pd.DataFrame([1,2,3,4])\n",
    "df.to_excel??\n",
    "# （1）制造一个excel写入者\n",
    "# writer = pd.ExcelWriter(path = 文件路径,mode = 'w'/'a' 覆盖/只读, engine = 'openpyxl/xlwings') \n",
    "\n",
    "# （2）正常的使用df.to_excel ，一次多表写入也ok\n",
    "# df.to_excel(excel_writer = writer,sheet_name = 表格名称1,na_rep= '-'空值如何表达)\n",
    "# df.iloc[:5].to_excel(excel_writer = writer,sheet_name = 表格名称2,na_rep= '-'空值如何表达)\n",
    "\n",
    "# （3）保存并关闭,不做就不会真的执行\n",
    "#     writer.save()\n",
    "#     writer.close()\n",
    "\n",
    "\n",
    "# df.to_excel(\n",
    "#     excel_writer,\n",
    "#     sheet_name: 'str' = 'Sheet1',\n",
    "#     na_rep: 'str' = '',\n",
    "#     float_format: 'str | None' = None,\n",
    "#     columns: 'Sequence[Hashable] | None' = None,\n",
    "#     header: 'Sequence[Hashable] | bool_t' = True,\n",
    "#     index: 'bool_t' = True,\n",
    "#     index_label: 'IndexLabel' = None,\n",
    "#     startrow: 'int' = 0,\n",
    "#     startcol: 'int' = 0,\n",
    "#     engine: 'str | None' = None,\n",
    "#     merge_cells: 'bool_t' = True,\n",
    "#     encoding: 'lib.NoDefault' = <no_default>,\n",
    "#     inf_rep: 'str' = 'inf',\n",
    "#     verbose: 'lib.NoDefault' = <no_default>,\n",
    "#     freeze_panes: 'tuple[int, int] | None' = None,\n",
    "#     storage_options: 'StorageOptions' = None,\n",
    "# ) -> 'None'\n",
    "# Docstring:\n",
    "# Write object to an Excel sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff9394",
   "metadata": {},
   "source": [
    "### 十四、Pandas与其他软件的交互，数据输入输出方式\n",
    "###### 2、主流数据库\n",
    "（1）输入靠链接数据库 read_sql_query\n",
    "（2）输出也可以写入数据库 to_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64a8e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （1） 安装mysql驱动模块\n",
    "# mysql                通过pymysql等模块\n",
    "# microsoft SQL Server 通过pymssql\n",
    "# ORACLE               通过cx-Oracle\n",
    "\n",
    "# （2） 安装简化驱动模块的操作的一个库\n",
    "# sqlalchemy \n",
    "# !pip list查询已按照的三方库\n",
    "\n",
    "# （3） 建立一个链接数据库的引擎\n",
    "import pandas as pd\n",
    "import sqlalchemy as sac\n",
    "#engine = sac.create_engine(mysql+pymysql://root:yangyang@localhost:3306/bank)\n",
    "# mysql+：代表是哪个数据库\n",
    "# pymysql：代表使用的三方库\n",
    "# localhost:3306 ：代表本机数据库服务器的ip地址或者机器名以及网络端口\n",
    "# root:yangyang ：代表的是秘密为yangyang 账号为localhost\n",
    "# bank ：代表我要多个库中的哪个库\n",
    "# 这里缺乏讲解如何链接ssh等操作\n",
    "\n",
    "# （4） 读取数据库\n",
    "# pd.read_sql_query(sql = 'select * from xxx.xxxxx',con = engine)\n",
    "\n",
    "# （5） 写回去数据库中\n",
    "df = pd.DataFrame([1,2,3,4])\n",
    "df.to_sql??\n",
    "# Signature:\n",
    "# df.to_sql(\n",
    "#     name: 'str',\n",
    "#     con,\n",
    "#     schema: 'str | None' = None,\n",
    "#     if_exists: 'str' = 'fail',\n",
    "#     index: 'bool_t' = True,\n",
    "#     index_label: 'IndexLabel' = None,\n",
    "#     chunksize: 'int | None' = None,\n",
    "#     dtype: 'DtypeArg | None' = None,\n",
    "#     method: 'str | None' = None,\n",
    "# ) -> 'int | None'\n",
    "# Docstring:\n",
    "# Write records stored in a DataFrame to a SQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216154e6",
   "metadata": {},
   "source": [
    "### 十四、Pandas与其他软件的交互，数据输入输出方式\n",
    "###### 3、html交互\n",
    "（1）输入靠链接数据库 read_html\n",
    "（2）输出写df.to_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42f6502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "     -------------                          41.0/112.2 kB 22.9 kB/s eta 0:00:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 437, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 526, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\http\\client.py\", line 465, in read\n",
      "    s = self.fp.read(amt)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\ssl.py\", line 1274, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\ssl.py\", line 1130, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 160, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 247, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 400, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 92, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 481, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 348, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 155, in __bool__\n",
      "    return any(self)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 47, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 206, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 297, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 162, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 231, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 308, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 491, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 536, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 166, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 107, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 147, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 621, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 559, in read\n",
      "    with self._error_catcher():\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"C:\\ProgramData\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 442, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "html5lib not found, please install it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 输入网页数据\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df1\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://money.163.com/ipo/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#返回一个列表，注意增加https，注意增加解析器专项网抓学习\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 不然就会报错提示html5lib not found, please install it\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 输出html\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:1205\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links)\u001b[0m\n\u001b[0;32m   1201\u001b[0m validate_header_arg(header)\n\u001b[0;32m   1203\u001b[0m io \u001b[38;5;241m=\u001b[39m stringify_path(io)\n\u001b[1;32m-> 1205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:982\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    980\u001b[0m retained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m flav \u001b[38;5;129;01min\u001b[39;00m flavor:\n\u001b[1;32m--> 982\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43m_parser_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m     p \u001b[38;5;241m=\u001b[39m parser(io, compiled_match, attrs, encoding, displayed_only, extract_links)\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\html.py:931\u001b[0m, in \u001b[0;36m_parser_dispatch\u001b[1;34m(flavor)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flavor \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml5lib\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_HTML5LIB:\n\u001b[1;32m--> 931\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml5lib not found, please install it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_BS4:\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeautifulSoup4 (bs4) not found, please install it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: html5lib not found, please install it"
     ]
    }
   ],
   "source": [
    "!pip install html5lib\n",
    "\n",
    "# 输入网页数据\n",
    "import pandas as pd\n",
    "df1= pd.read_html(r'http://money.163.com/ipo/') #返回一个列表，注意增加https，注意增加解析器专项网抓学习\n",
    "# 不然就会报错提示html5lib not found, please install it\n",
    "\n",
    "\n",
    "# 输出html\n",
    "import pandas as pd\n",
    "pd.DataFrame([1,2,3,4])\n",
    "df.to_html(r\"D:\\1-script\\3-PYTHON\\data存储jupyter用的个文件类型数据\\demo测试用的导出成为html.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f282da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c9dc4",
   "metadata": {},
   "source": [
    "### 完整展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3814d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 调整 pandas 的显示选项\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# 显示所有行\u001b[39;00m\n\u001b[0;32m      3\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# 显示所有列\u001b[39;00m\n\u001b[0;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.width\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# 自动调整宽度\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 调整 pandas 的显示选项\n",
    "pd.set_option('display.max_rows', None)  # 显示所有行\n",
    "pd.set_option('display.max_columns', None)  # 显示所有列\n",
    "pd.set_option('display.width', None)  # 自动调整宽度\n",
    "pd.set_option('display.max_colwidth', None)  # 自动调整列宽"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
