{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8cbd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "线性代表两者存在线性关系\n",
    "一元线性回归方程：y = b + mx m代表斜率 b代表截距 但是本质是m和b都是系数\n",
    "多元线性回归方程：y = b0 + b0x0 + b1x1 + …… +误差（实际与回归公式预测出来的y之间的误差）\n",
    "回归代表你需完成的任务\n",
    "PS 指数函数和幂函数都可以采用对数的形式 先计算线性公式的系数 在返回去变为指数幂函数\n",
    "\n",
    "\n",
    "\n",
    "线性回归的目标函数/损失函数 在不同的回归方式中不同\n",
    "线性回归可采取的方法：\n",
    "最小二乘法（OLS   ）           LOSS =  MSE/MAE             \n",
    "岭回归 (Ridge Regression)      LOSS =  MSE/MAE + L2\n",
    "套索回归 (Lasso Regression)    LOSS =  MSE/MAE + L1\n",
    "弹性网回归 (Elastic Net)       LOSS =  MSE/MAE + L1 +L2\n",
    "ps:回归公式必定经过样本均值 为中心点\n",
    "\n",
    "\n",
    "    \n",
    "捕捉非线性关系的方法\n",
    "支持向量机回归 (SVR)；       \n",
    "决策树回归 (Decision Tree Regression) ；       \n",
    "神经网络回归 ；      \n",
    "非参数回归 (如局部加权回归)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4eba62",
   "metadata": {},
   "source": [
    "### 二、最小二乘法    Ordinary Least Squares, OLS             \n",
    "适用场景：\n",
    "1. 数据不含多重共线性   \n",
    "2. 特征数小于样本数    \n",
    "3. 异常值不多    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ea0da",
   "metadata": {},
   "source": [
    "###### （一）一元回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3548bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.回归公式解释\n",
    "# y = b + mx     # 因变量 = 截距 + 斜率*自变量 \n",
    "# y = b0+ b1x1   # 因变量 = 截距 + 系数*自变量 \n",
    "\n",
    "# 2. 系数算法\n",
    "# b1 = ∑(xi-x_mean)(yi - y_mean) / ∑ (xi - x_mean)的平方        协方差/方差 \n",
    "# b0 = y_mean - b1 * x_mean  \n",
    "\n",
    "# 3. 模型目标函数/损失函数\n",
    "# RSS = Residual Sum of Squares，即 残差平方和 = ∑( y预测i - y实际i）的平方\n",
    "# MSE = RSS/样本数\n",
    "\n",
    "# 4. 案例演示\n",
    "import numpy as np\n",
    "x = [1,2,3,4,5]\n",
    "y = [2,3,5,7,11]\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "b1 = np.sum((np.array(x) - x_mean ) * (np.array(y) - y_mean ) )  /  (np.var(x)* len(x))\n",
    "b0 = y_mean -  b1 * x_mean\n",
    "print(f'拟合的一元回归方程式： y= {round(b0,2)}+{b1}x')\n",
    "\n",
    "# 5. 检查目标函数\n",
    "y_predict = np.array(x) * b1 + b0\n",
    "RSS = np.sum( np.square(( y - y_predict )) )\n",
    "MSE = RSS/len(x)\n",
    "print(f'残差平方和为:{RSS}, MSE为{MSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8616a395",
   "metadata": {},
   "source": [
    "###### （二）多元回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b1d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.回归公式解释\n",
    "# y = b0+ b1x1 + b2x2 + b3x3 + …… bnxn + ϵ \n",
    "# 因变量 = 截距 + 系数1 *自变量1 + 到第n个特征 + 误差 （实际与回归公式预测出来的y之间的误差）\n",
    "\n",
    "# 2. 系数算法\n",
    "# β=(XT @ X) 的−1 次方   @ XT @ y\n",
    "# β系数矩阵 = （ X的转置 点积 X 之后的矩阵 再逆向化  ） * X的转置 * Y的矩阵\n",
    "# 行列式不等于0 矩阵才是可逆的\n",
    "\n",
    "# 3. 模型目标函数/损失函数\n",
    "# RSS = Residual Sum of Squares，即 残差平方和 = ∑( y预测i - y实际i）的平方\n",
    "# MSE = RSS/样本数\n",
    "\n",
    "# 4. 案例演示\n",
    "# 一元x验证 情况案例 \n",
    "import numpy as np\n",
    "x = [1,2,3,4,5]\n",
    "y = [2,3,5,7,11]\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "intercept = np.ones( len(x))  # intercept 截距\n",
    "x_array = np.c_[intercept,x]  # 因为截距在多元中作为系数的一项共同算出 所以 会有默认的b0 列 1占位\n",
    "β= np.linalg.inv( np.dot(x_array.T,x_array) )   @ x_array.T  @ np.array(y)   # 计算逆函数前之所以 计算xT @ x的点积，是为了保证他是一个方阵\n",
    "β0 = β[0]\n",
    "β1 = β[1]\n",
    "# 解释\n",
    "# linalg  linear algebra  线性代数模块\n",
    "# inv     inverse         求逆\n",
    "# 在数学中，矩阵的逆是一个特殊的矩阵，满足以下条件： A A逆 =I\n",
    "# 其中 𝐴 是原始矩阵，A逆是其逆矩阵， 𝐼是单位矩阵  单位矩阵在乘法中起到“不改变”的作用，因此可以认为它是线性代数中的“1” 但是维度不同的时候 ，只能I * A = A  \n",
    "# 注意最后的结果是  2x2 @ 2x5 = 2x5 @ 5x1 = 2x1  np认为1维的数组是列 所以结果是2行1列 符合点积原则\n",
    "# @ 符号是 矩阵乘法，适用于线性代数中的矩阵运算。 要求 m*n n*p 才能矩阵乘法 结果会是 m*p\n",
    "# * 符号是 元素级乘法，适用于数组的对应元素相乘，维度不同 广播原则处理\n",
    "\n",
    "\n",
    "# 5. 检查目标函数\n",
    "print(f'拟合的一元回归方程式： y= {round(β0,2)}+{β1}x')\n",
    "y_predict = np.array(x) * β1 + β0\n",
    "RSS = np.sum( np.square((y - y_predict )) )\n",
    "MSE = RSS/len(x)\n",
    "print(f'残差平方和为:{RSS}, MSE为{MSE}')\n",
    "\n",
    "\n",
    "# 6. 直接用sklearn库算的时候无需手动增加截距系数\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear = LinearRegression()\n",
    "\n",
    "# 只有一个特征注意用reshape调整维度为行\n",
    "x = np.array(x)\n",
    "x = x.reshape(-1, 1) \n",
    "linear.fit(x,y)\n",
    "print('模型的系数为:',linear.coef_)\n",
    "print('模型的截距为:',linear.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "image_path = r\"D:\\1-script\\3-PYTHON\\image存储jupyter在用的图片存放于此\\矩阵x和xt.jpg\" \n",
    "display.Image(image_path, width= 300, height = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacfab59",
   "metadata": {},
   "source": [
    "### 三、岭回归   Ridge Regression      \n",
    "1. 引入l2正则化 ，使得梯度下降平缓， 约束参数空间在球形内 ，尽量选择出简单的模型参数 \n",
    "2. L2 正则化的特性：         \n",
    "L2 正则化的平方惩罚会使得所有系数趋于零，但不会将其完全归零。因为平方惩罚的损失函数在原点处是平滑的，优化过程中的调整不会导致某个系数完全消失。L2 正则化更倾向于减少所有特征的系数，而不是选择某些特征。    \n",
    "L2 正则化形成一个圆形的约束区域。优化过程中的等高线与圆形边缘相交时，几乎总是会在某个点上，而不是在坐标轴上，因此不容易形成完全为零的系数。        \n",
    "3. 引入方式：损失 +（超参数 * 系数平方和） 构成L2正则   \n",
    "4. 超参数λ 为一个控制正则强度的参数        \n",
    "5. 不会将系数收缩为零，所有特征都保留，但会抑制系数值。\n",
    "6. 适用场景：\n",
    "特征存在多重共线性，惩罚系数减小波动           \n",
    "样本数量略小于或接近特征数量    \n",
    "不想舍弃特征 认为所有特征都具有价值    \n",
    "7. 此时系数仍旧可以通过改变最小二乘法的方式直接算出来。计算见案例        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8771e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拟合的一元回归方程式： y= -0.93+2.1782178217821775x\n",
      "残差平方和为:2.8047446328791317,MSE是0.5609489265758263，损失函数为：1.1227699245172047\n",
      "模型的系数为: [2.17821782]\n",
      "模型的截距为: -0.9346534653465355\n"
     ]
    }
   ],
   "source": [
    "# 1.回归公式解释\n",
    "# y = b0+ b1x1 + b2x2 + b3x3 + …… bnxn + ϵ \n",
    "# 因变量 = 截距 + 系数1 *自变量1 + 到第n个特征 + 误差 （实际与回归公式预测出来的y之间的误差）\n",
    "\n",
    "# 2. 系数算法(也还有梯度下降算法来优化)\n",
    "# β=(XT @ X + λ*I ) 的−1 次方   @ XT @ y\n",
    "# β系数矩阵 = （ X的转置 点积 X 之后的矩阵 + 超参数 * 单位矩阵 再逆向化  ） * X的转置 * Y的矩阵\n",
    "# 行列式不等于0 矩阵才是可逆的\n",
    "\n",
    "# 3. 模型目标函数/损失函数\n",
    "# L2正则  =  超参数λ * ∑bj**2   系数平方和\n",
    "# loss    =  1/n ∑(yi - ypredict)**2 + 超参数λ * 系数平方总和 ∑bj**2 \n",
    "\n",
    "# 4. 案例演示\n",
    "# 一元x验证 情况案例 \n",
    "import numpy as np\n",
    "x = [1,2,3,4,5]\n",
    "y = [2,3,5,7,11]\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "intercept = np.ones( len(x))  # intercept 截距\n",
    "x_array = np.c_[intercept,x]  # 因为截距在多元中作为系数的一项共同算出 所以 会有默认的b0 列 1占位\n",
    "\n",
    "I =  np.eye(x_array.shape[1]) # 特征的数量\n",
    "I[0,0] = 0 # 不对截距项正则\n",
    "\n",
    "β= np.linalg.inv( np.dot(x_array.T,x_array) +  0.1 * I  )   @ x_array.T  @ np.array(y)   # 计算逆函数前之所以 计算xT @ x的点积，是为了保证他是一个方阵\n",
    "β0 = β[0]\n",
    "β1 = β[1]\n",
    "# 解释\n",
    "# linalg  linear algebra  线性代数模块\n",
    "# inv     inverse         求逆\n",
    "# 在数学中，矩阵的逆是一个特殊的矩阵，满足以下条件： A A逆 =I\n",
    "# 其中 𝐴 是原始矩阵，A逆是其逆矩阵， 𝐼是单位矩阵  单位矩阵在乘法中起到“不改变”的作用，因此可以认为它是线性代数中的“1”\n",
    "# 注意最后的结果是  2x2 @ 2x5 = 2x5 @ 5x1 = 2x1  np认为1维的数组是列 所以结果是2行1列 符合点积原则\n",
    "# @ 符号是 矩阵乘法，适用于线性代数中的矩阵运算。 要求 m*n n*p 才能矩阵乘法 结果会是 m*p\n",
    "# * 符号是 元素级乘法，适用于数组的对应元素相乘，维度不同 广播原则处理\n",
    "\n",
    "\n",
    "# 5. 检查目标函数\n",
    "print(f'拟合的一元回归方程式： y= {round(β0,2)}+{β1}x')\n",
    "y_predict = np.array(x) * β1 + β0\n",
    "RSS  = np.sum( np.square((y_predict-y )) )\n",
    "MSE  = RSS/len(x)\n",
    "λ   = 0.1 \n",
    "LOSS = MSE + λ * np.sum(β**2) # 此处采用MSE计算损失\n",
    "print(f'残差平方和为:{RSS},MSE是{MSE}，损失函数为：{LOSS}')\n",
    "\n",
    "# 6. 采取模型直接运算\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha = 0.1)\n",
    "x = np.array(x)\n",
    "x = x.reshape(-1, 1) \n",
    "ridge.fit(x,y)\n",
    "\n",
    "print('模型的系数为:',ridge.coef_)\n",
    "print('模型的截距为:',ridge.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13e538",
   "metadata": {},
   "source": [
    "### 四、套索回归  Lasso Regression\n",
    "1. 引入l1正则化 ，不取系数的平方和 而是取系数绝对值的和 LASSO回归在特征选择方面更具优势，而岭回归则更注重于减少模型的方差  \n",
    "2. L1 正则化的特性：          \n",
    "L1 正则化的绝对值惩罚会导致某些系数变为零。这是因为 L1 的损失函数在原点处存在尖角，使得在优化过程中，当某个系数接近于零时，绝对值的惩罚会促使模型将其完全归零。这种特性使得 L1 正则化能够实现特征选择，即选出对模型贡献最大的特征。           \n",
    "在参数空间中，L1 正则化形成一个菱形的约束区域。优化过程中，损失函数的等高线与菱形的边缘相交时，可能会发生系数为零的情况。     \n",
    "3. 引入方式：损失 + （超参数 * 系数绝对值和） 构成L1正则           \n",
    "4. 超参数λ 为一个控制正则强度的参数    \n",
    "5. 会将某些系数收缩为零，从而实现特征选择。\n",
    "6. 适用场景：         \n",
    "样本数小于特征数，高纬稀疏数据       \n",
    "特征需筛选 简化        \n",
    "7. 坐标下降法和最小角回归法（LARS）是实现套索回归的常用算法            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler  # 导入标准化工具\n",
    "\n",
    "# 步骤 1: 加载数据集\n",
    "california = fetch_california_housing()\n",
    "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "y = california.target\n",
    "\n",
    "# 步骤 2: 切分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 步骤 3: 归一化特征\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # 适配并转换训练数据\n",
    "X_test_scaled = scaler.transform(X_test)          # 仅转换测试数据\n",
    "\n",
    "# 步骤 4: 参数调整\n",
    "param_grid = {'alpha': np.logspace(-4, 1, 50)}  # 设置 alpha 的范围\n",
    "lasso = Lasso()\n",
    "grid_search = GridSearchCV(lasso, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train_scaled, y_train)  # 使用归一化后的训练数据\n",
    "\n",
    "# 输出最佳参数\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(f'Best alpha: {best_alpha}')\n",
    "\n",
    "# 步骤 5: 使用最佳参数训练模型\n",
    "best_lasso = grid_search.best_estimator_\n",
    "y_pred = best_lasso.predict(X_test_scaled)  # 使用归一化后的测试数据\n",
    "\n",
    "# 评估模型\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n",
    "\n",
    "# 步骤 6: 可视化预测结果（可选）\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('Actual vs Predicted Prices')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')  # 参考线\n",
    "plt.show()\n",
    "\n",
    "# 打印出公式\n",
    "coefficients = best_lasso.coef_\n",
    "intercept = best_lasso.intercept_\n",
    "\n",
    "# 构建线性公式\n",
    "formula = \"y = {:.4f}\".format(intercept)\n",
    "for feature, coef in zip(X.columns, coefficients):\n",
    "    if coef != 0:  # 只显示非零系数\n",
    "        formula += \" + {:.4f} * {}\".format(coef, feature)\n",
    "\n",
    "print(\"Lasso Regression Formula:\")\n",
    "print(formula)\n",
    "\n",
    "# x特质\n",
    "# MedInc: 中位收入（Median Income） - float64\n",
    "# HouseAge: 房屋年龄（House Age） - float64\n",
    "# AveRooms: 平均房间数（Average Rooms） - float64\n",
    "# AveBedrms: 平均卧室数（Average Bedrooms） - float64\n",
    "# Population: 人口（Population） - float64\n",
    "# AveOccup: 平均居住人数（Average Occupants） - float64\n",
    "# Latitude: 纬度（Latitude） - float64\n",
    "# Longitude: 经度（Longitude） - float64\n",
    "\n",
    "# y 目标\n",
    "# MedHouseVal 中位房价（Median House Value），单位为千美元。这是您希望预测的变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f96c6",
   "metadata": {},
   "source": [
    "### 五、弹力网回归 Elastic Net Regression\n",
    "1. 同时使用了L1和L2  \n",
    "2. 引入方式：损失 + L1 + L2         \n",
    "3. 适用场景：         \n",
    "样本数小于特征数，高纬稀疏数据       \n",
    "特征需筛选 简化     \n",
    "特征多重共线性强\n",
    "4. 坐标下降法（Coordinate Descent）和梯度下降法（Gradient Descent）是实现弹力网回归的常用算法  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbaf690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "image_path = r\"D:\\1-script\\3-PYTHON\\image存储jupyter在用的图片存放于此\\弹力网损失函数.jpg\"\n",
    "display.Image(image_path, width= 300, height = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12272791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 生成示例数据\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
    "\n",
    "# 将数据分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建弹力网回归模型\n",
    "# alpha 是正则化参数，l1_ratio 是 Lasso 和 Ridge 的比例\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 进行预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 计算均方误差和 R^2 分数\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"均方误差: {mse:.2f}\")\n",
    "print(f\"R^2 分数: {r2:.2f}\")\n",
    "\n",
    "# 可视化预测结果\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], '--', color='red')\n",
    "plt.xlabel('真实值')\n",
    "plt.ylabel('预测值')\n",
    "plt.title('弹力网回归预测结果')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb7be8",
   "metadata": {},
   "source": [
    "### 六、线性回归模型的评价方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c58615",
   "metadata": {},
   "source": [
    "###### （一）均方差 （Mean Squared Error, MSE）\n",
    "均方差 = 残差平方和 / 样本数 因为样本数固定 其实这个可以直接用目标函数 残差平方和计算也是一样的  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dccaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "image_path = r\"D:\\1-script\\3-PYTHON\\image存储jupyter在用的图片存放于此\\均方差含义.jpg\"\n",
    "display.Image(image_path, width= 300, height = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81cdfc",
   "metadata": {},
   "source": [
    "###### （二）均方根误差（Root Mean Squared Error, RMSE）\n",
    "均方根误差 = 开根号 （ 残差平方和 / 样本数  ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b45bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "image_path = r\"D:\\1-script\\3-PYTHON\\image存储jupyter在用的图片存放于此\\均方根误差含义.jpg\"\n",
    "display.Image(image_path, width= 300, height = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aac463c",
   "metadata": {},
   "source": [
    "###### （三）平均绝对误差（Mean Absolute Error, MAE）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf212a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "image_path = r\"D:\\1-script\\3-PYTHON\\image存储jupyter在用的图片存放于此\\平均绝对误差含义.jpg\"\n",
    "display.Image(image_path, width= 300, height = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 过拟合的风险\n",
    "高维数据：如果您的特征数量（维度）相对于样本数量较多，岭回归可以帮助减少过拟合的风险。\n",
    "复杂模型：如果模型过于复杂，拟合到了噪声而不是信号，考虑使用岭回归。\n",
    "2. 多重共线性\n",
    "特征之间的相关性：如果自变量之间存在较强的相关性（即多重共线性），岭回归可以通过引入正则化项来降低系数的波动性，从而提高模型的稳定性。\n",
    "3. 模型评估\n",
    "交叉验证：使用交叉验证来比较普通最小二乘法（OLS）和岭回归的性能。如果岭回归在验证集上的表现优于 OLS，可能意味着它更适合您的数据。\n",
    "4. 目标和需求\n",
    "可解释性：如果模型的可解释性非常重要，且您希望得到更简洁的模型，可以优先考虑 OLS。岭回归通常会导致一些系数收缩到接近于零，但不会完全去掉。\n",
    "预测精度：如果您的主要目标是提高预测精度，尤其是在面对噪声和不确定性时，岭回归可能是更好的选择。\n",
    "5. 模型选择标准\n",
    "信息准则：使用 AIC（赤池信息量准则）、BIC（贝叶斯信息量准则）等模型选择标准来评估岭回归的效果。如果引入正则化后模型的 AIC 或 BIC 降低，说明岭回归可能更适合。\n",
    "6. 数据的性质\n",
    "特征缩放：岭回归对特征缩放敏感，因此在使用之前确保特征已经被标准化（均值为0，方差为1）。如果数据的尺度差异很大，岭回归能够提供更稳定的结果。\n",
    "7. 可视化分析\n",
    "残差图：通过绘制残差图，观察模型预测与实际值的差异。如果存在明显的模式，可能表明模型未能捕捉到数据的某些特征，岭回归可能有助于改善这一点。\n",
    "总结\n",
    "如果您在模型的稳定性、预测性能和特征之间的相关性方面遇到问题，且希望通过引入正则化来改善模型，岭回归是一个值得考虑的选择。使用交叉验证和模型评估标准来比较不同模型的性能，将有助于做出更好的决策。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68af695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_excel(r\"C:\\Users\\yaoyan\\Downloads\\拟合数据.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53744c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sum'] = np.zeros(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(0,df.shape[0]):\n",
    "    print(f'------------计算第{row}行-----------------')\n",
    "    s = 0\n",
    "    j = 0\n",
    "    if row == 0:\n",
    "        df.loc[row,'sum'] = df.iloc[row,0]\n",
    "        \n",
    "    else:\n",
    "        for i in range(row,-1,-1):\n",
    "            print(f'本次取得是 {i} 行,本次取得是 {j} 列')\n",
    "            if i is not None and j is not None:\n",
    "                print(f'本次增加的值是：{df.iloc[i,j]}')\n",
    "                s += df.iloc[i,j]\n",
    "                j += 1\n",
    "                print(f\"s的值为：{s}\")\n",
    "                \n",
    "        df.loc[row,'sum'] =  s\n",
    "        print('---------------------')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6fb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sum'].to_csv(r\"C:\\Users\\yaoyan\\Downloads\\拟合数据结果.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fcd6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 8\n",
    "s = np.sqrt(s)\n",
    "s == np.sqrt(2)+np.sqrt(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf8219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
